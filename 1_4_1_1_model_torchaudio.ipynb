{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "ivzswNrhKdMp"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchaudio\n",
        "import torchaudio.transforms as T\n",
        "import numpy as np\n",
        "import scipy.signal\n",
        "import cv2\n",
        "import torch.nn as nn\n",
        "\n",
        "class AudioPreprocessing(nn.Module):\n",
        "    def __init__(self, sample_rate=22050, n_fft=512, win_length=512, hop_length=int(512 * 0.75)):\n",
        "        super(AudioPreprocessing, self).__init__()\n",
        "        self.resampler = T.Resample(orig_freq=44100, new_freq=sample_rate)\n",
        "        self.spectrogram = T.Spectrogram(n_fft=n_fft, win_length=win_length, hop_length=hop_length, power=None, window_fn=torch.hann_window)\n",
        "        self.sample_rate = sample_rate\n",
        "        self.n_fft = n_fft\n",
        "        self.win_length = win_length\n",
        "        self.hop_length = hop_length\n",
        "\n",
        "    def __call__(self, waveform):\n",
        "        # 1. Resample to 22050Hz\n",
        "        waveform = self.resampler(waveform)\n",
        "\n",
        "        # 2. Apply STFT\n",
        "        spectrogram = self.spectrogram(waveform)\n",
        "\n",
        "        # 3. Normalize\n",
        "        spectrogram = torch.abs(spectrogram)\n",
        "        max_val = spectrogram.max()\n",
        "        if max_val:\n",
        "            spectrogram /= max_val\n",
        "\n",
        "        # 4. Median Clipping\n",
        "        freq_median = torch.median(spectrogram, dim=2, keepdim=True)[0]\n",
        "        time_median = torch.median(spectrogram, dim=1, keepdim=True)[0]\n",
        "\n",
        "        mask = (spectrogram > (3 * freq_median)) & (spectrogram > (3 * time_median))\n",
        "        spectrogram = torch.where(mask, torch.tensor(1.0).to(spectrogram.device), torch.tensor(0.0).to(spectrogram.device))\n",
        "\n",
        "        # 5. Image processing techniques\n",
        "        # Convert to numpy for OpenCV processing\n",
        "        img = spectrogram.squeeze(0).cpu().numpy()\n",
        "        kernel = np.ones((3,3),np.uint8)\n",
        "\n",
        "        # Closing & Dilation\n",
        "        img = cv2.morphologyEx(img, cv2.MORPH_CLOSE, kernel)\n",
        "        img = cv2.dilate(img, kernel, iterations=1)\n",
        "\n",
        "        # Median Filter & Remove small objects\n",
        "        img = cv2.medianBlur(img, 3)\n",
        "        num_labels, labels, stats, centroids = cv2.connectedComponentsWithStats(img.astype(np.uint8), 4, cv2.CV_32S)\n",
        "        for i in range(1, num_labels):\n",
        "            if stats[i][-1] < 100:  # Change this threshold according to your needs\n",
        "                img[labels == i] = 0\n",
        "\n",
        "        # Convert back to torch tensor\n",
        "        spectrogram = torch.tensor(img).float().unsqueeze(0)\n",
        "\n",
        "        return spectrogram"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Cij2N6mKeYN",
        "outputId": "cf2b3dba-0144-4a4b-cfb2-4f8b5ee9284c"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "Og-dzxQhKdMs"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import torchaudio\n",
        "import torchaudio.transforms as T\n",
        "import pandas as pd\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torchaudio.transforms as transforms\n",
        "import torch.nn as nn\n",
        "\n",
        "class BirdSongDataset(Dataset):\n",
        "    def __init__(self, df, audio_dir, class_info, transform=None):\n",
        "        self.df = df\n",
        "        self.audio_dir = audio_dir\n",
        "        self.class_info = class_info\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        filename = self.df.iloc[idx, 0]\n",
        "        audio_path = os.path.join(self.audio_dir, filename)\n",
        "        waveform, sample_rate = torchaudio.load(audio_path)\n",
        "\n",
        "        # Getting the labels\n",
        "        labels = self.df[self.df['filename'] == filename]\n",
        "        target = torch.zeros(len(self.class_info))\n",
        "        for _, label in labels.iterrows():\n",
        "            class_name = label['class']\n",
        "            target[self.class_info.index(class_name)] = 1.0\n",
        "\n",
        "        if self.transform:\n",
        "            waveform = self.transform(waveform)\n",
        "\n",
        "        return waveform, target\n",
        "\n",
        "# Load csv files\n",
        "train_csv = pd.read_csv('/content/drive/MyDrive/DeepLearning/data/train.csv')\n",
        "class_info_csv = pd.read_csv('/content/drive/MyDrive/DeepLearning/data/class_info.csv')\n",
        "class_names = class_info_csv['class name'].tolist()\n",
        "\n",
        "# Split data into train and validation sets\n",
        "train_df, valid_df = train_test_split(train_csv, test_size=0.1, random_state=42)\n",
        "\n",
        "# Chain transformations using nn.Sequential\n",
        "transform = nn.Sequential(\n",
        "    AudioPreprocessing()\n",
        ")\n",
        "\n",
        "train_dataset = BirdSongDataset(train_df, '/content/drive/MyDrive/DeepLearning/data/train/', class_names, transform=transform)\n",
        "valid_dataset = BirdSongDataset(valid_df, '/content/drive/MyDrive/DeepLearning/data/train/', class_names, transform=transform)\n",
        "\n",
        "\n",
        "# Define DataLoader objects\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=32, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "c-ZXYhlCKdMs"
      },
      "outputs": [],
      "source": [
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Calculate the global max length of waveforms in the dataset\n",
        "global_max_len = max(max(wf.shape[2] for wf, _ in dataset) for dataset in [train_dataset, valid_dataset])\n",
        "\n",
        "def collate_fn(batch):\n",
        "    # A batch is a list of (waveform, target) pairs\n",
        "    waveforms, targets = zip(*batch)\n",
        "\n",
        "    # Pad the waveforms to the global max length\n",
        "    waveforms = [torch.cat([wf, torch.zeros((1, wf.shape[1], global_max_len - wf.shape[2]))], dim=2) for wf in waveforms]\n",
        "\n",
        "    waveforms = torch.stack(waveforms)\n",
        "    targets = torch.stack(targets)\n",
        "\n",
        "    return waveforms, targets\n",
        "\n",
        "# Update DataLoader objects with the new collate_fn\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn, num_workers=2)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn, num_workers=2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "m8-S3jGAKdMt"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
        "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
        "        self.fc1 = nn.Linear(32 * 64 * 72, 128)\n",
        "        self.fc2 = nn.Linear(128, num_classes)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(self.relu(self.conv1(x)))\n",
        "        x = self.pool(self.relu(self.conv2(x)))\n",
        "        #print(x.shape)\n",
        "        x = x.view(x.size(0), 32 * 64 * 72)\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        x = self.sigmoid(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "QDEtRrK_KdMt"
      },
      "outputs": [],
      "source": [
        "# Set up the device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Initialize the model\n",
        "model = SimpleCNN(num_classes=len(class_names)).to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IPW3M4gxKdMu",
        "outputId": "e85af24c-cf1a-4327-bd7e-48e7a1070c75"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Train Loss: 0.2410, Validation Loss: 0.2283\n",
            "Epoch 2, Train Loss: 0.2141, Validation Loss: 0.2317\n",
            "Epoch 3, Train Loss: 0.2108, Validation Loss: 0.2282\n",
            "Epoch 4, Train Loss: 0.2091, Validation Loss: 0.2295\n",
            "Epoch 5, Train Loss: 0.2085, Validation Loss: 0.2280\n",
            "Epoch 6, Train Loss: 0.2080, Validation Loss: 0.2296\n",
            "Epoch 7, Train Loss: 0.2074, Validation Loss: 0.2291\n",
            "Epoch 8, Train Loss: 0.2071, Validation Loss: 0.2299\n",
            "Epoch 9, Train Loss: 0.2073, Validation Loss: 0.2286\n",
            "Epoch 10, Train Loss: 0.2068, Validation Loss: 0.2293\n",
            "Epoch 11, Train Loss: 0.2067, Validation Loss: 0.2287\n",
            "Epoch 12, Train Loss: 0.2066, Validation Loss: 0.2290\n",
            "Epoch 13, Train Loss: 0.2063, Validation Loss: 0.2301\n",
            "Epoch 14, Train Loss: 0.2063, Validation Loss: 0.2312\n",
            "Epoch 15, Train Loss: 0.2063, Validation Loss: 0.2295\n",
            "Epoch 16, Train Loss: 0.2064, Validation Loss: 0.2302\n",
            "Epoch 17, Train Loss: 0.2063, Validation Loss: 0.2295\n",
            "Epoch 18, Train Loss: 0.2061, Validation Loss: 0.2284\n",
            "Epoch 19, Train Loss: 0.2061, Validation Loss: 0.2296\n",
            "Epoch 20, Train Loss: 0.2060, Validation Loss: 0.2286\n",
            "Epoch 21, Train Loss: 0.2060, Validation Loss: 0.2282\n",
            "Epoch 22, Train Loss: 0.2059, Validation Loss: 0.2292\n",
            "Epoch 23, Train Loss: 0.2059, Validation Loss: 0.2289\n",
            "Epoch 24, Train Loss: 0.2059, Validation Loss: 0.2290\n",
            "Epoch 25, Train Loss: 0.2059, Validation Loss: 0.2291\n",
            "Epoch 26, Train Loss: 0.2060, Validation Loss: 0.2284\n",
            "Epoch 27, Train Loss: 0.2064, Validation Loss: 0.2305\n",
            "Epoch 28, Train Loss: 0.2061, Validation Loss: 0.2289\n",
            "Epoch 29, Train Loss: 0.2057, Validation Loss: 0.2293\n",
            "Epoch 30, Train Loss: 0.2057, Validation Loss: 0.2287\n",
            "Finished Training\n"
          ]
        }
      ],
      "source": [
        "# Loss and Optimizer\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "class SmoothBCELoss(nn.Module):\n",
        "    def __init__(self, smoothing=0.1):\n",
        "        super(SmoothBCELoss, self).__init__()\n",
        "        self.smoothing = smoothing\n",
        "\n",
        "    def forward(self, pred, target):\n",
        "        target = target * (1.0 - self.smoothing) + 0.5 * self.smoothing\n",
        "        loss = nn.BCELoss()(pred, target)\n",
        "        return loss\n",
        "\n",
        "\n",
        "criterion = SmoothBCELoss(smoothing=0.1)\n",
        "\n",
        "\n",
        "num_epochs = 30\n",
        "for epoch in range(num_epochs):\n",
        "    # Training\n",
        "    model.train()\n",
        "    running_train_loss = 0.0\n",
        "    for i, (inputs, labels) in enumerate(train_loader):\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_train_loss += loss.item()\n",
        "\n",
        "    train_loss = running_train_loss / len(train_loader)\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    running_val_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in valid_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            running_val_loss += loss.item()\n",
        "\n",
        "    val_loss = running_val_loss / len(valid_loader)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}\")\n",
        "\n",
        "print('Finished Training')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8g6kj9f2KdMv",
        "outputId": "2e681c62-2229-45a8-b928-4fe6e659ddd8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1 Score (Samples): 0.6620396306670816\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "model.eval()\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in valid_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        outputs = model(inputs)\n",
        "        preds = outputs.round()  # Convert to binary: 0 or 1\n",
        "\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "f1_macro = f1_score(all_labels, all_preds, average='samples')\n",
        "print(f\"F1 Score (Samples): {f1_macro}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "q6z5GWm1KdMv"
      },
      "outputs": [],
      "source": [
        "def test_collate_fn(batch):\n",
        "    # A batch is a list of (waveform, filename) pairs for the test dataset\n",
        "    waveforms, filenames = zip(*batch)\n",
        "\n",
        "    # Pad the waveforms to the global max length\n",
        "    waveforms = [torch.cat([wf, torch.zeros((1, wf.shape[1], global_max_len - wf.shape[2]))], dim=2) for wf in waveforms]\n",
        "\n",
        "    waveforms = torch.stack(waveforms)\n",
        "\n",
        "    return waveforms, filenames\n",
        "\n",
        "\n",
        "# Loading the test set\n",
        "class BirdSongTestDataset(Dataset):\n",
        "    def __init__(self, df, audio_dir, transform=None):\n",
        "        self.df = df\n",
        "        self.audio_dir = audio_dir\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        filename = self.df.iloc[idx, 0]\n",
        "        audio_path = os.path.join(self.audio_dir, filename)\n",
        "        waveform, sample_rate = torchaudio.load(audio_path)\n",
        "\n",
        "        if self.transform:\n",
        "            waveform = self.transform(waveform)\n",
        "\n",
        "        return waveform, filename\n",
        "\n",
        "test_csv = pd.read_csv('/content/drive/MyDrive/DeepLearning/data/test.csv')\n",
        "test_dataset = BirdSongTestDataset(test_csv, '/content/drive/MyDrive/DeepLearning/data/test/', transform=transform)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, collate_fn=test_collate_fn, num_workers=2)\n",
        "\n",
        "# Make predictions on test set\n",
        "model.eval()\n",
        "predictions = {}\n",
        "with torch.no_grad():\n",
        "    for inputs, filenames in test_loader:\n",
        "        inputs = inputs.to(device)\n",
        "        outputs = model(inputs)\n",
        "        preds = outputs.round()  # Convert to binary: 0 or 1\n",
        "        for fname, pred in zip(filenames, preds):\n",
        "            predictions[fname] = pred.cpu().numpy()\n",
        "\n",
        "# Convert predictions to submission format\n",
        "submission_df = pd.DataFrame.from_dict(predictions, orient='index', columns=class_names)\n",
        "submission_df.reset_index(inplace=True)\n",
        "submission_df.rename(columns={'index': 'filename'}, inplace=True)\n",
        "submission_df.to_csv('/content/drive/MyDrive/DeepLearning/submission.csv', index=False)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}