{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "za_-0XklmBeX"
      },
      "source": [
        "# Aprendizaje Profundo\n",
        "Daniel López Gala - UO281798\n",
        "\n",
        "Se dispone del conjunto de datos NIPS4BPLUS, el cual contiene 674 ficheros de audio con una duración total de menos de una hora. En estos audios podemos encontrar grabaciones de aproximadamente 5 segundos con cantos de pájaros realizadas en 39 localizaciones diferentes repartidas por 7 regiones de Francia y España."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kISgPuot6Sx_"
      },
      "source": [
        "# Aprendizaje Profundo\n",
        "Daniel López Gala - UO281798\n",
        "\n",
        "Se dispone del conjunto de datos NIPS4BPLUS, el cual contiene 674 ficheros de audio con una duración total de menos de una hora. En estos audios podemos encontrar grabaciones de aproximadamente 5 segundos con cantos de pájaros realizadas en 39 localizaciones diferentes repartidas por 7 regiones de Francia y España."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "urB-A95SsJri"
      },
      "outputs": [],
      "source": [
        "#base_path = \"/content/drive/MyDrive/DeepLearning/\"\n",
        "base_path = \"\"\n",
        "DEBUG = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z8PtqgSelI8r",
        "outputId": "67bc0146-7a1c-4a13-8ed5-f302dc5963b0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "62qWTY1kk7e6",
        "outputId": "2c8a4c3d-6a1f-4bbe-b51d-e75691407c40"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Collecting scikit-multilearn\n",
            "  Downloading scikit_multilearn-0.2.0-py3-none-any.whl (89 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.4/89.4 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: scikit-multilearn\n",
            "Successfully installed scikit-multilearn-0.2.0\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import random\n",
        "from collections import defaultdict\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "\n",
        "import torchaudio\n",
        "import torchaudio.transforms as T\n",
        "from torchaudio.transforms import Resample\n",
        "import librosa\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau, CosineAnnealingLR\n",
        "import torchvision.models as models\n",
        "\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "#%pip install transformers\n",
        "#%pip install scikit-multilearn\n",
        "from skmultilearn.model_selection import iterative_train_test_split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fwU9YFOunaak"
      },
      "source": [
        "## Preprocesamiento y visualización\n",
        "\n",
        "- Se define una función `visualize_intermediates` para crear imágenes de los pasos intermedios usados en el preprocesamiento de los audios.\n",
        "\n",
        "- La clase `AudioPreprocessing` define los pasos para procesar la imagen. Se incluyen:\n",
        "  - Resample (No usado)\n",
        "  - STFT (Convertir a espectrograma)\n",
        "  - Normalización\n",
        "  - Median clipping\n",
        "  - Conectar puntos cercanos mediante filtros\n",
        "  - Closing\n",
        "  - Dilation\n",
        "  - Median blur\n",
        "  - Eliminar residuos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "xKj6CGbCH2RX"
      },
      "outputs": [],
      "source": [
        "def visualize_intermediates(intermediates, sample_rate=44100, hop_length=196):\n",
        "\n",
        "    # Set default background color for figures to white\n",
        "    plt.rcParams['figure.facecolor'] = 'white'\n",
        "\n",
        "    for key, value in intermediates.items():\n",
        "        if len(value.shape) == 2 and value.shape[1] > 2:  # This indicates a waveform\n",
        "            plt.figure(figsize=(12, 4))\n",
        "\n",
        "            # Calculate time axis in seconds for waveform\n",
        "            time_axis_waveform = np.linspace(0, value.shape[1] / sample_rate, value.shape[1])\n",
        "\n",
        "            plt.plot(time_axis_waveform, value[0].cpu().numpy())\n",
        "            plt.xlabel(\"Time (seconds)\")\n",
        "            plt.title(f\"{key}\")\n",
        "            plt.show()\n",
        "            continue\n",
        "\n",
        "        print(f\"Processing {key} with shape {value.shape}\")\n",
        "\n",
        "        if value.dim() == 4 and value.shape[-1] == 2:\n",
        "            complex_representation = value[0, ..., 0] + 1j * value[0, ..., 1]  # Convert to complex\n",
        "            magnitude = torch.abs(complex_representation).cpu().numpy()\n",
        "            phase = torch.angle(complex_representation).cpu().numpy()\n",
        "        elif value.is_complex():\n",
        "            magnitude = torch.abs(value).squeeze().cpu().numpy()\n",
        "            phase = torch.angle(value).squeeze().cpu().numpy()\n",
        "        else:\n",
        "            magnitude = value.squeeze().cpu().numpy()\n",
        "            phase = None\n",
        "\n",
        "        # Calculate time axis in seconds for magnitude\n",
        "        time_axis_magnitude = np.linspace(0, magnitude.shape[1] * hop_length / sample_rate, magnitude.shape[1])\n",
        "\n",
        "        # Plot magnitude with inverted grayscale colormap\n",
        "        plt.figure(figsize=(12, 4))\n",
        "        plt.imshow(magnitude, cmap='gray_r', aspect='auto', origin='lower', extent=[time_axis_magnitude[0], time_axis_magnitude[-1], 0, magnitude.shape[0]])\n",
        "        plt.xlabel(\"Time (seconds)\")\n",
        "        plt.title(f\"{key} Magnitude\")\n",
        "        plt.colorbar()\n",
        "        plt.show()\n",
        "\n",
        "        # Plot phase\n",
        "        if phase is not None:\n",
        "            plt.figure(figsize=(12, 4))\n",
        "            plt.imshow(((phase + np.pi) % (2 * np.pi) - np.pi), cmap='hsv', aspect='auto', origin='lower', vmin=-np.pi, vmax=np.pi, extent=[time_axis_magnitude[0], time_axis_magnitude[-1], 0, phase.shape[0]])\n",
        "            plt.xlabel(\"Time (seconds)\")\n",
        "            plt.title(f\"{key} Phase\")\n",
        "            plt.colorbar()\n",
        "            plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "jFTKwdi0v96g"
      },
      "outputs": [],
      "source": [
        "class AudioPreprocessing(nn.Module):\n",
        "    def __init__(self, debug=DEBUG, sample_rate=16000, n_fft=1024, win_length=1024, hop_length=196, augment=False):\n",
        "        super().__init__()\n",
        "        self.debug = debug\n",
        "        self.augment = augment\n",
        "        self.sample_rate = sample_rate\n",
        "        self.resampler = T.Resample(44100, sample_rate)\n",
        "        self.spectrogram = T.MelSpectrogram(sample_rate, n_fft=n_fft, win_length=win_length, hop_length=hop_length, f_min=500, f_max=15000)\n",
        "\n",
        "    def normalize(self, spectrogram):\n",
        "        min_val = torch.min(spectrogram)\n",
        "        return (spectrogram - min_val) / (torch.max(spectrogram) - min_val + 1e-5)\n",
        "\n",
        "    def median_blurring(self, spectrogram):\n",
        "        img = spectrogram.squeeze(0).cpu().numpy()\n",
        "        img = cv2.medianBlur(img.astype(np.float32), 5)\n",
        "        return torch.tensor(img, device=spectrogram.device).float().unsqueeze(0)\n",
        "\n",
        "    def binary_image_creation(self, spectrogram, threshold=1.5):\n",
        "        freq_median = torch.median(spectrogram, dim=2, keepdim=True).values\n",
        "        time_median = torch.median(spectrogram, dim=1, keepdim=True).values\n",
        "        mask = (spectrogram > threshold * freq_median) & (spectrogram > threshold * time_median)\n",
        "        return mask.float()\n",
        "\n",
        "    def spot_removal(self, spectrogram, threshold=0.5):\n",
        "        # Threshold the spectrogram to get a binary mask\n",
        "        binary_mask = (spectrogram > threshold).float()\n",
        "\n",
        "        # Convert to numpy for morphological operations\n",
        "        binary_np = binary_mask.squeeze(0).cpu().numpy().astype(np.uint8)\n",
        "\n",
        "        # Define a kernel for morphological operations (adjust size as needed)\n",
        "        kernel = np.ones((3, 3), np.uint8)\n",
        "\n",
        "        # Perform morphological opening to remove small noise\n",
        "        cleaned_binary_np = cv2.morphologyEx(binary_np, cv2.MORPH_OPEN, kernel)\n",
        "\n",
        "        # Convert back to tensor\n",
        "        cleaned_binary_mask = torch.tensor(cleaned_binary_np, device=spectrogram.device).float().unsqueeze(0)\n",
        "\n",
        "        # Multiply with original spectrogram to remove the noise\n",
        "        denoised_spectrogram = spectrogram * cleaned_binary_mask\n",
        "\n",
        "        return denoised_spectrogram\n",
        "\n",
        "    def morph_closing(self, spectrogram):\n",
        "        img = spectrogram.squeeze(0).cpu().numpy()\n",
        "        kernel = np.ones((3, 3), np.uint8)\n",
        "        img = cv2.morphologyEx(img, cv2.MORPH_CLOSE, kernel)\n",
        "        return torch.tensor(img, device=spectrogram.device).float().unsqueeze(0)\n",
        "\n",
        "    def random_time_shift(self, waveform, max_shift_sec=0.2):\n",
        "        \"\"\" Randomly shifts the waveform in the time domain \"\"\"\n",
        "        max_shift = int(max_shift_sec * self.sample_rate)\n",
        "        shift = random.randint(-max_shift, max_shift)\n",
        "        return torch.roll(waveform, shifts=shift, dims=-1)\n",
        "\n",
        "    def random_pitch_shift(self, waveform, max_shift=2):\n",
        "        \"\"\" Randomly shifts the pitch of the waveform \"\"\"\n",
        "        shift = random.uniform(-max_shift, max_shift)\n",
        "        return T.FrequencyMasking(freq_mask_param=int(shift))(waveform)\n",
        "\n",
        "    def random_volume_gain(self, waveform, min_gain=0.5, max_gain=1.5):\n",
        "        \"\"\" Randomly changes the volume of the waveform \"\"\"\n",
        "        gain = random.uniform(min_gain, max_gain)\n",
        "        return waveform * gain\n",
        "\n",
        "    def random_noise_injection(self, waveform, noise_level=0.005):\n",
        "        \"\"\" Adds random noise to the waveform \"\"\"\n",
        "        noise = torch.randn_like(waveform) * noise_level\n",
        "        return waveform + noise\n",
        "\n",
        "    def forward(self, waveform):\n",
        "        intermediates = {}\n",
        "\n",
        "        # Data augmentation\n",
        "        if self.augment:\n",
        "            waveform = self.random_time_shift(waveform)\n",
        "            waveform = self.random_pitch_shift(waveform)\n",
        "            waveform = self.random_volume_gain(waveform)\n",
        "            # waveform = self.random_noise_injection(waveform)\n",
        "\n",
        "        # Resampling to the target sample rate\n",
        "        waveform = self.resampler(waveform)\n",
        "\n",
        "        # Convert stereo to mono if necessary by averaging the channels\n",
        "        if waveform.shape[0] > 1:\n",
        "            waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
        "\n",
        "        spectrogram = self.spectrogram(waveform)\n",
        "        if self.debug: intermediates['original_spectrograms'] = spectrogram\n",
        "\n",
        "        spectrogram = self.normalize(spectrogram)\n",
        "        spectrogram = self.median_blurring(spectrogram)\n",
        "        if self.debug: intermediates['spectrograms_after_median_blurring'] = spectrogram\n",
        "\n",
        "        spectrogram = self.binary_image_creation(spectrogram)\n",
        "        if self.debug: intermediates['binary_image'] = spectrogram\n",
        "\n",
        "        spectrogram = self.spot_removal(spectrogram)\n",
        "        if self.debug: intermediates['spectrograms_after_spot_removal'] = spectrogram\n",
        "\n",
        "        spectrogram = self.morph_closing(spectrogram)\n",
        "        if self.debug: intermediates['spectrograms_after_morph_closing'] = spectrogram\n",
        "\n",
        "        if not self.debug:\n",
        "            return spectrogram, {}\n",
        "\n",
        "        return (spectrogram, intermediates)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95KHcfMgocTZ"
      },
      "source": [
        "## Carga de datos\n",
        "\n",
        "Se leen los audios de forma individual. Cada audio es un objeto. `BirdSongDataset` define el método `__getitem__` para obtener cada instancia del dataset.\n",
        "\n",
        "No se tiene en cuenta en qué momento del audio suena cada pájaro, tan sólo qué pájaros suenan en cada audio. El problema se plantea como **clasificación multietiqueta**.\n",
        "\n",
        "El método `get_class_proportions` se utiliza para comprobar que los datasets *train* y *validation* contienen la misma proporción de clases, es decir, están estratíficados."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Og-dzxQhKdMs",
        "outputId": "ca7f40ea-25f4-4d8a-e0b1-6a928359e305"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchaudio/functional/functional.py:584: UserWarning: At least one mel filterbank has all zero values. The value for `n_mels` (128) may be set too high. Or, the value for `n_freqs` (513) may be set too low.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "class BirdSongDataset(Dataset):\n",
        "    def __init__(self, df, audio_dir, class_info, transform=None):\n",
        "        segments = []\n",
        "\n",
        "        unique_filenames = df['filename'].unique()  # Only process each audio file once\n",
        "        for unique_filename in unique_filenames:\n",
        "            audio_path = os.path.join(audio_dir, unique_filename)\n",
        "            waveform, sample_rate = torchaudio.load(audio_path)\n",
        "            total_segments = int(math.ceil(waveform.shape[1] / sample_rate))  # Total segments in the audio\n",
        "\n",
        "            # Calculate the unique labels for each segment\n",
        "            for idx in range(total_segments):\n",
        "                start_time, end_time = idx, idx + 1\n",
        "                labels_in_segment = df[(df['filename'] == unique_filename) &\n",
        "                                       (df['end'] > start_time) &\n",
        "                                       (df['start'] < end_time)]['class'].unique().tolist()\n",
        "                segments.append({\n",
        "                    'filename': unique_filename,\n",
        "                    'segment_idx': idx,\n",
        "                    'start': start_time,\n",
        "                    'end': end_time,\n",
        "                    'class': \",\".join(labels_in_segment)\n",
        "                })\n",
        "\n",
        "        self.segments = pd.DataFrame(segments)\n",
        "        self.audio_dir = audio_dir\n",
        "        self.class_info = class_info\n",
        "        # Store the transform, but we will not apply it in __getitem__\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.segments)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.segments.iloc[idx]\n",
        "        audio_path = os.path.join(self.audio_dir, row['filename'])\n",
        "        waveform, sample_rate = torchaudio.load(audio_path)\n",
        "\n",
        "        # Ensure waveform is mono\n",
        "        if waveform.shape[0] > 1:\n",
        "            waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
        "\n",
        "        # Resample from 44100 Hz to 16000 Hz if necessary\n",
        "        if sample_rate != 16000:\n",
        "            resampler = Resample(orig_freq=sample_rate, new_freq=16000)\n",
        "            waveform = resampler(waveform)\n",
        "\n",
        "        # Extract 1-second segment\n",
        "        start_sample = int(row['start'] * 16000)  # Use the new sample rate here\n",
        "        end_sample = int(row['end'] * 16000)      # Use the new sample rate here\n",
        "        waveform = waveform[:, start_sample:end_sample]\n",
        "\n",
        "        # Padding if needed\n",
        "        if waveform.shape[1] < 16000:  # Use the new sample rate here\n",
        "            num_padding = 16000 - waveform.shape[1]  # Use the new sample rate here\n",
        "            waveform = torch.cat([waveform, torch.zeros(1, num_padding)], dim=1)\n",
        "\n",
        "        class_names = row['class'].split(\",\") if row['class'] else []\n",
        "        target = torch.zeros(len(self.class_info))\n",
        "        for class_name in class_names:\n",
        "            target[self.class_info.index(class_name)] = 1.0\n",
        "\n",
        "        return waveform, target\n",
        "\n",
        "    def get_filename(self, idx):\n",
        "        return self.segments.iloc[idx]['filename']\n",
        "\n",
        "train_csv = pd.read_csv(f'{base_path}data/train.csv') # CSV with train audio filenames, and bird class names labels.\n",
        "class_info_csv = pd.read_csv(f'{base_path}data/class_info.csv')\n",
        "class_names = class_info_csv['class name'].tolist()\n",
        "\n",
        "# Convert the labels to a binary matrix form\n",
        "y = np.zeros((len(train_csv), len(class_names)))\n",
        "for i, (_, row) in enumerate(train_csv.iterrows()):\n",
        "    labels = row['class'].split(\",\")\n",
        "    for label in labels:\n",
        "        y[i, class_names.index(label)] = 1\n",
        "\n",
        "X_train, y_train, X_val, y_val = iterative_train_test_split(np.array(train_csv), y, test_size=0.2)\n",
        "\n",
        "train_df = pd.DataFrame(X_train, columns=train_csv.columns)\n",
        "valid_df = pd.DataFrame(X_val, columns=train_csv.columns)\n",
        "\n",
        "train_transform = nn.Sequential(\n",
        "    AudioPreprocessing(augment=True)\n",
        ")\n",
        "\n",
        "valid_transform = nn.Sequential(\n",
        "    AudioPreprocessing(augment=False)\n",
        ")\n",
        "\n",
        "train_dataset = BirdSongDataset(train_df, f'{base_path}data/train/', class_names, transform=train_transform)\n",
        "valid_dataset = BirdSongDataset(valid_df, f'{base_path}data/train/', class_names, transform=valid_transform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "DBrPrvtp-rIt"
      },
      "outputs": [],
      "source": [
        "def aggregate_predictions(predictions, segments_df):\n",
        "    aggregated_predictions = {}\n",
        "    for filename in segments_df['filename'].unique():\n",
        "        aggregated_predictions[filename] = set()\n",
        "        segments = segments_df[segments_df['filename'] == filename]\n",
        "        for idx, row in segments.iterrows():\n",
        "            aggregated_predictions[filename].update(predictions[idx])\n",
        "    return aggregated_predictions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_FVZvTwP-rIt",
        "outputId": "54c4bdbc-5e4a-45c2-a40b-faebefb00dff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              filename  segment_idx  start  end                    class\n",
            "0  nips4b_train001.wav            0      0    1              Sylcan_song\n",
            "1  nips4b_train001.wav            1      1    2  Sylcan_song,Petpet_song\n",
            "2  nips4b_train001.wav            2      2    3  Sylcan_song,Petpet_song\n",
            "3  nips4b_train001.wav            3      3    4                         \n",
            "4  nips4b_train001.wav            4      4    5              Petpet_song\n",
            "5  nips4b_train001.wav            5      5    6              Petpet_song\n"
          ]
        }
      ],
      "source": [
        "specific_audio_segments = train_dataset.segments[train_dataset.segments['filename'] == 'nips4b_train001.wav']\n",
        "print(specific_audio_segments)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wOJ3S-BI-rIu",
        "outputId": "58905c2c-c73c-496d-978d-e025c9fefebc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Label: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Predicted Classes: ['Petpet_song', 'Sylcan_song']\n"
          ]
        }
      ],
      "source": [
        "segment_idx = 2\n",
        "waveform, label = train_dataset[segment_idx]\n",
        "print(f\"Label: {label}\")\n",
        "\n",
        "# Convert tensor label back to class names to check\n",
        "predicted_classes = [class_name for idx, class_name in enumerate(class_names) if label[idx] == 1.0]\n",
        "print(\"Predicted Classes:\", predicted_classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "eeesSAdr-rIu"
      },
      "outputs": [],
      "source": [
        "# if DEBUG:\n",
        "#     print(f\"Number of elements: {len(valid_dataset)}\")\n",
        "#     sample, target = valid_dataset[90]\n",
        "#     processed_sample, intermediates = sample\n",
        "\n",
        "#     print(processed_sample.shape)\n",
        "#     num_positive_labels = target.sum().item()\n",
        "#     print(target)\n",
        "#     print(f\"Number of positive labels: {num_positive_labels}\")\n",
        "\n",
        "#     predicted_classes = [class_name for idx, class_name in enumerate(class_names) if target[idx] == 1.0]\n",
        "#     print(\"Predicted Classes:\", predicted_classes)\n",
        "\n",
        "#     visualize_intermediates(intermediates)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "EAU_nQNdH2RZ"
      },
      "outputs": [],
      "source": [
        "# if DEBUG:\n",
        "#     print(f\"Number of elements: {len(train_dataset)}\")\n",
        "#     sample, target = train_dataset[30]\n",
        "#     processed_sample, intermediates = sample\n",
        "\n",
        "#     print(processed_sample.shape)\n",
        "#     num_positive_labels = target.sum().item()\n",
        "#     print(target)\n",
        "#     print(f\"Number of positive labels: {num_positive_labels}\")\n",
        "\n",
        "#     predicted_classes = [class_name for idx, class_name in enumerate(class_names) if target[idx] == 1.0]\n",
        "#     print(\"Predicted Classes:\", predicted_classes)\n",
        "#     visualize_intermediates(intermediates)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-pXedOD0UwwG"
      },
      "source": [
        "**Calcular la longitud máxima de las formas de onda**\n",
        "\n",
        "Se determina la longitud máxima entre todas las formas de onda para poder rellenar (padding) o truncar los audios posteriormente, garantizando que todos tengan la misma longitud.\n",
        "\n",
        "La función `collate_fn` se utiliza para procesar y combinar un lote (batch) de muestras en el dataloader. Asegura que todas las formas de onda tengan la misma longitud (rellenando con ceros si es necesario) y devuelve las formas de onda junto con sus objetivos (etiquetas). Para esto, necesita la longitud máxima calculada anteriormente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "gvjVdc97TymV"
      },
      "outputs": [],
      "source": [
        "def collate_fn(batch):\n",
        "    # Extract the waveform (or spectrogram) and ignore intermediates (the dict used to debug) if they exist\n",
        "    waveforms_or_spectrograms = [wf[0] if isinstance(wf, tuple) else wf for wf, _ in batch]\n",
        "\n",
        "    if isinstance(batch[0][1], str):\n",
        "        # This handles the Test set scenario where targets might be filenames\n",
        "        waveforms_or_spectrograms = torch.stack(waveforms_or_spectrograms)\n",
        "        _, filenames = zip(*batch)\n",
        "        return waveforms_or_spectrograms, filenames\n",
        "\n",
        "    elif isinstance(batch[0][1], dict):\n",
        "        # This handles the scenario where DEBUG is False and intermediates dictionary is returned\n",
        "        waveforms_or_spectrograms = torch.stack(waveforms_or_spectrograms)\n",
        "        return waveforms_or_spectrograms  # Note: Here we return only the waveforms as there are no target labels\n",
        "\n",
        "    else:\n",
        "        # This handles the Training or validation batch scenario\n",
        "        waveforms_or_spectrograms = torch.stack(waveforms_or_spectrograms)\n",
        "        _, targets = zip(*batch)\n",
        "        targets = torch.stack(targets)\n",
        "        return waveforms_or_spectrograms, targets\n",
        "\n",
        "BATCH_SIZE=6\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QHV02H3TVZtm"
      },
      "source": [
        "## Definición del modelo\n",
        "\n",
        "- Se define una arquitectura basada en el modelo ResNet50 preentrenado.\n",
        "- Se adapta la primera capa convolucional para aceptar imágenes de un solo canal (grises).\n",
        "- Se elimina la última capa completamente conectada del ResNet y se agrega una clasificación personalizada para adaptar la arquitectura al problema multietiqueta.\n",
        "\n",
        "Una de las cosas que se ha probado, es utilizar una mezcla de *transfer-learning* y *fine-tuning*.\n",
        "\n",
        "**Transferencia de aprendizaje**:\n",
        "\n",
        "El modelo se carga y se adaptan algunas capas. Se congelan los pesos de las capas del modelo preentrenado para que no se actualicen durante el entrenamiento inicial, por lo que sólo las capas personalizadas, como la capa de clasificación, se entrenarán. Es decir, se adapta a una tarea diferente el modelo, manteniendo los pesos originales.\n",
        "\n",
        "**Fine-tuning**:\n",
        "\n",
        "Después de algunas épocas de entrenamiento determinadas en el código se desbloquean las capas del modelo preentrenado para que sus pesos también puedan actualizarse durante el entrenamiento\n",
        "\n",
        "```python\n",
        "if epoch == X:\n",
        "    for param in model.features.parameters():\n",
        "        param.requires_grad = True\n",
        "```\n",
        "\n",
        "Este fine-tuning ajusta el modelo a los datos específicos para mejorar el rendimiento, aunque causa cierto *overfitting* al sobreescribir los pesos originales con los datos de entrenamiento.\n",
        "\n",
        "Finalmente, tan sólo se utiliza fine-tuning. Los resultados no cambiaban excesivamente."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D1zpmtnhXXEk"
      },
      "source": [
        "## Entrenamiento\n",
        "\n",
        "- Se utiliza BCE (Binary Cross Entropy), adecuada para problemas de clasificación multietiqueta junto a un optimizador Adam con tasa de aprendizaje `0.0001`.\n",
        "- Se utiliza un programador de learning rate ReduceLROnPlateau, o CosineAnnealingLR, que disminuye la tasa de aprendizaje si la función de pérdida no mejora.\n",
        "\n",
        "El proceso de entrenamiento se ejecuta a través de 20-50 épocas, y durante cada época se calcula la pérdida en entrenamiento y se ajustan los pesos del modelo, se calcula el F1 en entrenamiento, y se pasa el modelo a modo de evaluación para evaluar en el conjunto de validación, calculando tanto la pérdida como el F1 score.\n",
        "\n",
        "Si el modelo mejora se guarda un checkpoint de los pesos. Está implementado un sistema de early stopping para evitar el sobreajuste restaurando el mejor modelo.\n",
        "\n",
        "Después de cada época se ajusta el learning rate según la evolución de la pérdida en validación.\n",
        "\n",
        "**Búsqueda de umbral (threshold)**:\n",
        "- Se inicializa una lista de posibles `thresholds` de 0.1 a 1 en incrementos de 0.1. Estos son los umbrales para decidir si una predicción (probabilidad) del modelo es positiva o negativa.\n",
        "- Para cada umbral se calcula el F1 score en entrenamiento y validación y se elige el umbral que produce el mejor F1 score en el conjunto de validación.\n",
        "\n",
        "Esto es importante porque las salidas del modelo son valores continuos entre 0 y 1, que representan la confianza del modelo en que esa etiqueta es positiva, y es necesario decidir un umbral (`threshold`) para convertir estas salidas continuas en etiquetas binarias definitivas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269,
          "referenced_widgets": [
            "17dbc949472b473e99806bc15b503d6f",
            "a9b4413a8bb24d279e819d0d23df12d1",
            "93a8cc7a91a24dc48d64e1816fc9d5b2",
            "cffd7cafd8db417c97a10bc4767b4bac",
            "70240c7f8f9d4f4b876d31c8c4f83721",
            "a31c64e6fceb45189787230e1bd21313",
            "6d83e503df73440399c31fb96490e584",
            "a7286590c8514fa1808e0bb4095deda7",
            "d711bd2d8054406da64e61f66eab0070",
            "c0d933a8ec4349c09f51e5c7783e216d",
            "a0d1454da85a42da90e4e9fb5ca3140c",
            "5e1400b6c6754a5ebe9aed611d7b2d3d",
            "416182a2386d4090b0f63c885e1d446a",
            "9364225b595d43c188470de773b00027",
            "5791d8e238c943b9834c00e846bf444f",
            "9399e36c52ba470a9b5b7de4567a0de7",
            "131da14eaf69452d8fb0bd306e107ef0",
            "53a007432f854f6a961326388600c894",
            "5cb21c09058043ee9daa35d349ae046f",
            "0532f01962824494a659d4011625b537",
            "a61bfaa25ccb4b6ea0a344cf48af8a43",
            "857770f5f2254fc48a98fa8df0fdb70b",
            "a03b99d89eb74878bb48161f113ef74b",
            "3f837cbf74794df8b3aeaca22642e633",
            "7b0cd102b3c34316a329471c8ae58f7d",
            "596b7b60353f44ffadb06128081d8087",
            "81ba194876fa4e2c9d78a6065edb1f6a",
            "814ff308d4a343818e822d13a0f038ca",
            "5bc707a882e14be19b095b0185c1c9e9",
            "262d21afce3f41e7964e876e5af55674",
            "984ed1249d61470387193a1e62ada4ac",
            "977c2bb85f9a432a81abdffb58e7e14a",
            "60921ee726794f94bb592cb7c9526283"
          ]
        },
        "id": "U52GfnjY6SyF",
        "outputId": "322bd8d9-fda4-4659-b471-0aa790c03349"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "17dbc949472b473e99806bc15b503d6f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "preprocessor_config.json:   0%|          | 0.00/297 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5e1400b6c6754a5ebe9aed611d7b2d3d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a03b99d89eb74878bb48161f113ef74b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/346M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n",
            "GPU Name: Tesla T4\n",
            "GPU Memory Allocated: 345756160\n",
            "GPU Memory Cached: 400556032\n",
            "Torch version: 2.1.0+cu118\n",
            "CUDA available: True\n",
            "Number of GPUs: 1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/cuda/memory.py:444: FutureWarning: torch.cuda.memory_cached has been renamed to torch.cuda.memory_reserved\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoFeatureExtractor, ASTForAudioClassification\n",
        "import torch\n",
        "\n",
        "# Initialize feature extractor and model\n",
        "feature_extractor = AutoFeatureExtractor.from_pretrained(\"MIT/ast-finetuned-audioset-10-10-0.4593\")\n",
        "model = ASTForAudioClassification.from_pretrained(\"MIT/ast-finetuned-audioset-10-10-0.4593\")\n",
        "\n",
        "# Assuming class_names is defined somewhere in your code\n",
        "model.classifier.dense = torch.nn.Linear(model.config.hidden_size, len(class_names))\n",
        "model.num_labels = len(class_names)\n",
        "\n",
        "# Move model to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Debugging information\n",
        "print(f\"Using device: {device}\")\n",
        "if device.type == 'cuda':\n",
        "    print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU Memory Allocated: {torch.cuda.memory_allocated(0)}\")\n",
        "    print(f\"GPU Memory Cached: {torch.cuda.memory_cached(0)}\")\n",
        "else:\n",
        "    print(\"No GPU available, using CPU.\")\n",
        "\n",
        "# Additional system info\n",
        "print(f\"Torch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
        "\n",
        "# Note: Uncomment and modify the lines related to 'class_names' based on your specific use case.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u3_P-iwc6SyF",
        "outputId": "b26dcc76-42f1-4fcd-9af0-6990540df7d8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1, Train Loss: 0.0672, Train F1: 0.1527, Val Loss: 0.0441, Val F1: 0.0726\n",
            "Finished Training\n"
          ]
        }
      ],
      "source": [
        "total_epochs = 1\n",
        "\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=0.001)\n",
        "scheduler = ReduceLROnPlateau(optimizer, 'min', factor=0.5, patience=2)\n",
        "\n",
        "best_val_loss = float('inf')\n",
        "epochs_no_improve = 0\n",
        "n_epochs_stop = 20\n",
        "early_stop = False\n",
        "thresholds = np.arange(0.1, 1, 0.1)\n",
        "\n",
        "# Your training loop with modifications for the AST model\n",
        "for epoch in range(total_epochs):\n",
        "    model.train()\n",
        "    running_train_loss = 0.0\n",
        "    train_labels = []\n",
        "    train_preds = []\n",
        "\n",
        "    for inputs, labels in train_loader:\n",
        "        # Make sure inputs are single channel\n",
        "        if inputs.ndim == 3 and inputs.size(1) > 1:\n",
        "            print(f\"Input has multiple channels, shape: {inputs.shape}\")\n",
        "            inputs = torch.mean(inputs, dim=1, keepdim=True)\n",
        "        elif inputs.ndim == 2:\n",
        "            inputs = inputs.unsqueeze(1)  # Add channel dimension\n",
        "\n",
        "        # Now inputs should be of shape [batch, 1, time]\n",
        "        #print(f\"Shape before feature extractor: {inputs.shape}\")\n",
        "\n",
        "        # Process each waveform through the feature extractor\n",
        "        inputs = feature_extractor(inputs.squeeze(1).numpy(), sampling_rate=16000, return_tensors=\"pt\", padding=True)\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.logits\n",
        "\n",
        "        # Compute loss, no need to apply sigmoid since BCEWithLogitsLoss does that internally\n",
        "        loss = criterion(logits, labels.float())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_train_loss += loss.item()\n",
        "\n",
        "        # Store predictions and labels for F1 calculation\n",
        "        train_preds.extend(torch.sigmoid(logits).detach().cpu().numpy())\n",
        "        train_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    train_loss = running_train_loss / len(train_loader)\n",
        "\n",
        "    # Your validation loop remains the same, with changes for feature extraction and predictions handling\n",
        "    model.eval()\n",
        "    running_val_loss = 0.0\n",
        "    val_labels = []\n",
        "    val_preds = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in valid_loader:\n",
        "            # Process through feature extractor and to device\n",
        "            inputs = feature_extractor(inputs.squeeze(1).numpy(), sampling_rate=16000, return_tensors=\"pt\", padding=True)\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(**inputs)\n",
        "            logits = outputs.logits\n",
        "\n",
        "            loss = criterion(logits, labels.float())\n",
        "            running_val_loss += loss.item()\n",
        "\n",
        "            val_preds.extend(torch.sigmoid(logits).detach().cpu().numpy())\n",
        "            val_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    val_loss = running_val_loss / len(valid_loader)\n",
        "\n",
        "    # Calculate F1 scores without manual thresholding\n",
        "    train_f1 = f1_score(np.array(train_labels), np.array(train_preds) > 0.1, average='samples', zero_division=1)\n",
        "    val_f1 = f1_score(np.array(val_labels), np.array(val_preds) > 0.1, average='samples', zero_division=1)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Train F1: {train_f1:.4f}, Val Loss: {val_loss:.4f}, Val F1: {val_f1:.4f}\")\n",
        "\n",
        "    # Checkpointing\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        epochs_no_improve = 0\n",
        "        torch.save(model.state_dict(), 'best_model.pth')\n",
        "    else:\n",
        "        epochs_no_improve += 1\n",
        "\n",
        "    # Early stopping\n",
        "    if epochs_no_improve == n_epochs_stop:\n",
        "        print('Early stopping!')\n",
        "        early_stop = True\n",
        "        break\n",
        "\n",
        "    # Adjusting learning rate\n",
        "    scheduler.step(val_loss)\n",
        "\n",
        "if early_stop:\n",
        "    print(\"Stopped training. Loading best model weights!\")\n",
        "    model.load_state_dict(torch.load('best_model.pth'))\n",
        "\n",
        "print('Finished Training')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 504
        },
        "id": "xuuNXqBT-rIw",
        "outputId": "6ebf22a3-7571-459c-ed62-b0b03124703c"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1000x500 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Finished Training\n"
          ]
        }
      ],
      "source": [
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(train_losses, label='Training Loss')\n",
        "plt.plot(val_losses, label='Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and Validation Loss over Epochs')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "print('Finished Training')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wt_5gGclbVWf"
      },
      "source": [
        "## Evaluación y Predicción en el conjunto de Test\n",
        "\n",
        "1. **Evaluación de las predicciones**:\n",
        "Se pone el modelo en modo `eval()` y se itera sobre el conjunto de validación para obtener las predicciones y se calcula el F1 usando el mejor umbral.\n",
        "\n",
        "2. **Preparación del conjunto de Test**:\n",
        "Se crea la clase `BirdSongTestDataset` que lee de `test.csv`, y se crea un DataLoader para el conjunto de Test.\n",
        "\n",
        "3. **Predicciones en el conjunto de Test**:\n",
        "Se itera sobre el conjunto de test y se obtienen las predicciones del modelo para cada archivo de audio. Se binarizan usando el mejor umbral y se almacenan en un diccionario con el nombre del archivo como clave.\n",
        "Las predicciones se convierten en un DataFrame de Pandas y se preparan los datos en el formato esperado, y por último se guarda el DataFrame en un archivo CSV."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RhZzEOkWH2Rb",
        "outputId": "1e1ed034-b65e-4868-9d21-208776c47be6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best threshold: 0.1\n"
          ]
        }
      ],
      "source": [
        "best_threshold_val = 0.1\n",
        "print(f\"Best threshold: {best_threshold_val}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8g6kj9f2KdMv",
        "outputId": "3f49f320-f358-41a7-9b11-b3ebbfd1d71f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "F1 Score (Samples): 0.06806282722513089\n"
          ]
        }
      ],
      "source": [
        "model.eval()\n",
        "audio_preds = defaultdict(list)\n",
        "audio_labels = defaultdict(list)\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch_idx, (inputs, labels) in enumerate(valid_loader):\n",
        "        # Process through feature extractor and to device\n",
        "        inputs = feature_extractor(inputs.squeeze(1).numpy(), sampling_rate=16000, return_tensors=\"pt\", padding=True)\n",
        "\n",
        "        # Ensure the input shape is compatible with the model\n",
        "        if inputs.input_values.shape[-1] < 16:\n",
        "            padding = 16 - inputs.input_values.shape[-1]\n",
        "            inputs.input_values = F.pad(inputs.input_values, (0, padding), 'constant', 0)\n",
        "\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Forward pass through the model\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "        # Extract logits and apply sigmoid\n",
        "        logits = outputs.logits\n",
        "        probabilities = torch.sigmoid(logits)\n",
        "\n",
        "        # Compare with threshold\n",
        "        preds = (probabilities > best_threshold_val).float()\n",
        "\n",
        "        # Fetch the filename for each segment\n",
        "        for i, (input_, label, pred) in enumerate(zip(inputs, labels, preds)):\n",
        "            # Compute the correct index in the dataset\n",
        "            dataset_index = batch_idx * valid_loader.batch_size + i\n",
        "            filename = valid_dataset.get_filename(dataset_index)\n",
        "            audio_preds[filename].append(pred.cpu().numpy())\n",
        "            # The label should be the same for all segments of the same audio, so we append it only once\n",
        "            if filename not in audio_labels:\n",
        "                audio_labels[filename] = label.cpu().numpy()\n",
        "\n",
        "# Aggregating the segment-level predictions for each audio to generate a single prediction for the whole audio\n",
        "for filename in audio_preds:\n",
        "    # Here, we take the max prediction for each class across all segments as the audio-level prediction\n",
        "    audio_preds[filename] = np.maximum.reduce(audio_preds[filename])\n",
        "\n",
        "all_labels = list(audio_labels.values())\n",
        "all_preds = list(audio_preds.values())\n",
        "\n",
        "f1_macro = f1_score(all_labels, all_preds, average='samples', zero_division=1)\n",
        "print(f\"F1 Score (Samples): {f1_macro}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "q6z5GWm1KdMv"
      },
      "outputs": [],
      "source": [
        "class BirdSongTestDataset(Dataset):\n",
        "    def __init__(self, df, audio_dir, transform=None):\n",
        "        segments = []\n",
        "\n",
        "        unique_filenames = df['filename'].unique()\n",
        "        for unique_filename in unique_filenames:\n",
        "            audio_path = os.path.join(audio_dir, unique_filename)\n",
        "            waveform, sample_rate = torchaudio.load(audio_path)\n",
        "            total_segments = int(math.ceil(waveform.shape[1] / sample_rate))\n",
        "\n",
        "            for idx in range(total_segments):\n",
        "                start_time, end_time = idx, idx + 1\n",
        "                segments.append({\n",
        "                    'filename': unique_filename,\n",
        "                    'segment_idx': idx,\n",
        "                    'start': start_time,\n",
        "                    'end': end_time,\n",
        "                })\n",
        "\n",
        "        self.segments = pd.DataFrame(segments)\n",
        "        self.audio_dir = audio_dir\n",
        "        self.transform = transform\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.segments)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.segments.iloc[idx]\n",
        "        audio_path = os.path.join(self.audio_dir, row['filename'])\n",
        "        waveform, sample_rate = torchaudio.load(audio_path)\n",
        "\n",
        "        # Ensure waveform is mono\n",
        "        if waveform.shape[0] > 1:\n",
        "            waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
        "\n",
        "        # Resample to 16000 Hz if necessary\n",
        "        if sample_rate != 16000:\n",
        "            resampler = Resample(orig_freq=sample_rate, new_freq=16000)\n",
        "            waveform = resampler(waveform)\n",
        "\n",
        "        # Extract 1-second segment\n",
        "        start_sample = int(row['start'] * 16000)\n",
        "        end_sample = int(row['end'] * 16000)\n",
        "        waveform = waveform[:, start_sample:end_sample]\n",
        "\n",
        "        # Padding if needed\n",
        "        if waveform.shape[1] < 16000:\n",
        "            num_padding = 16000 - waveform.shape[1]\n",
        "            waveform = torch.cat([waveform, torch.zeros(1, num_padding)], dim=1)\n",
        "\n",
        "        # If the waveform is a tensor, this squeeze operation will work\n",
        "        return waveform, row['filename'], row['segment_idx']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "xiV6Xp5nDYnh"
      },
      "outputs": [],
      "source": [
        "test_csv = pd.read_csv(f'{base_path}data/test.csv')\n",
        "\n",
        "def collate_fn(batch):\n",
        "    segments = [item[0] for item in batch]  # Ensure item[0] is a tensor\n",
        "    filenames = [item[1] for item in batch]\n",
        "    segment_indices = [item[2] for item in batch]\n",
        "\n",
        "    segments_tensor = torch.stack(segments, dim=0)  # Stack the segment tensors\n",
        "\n",
        "    return segments_tensor, filenames, segment_indices\n",
        "\n",
        "\n",
        "test_dataset = BirdSongTestDataset(test_csv, f'{base_path}data/test/', transform=valid_transform)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "lMbYcgyUHnY6"
      },
      "outputs": [],
      "source": [
        "# for batch in test_loader:\n",
        "#     print(type(batch))\n",
        "#     break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "AxzJ8JxeUPbv"
      },
      "outputs": [],
      "source": [
        "# if DEBUG:\n",
        "#   (sample, intermediates), filename, label = test_dataset[25]\n",
        "#   print(filename)\n",
        "#   print(sample.shape)\n",
        "#   visualize_intermediates(intermediates)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "Gc77_1e9UOiz"
      },
      "outputs": [],
      "source": [
        "# Make predictions on test set\n",
        "model.eval()\n",
        "predictions = defaultdict(lambda: np.zeros(len(class_names), dtype=bool))\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, filenames, segment_indices in test_loader:\n",
        "        # Process through feature extractor and to device\n",
        "        inputs = feature_extractor(inputs.squeeze(1).numpy(), sampling_rate=16000, return_tensors=\"pt\", padding=True)\n",
        "\n",
        "\n",
        "        # Ensure the input shape is compatible with the model\n",
        "        if inputs.input_values.shape[-1] < 16:\n",
        "            padding = 16 - inputs.input_values.shape[-1]\n",
        "            inputs.input_values = F.pad(inputs.input_values, (0, padding), 'constant', 0)\n",
        "\n",
        "        inputs = inputs.to(device)\n",
        "\n",
        "        # Forward pass through the model\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "        # Extract logits and apply sigmoid\n",
        "        logits = outputs.logits\n",
        "        probabilities = torch.sigmoid(logits)\n",
        "\n",
        "        # Compare with threshold\n",
        "        preds = (probabilities > best_threshold_val).float().cpu().numpy().astype(bool)\n",
        "\n",
        "        # Aggregate predictions for each audio file\n",
        "        for fname, pred in zip(filenames, preds):\n",
        "            predictions[fname] = np.logical_or(predictions[fname], pred)\n",
        "\n",
        "# Convert boolean values to integer (0 or 1)\n",
        "for key in predictions:\n",
        "    predictions[key] = predictions[key].astype(int)\n",
        "\n",
        "submission_df = pd.DataFrame.from_dict(predictions, orient='index', columns=class_names)\n",
        "submission_df.reset_index(inplace=True)\n",
        "submission_df.rename(columns={'index': 'filename'}, inplace=True)\n",
        "submission_df.to_csv('submission.csv', index=False)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0532f01962824494a659d4011625b537": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "131da14eaf69452d8fb0bd306e107ef0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "17dbc949472b473e99806bc15b503d6f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a9b4413a8bb24d279e819d0d23df12d1",
              "IPY_MODEL_93a8cc7a91a24dc48d64e1816fc9d5b2",
              "IPY_MODEL_cffd7cafd8db417c97a10bc4767b4bac"
            ],
            "layout": "IPY_MODEL_70240c7f8f9d4f4b876d31c8c4f83721"
          }
        },
        "262d21afce3f41e7964e876e5af55674": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3f837cbf74794df8b3aeaca22642e633": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_814ff308d4a343818e822d13a0f038ca",
            "placeholder": "​",
            "style": "IPY_MODEL_5bc707a882e14be19b095b0185c1c9e9",
            "value": "model.safetensors: 100%"
          }
        },
        "416182a2386d4090b0f63c885e1d446a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_131da14eaf69452d8fb0bd306e107ef0",
            "placeholder": "​",
            "style": "IPY_MODEL_53a007432f854f6a961326388600c894",
            "value": "config.json: 100%"
          }
        },
        "53a007432f854f6a961326388600c894": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5791d8e238c943b9834c00e846bf444f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a61bfaa25ccb4b6ea0a344cf48af8a43",
            "placeholder": "​",
            "style": "IPY_MODEL_857770f5f2254fc48a98fa8df0fdb70b",
            "value": " 26.8k/26.8k [00:00&lt;00:00, 504kB/s]"
          }
        },
        "596b7b60353f44ffadb06128081d8087": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_977c2bb85f9a432a81abdffb58e7e14a",
            "placeholder": "​",
            "style": "IPY_MODEL_60921ee726794f94bb592cb7c9526283",
            "value": " 346M/346M [00:01&lt;00:00, 302MB/s]"
          }
        },
        "5bc707a882e14be19b095b0185c1c9e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5cb21c09058043ee9daa35d349ae046f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5e1400b6c6754a5ebe9aed611d7b2d3d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_416182a2386d4090b0f63c885e1d446a",
              "IPY_MODEL_9364225b595d43c188470de773b00027",
              "IPY_MODEL_5791d8e238c943b9834c00e846bf444f"
            ],
            "layout": "IPY_MODEL_9399e36c52ba470a9b5b7de4567a0de7"
          }
        },
        "60921ee726794f94bb592cb7c9526283": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6d83e503df73440399c31fb96490e584": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "70240c7f8f9d4f4b876d31c8c4f83721": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7b0cd102b3c34316a329471c8ae58f7d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_262d21afce3f41e7964e876e5af55674",
            "max": 346404948,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_984ed1249d61470387193a1e62ada4ac",
            "value": 346404948
          }
        },
        "814ff308d4a343818e822d13a0f038ca": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "81ba194876fa4e2c9d78a6065edb1f6a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "857770f5f2254fc48a98fa8df0fdb70b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9364225b595d43c188470de773b00027": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5cb21c09058043ee9daa35d349ae046f",
            "max": 26763,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0532f01962824494a659d4011625b537",
            "value": 26763
          }
        },
        "9399e36c52ba470a9b5b7de4567a0de7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "93a8cc7a91a24dc48d64e1816fc9d5b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a7286590c8514fa1808e0bb4095deda7",
            "max": 297,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d711bd2d8054406da64e61f66eab0070",
            "value": 297
          }
        },
        "977c2bb85f9a432a81abdffb58e7e14a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "984ed1249d61470387193a1e62ada4ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a03b99d89eb74878bb48161f113ef74b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3f837cbf74794df8b3aeaca22642e633",
              "IPY_MODEL_7b0cd102b3c34316a329471c8ae58f7d",
              "IPY_MODEL_596b7b60353f44ffadb06128081d8087"
            ],
            "layout": "IPY_MODEL_81ba194876fa4e2c9d78a6065edb1f6a"
          }
        },
        "a0d1454da85a42da90e4e9fb5ca3140c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a31c64e6fceb45189787230e1bd21313": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a61bfaa25ccb4b6ea0a344cf48af8a43": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a7286590c8514fa1808e0bb4095deda7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a9b4413a8bb24d279e819d0d23df12d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a31c64e6fceb45189787230e1bd21313",
            "placeholder": "​",
            "style": "IPY_MODEL_6d83e503df73440399c31fb96490e584",
            "value": "preprocessor_config.json: 100%"
          }
        },
        "c0d933a8ec4349c09f51e5c7783e216d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cffd7cafd8db417c97a10bc4767b4bac": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c0d933a8ec4349c09f51e5c7783e216d",
            "placeholder": "​",
            "style": "IPY_MODEL_a0d1454da85a42da90e4e9fb5ca3140c",
            "value": " 297/297 [00:00&lt;00:00, 8.11kB/s]"
          }
        },
        "d711bd2d8054406da64e61f66eab0070": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
