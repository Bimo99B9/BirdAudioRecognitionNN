{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "za_-0XklmBeX"
      },
      "source": [
        "# Aprendizaje Profundo\n",
        "Daniel López Gala - UO281798\n",
        "\n",
        "Se dispone del conjunto de datos NIPS4BPLUS, el cual contiene 674 ficheros de audio con una duración total de menos de una hora. En estos audios podemos encontrar grabaciones de aproximadamente 5 segundos con cantos de pájaros realizadas en 39 localizaciones diferentes repartidas por 7 regiones de Francia y España."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kISgPuot6Sx_"
      },
      "source": [
        "# Aprendizaje Profundo\n",
        "Daniel López Gala - UO281798\n",
        "\n",
        "Se dispone del conjunto de datos NIPS4BPLUS, el cual contiene 674 ficheros de audio con una duración total de menos de una hora. En estos audios podemos encontrar grabaciones de aproximadamente 5 segundos con cantos de pájaros realizadas en 39 localizaciones diferentes repartidas por 7 regiones de Francia y España."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "urB-A95SsJri"
      },
      "outputs": [],
      "source": [
        "base_path = \"/content/drive/MyDrive/DeepLearning/\"\n",
        "#base_path = \"\"\n",
        "DEBUG = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z8PtqgSelI8r",
        "outputId": "901ebc59-ee66-4c04-e4c7-a0ac3a7280e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "62qWTY1kk7e6",
        "outputId": "b3400425-2c29-4b9d-b246-4d3a849433f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.35.0-py3-none-any.whl (7.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.9/7.9 MB\u001b[0m \u001b[31m31.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers)\n",
            "  Downloading huggingface_hub-0.19.0-py3-none-any.whl (311 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.2/311.2 kB\u001b[0m \u001b[31m27.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Collecting tokenizers<0.15,>=0.14 (from transformers)\n",
            "  Downloading tokenizers-0.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m75.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m49.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers)\n",
            "  Downloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m32.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Installing collected packages: safetensors, huggingface-hub, tokenizers, transformers\n",
            "Successfully installed huggingface-hub-0.17.3 safetensors-0.4.0 tokenizers-0.14.1 transformers-4.35.0\n",
            "Collecting scikit-multilearn\n",
            "  Downloading scikit_multilearn-0.2.0-py3-none-any.whl (89 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.4/89.4 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: scikit-multilearn\n",
            "Successfully installed scikit-multilearn-0.2.0\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import random\n",
        "from collections import defaultdict\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "\n",
        "import torchaudio\n",
        "import torchaudio.transforms as T\n",
        "from torchaudio.transforms import Resample\n",
        "import librosa\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau, CosineAnnealingLR\n",
        "import torchvision.models as models\n",
        "\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "!pip install transformers\n",
        "!pip install scikit-multilearn\n",
        "from skmultilearn.model_selection import iterative_train_test_split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fwU9YFOunaak"
      },
      "source": [
        "## Preprocesamiento y visualización\n",
        "\n",
        "- Se define una función `visualize_intermediates` para crear imágenes de los pasos intermedios usados en el preprocesamiento de los audios.\n",
        "\n",
        "- La clase `AudioPreprocessing` define los pasos para procesar la imagen. Se incluyen:\n",
        "  - Resample (No usado)\n",
        "  - STFT (Convertir a espectrograma)\n",
        "  - Normalización\n",
        "  - Median clipping\n",
        "  - Conectar puntos cercanos mediante filtros\n",
        "  - Closing\n",
        "  - Dilation\n",
        "  - Median blur\n",
        "  - Eliminar residuos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xKj6CGbCH2RX"
      },
      "outputs": [],
      "source": [
        "def visualize_intermediates(intermediates, sample_rate=44100, hop_length=196):\n",
        "\n",
        "    # Set default background color for figures to white\n",
        "    plt.rcParams['figure.facecolor'] = 'white'\n",
        "\n",
        "    for key, value in intermediates.items():\n",
        "        if len(value.shape) == 2 and value.shape[1] > 2:  # This indicates a waveform\n",
        "            plt.figure(figsize=(12, 4))\n",
        "\n",
        "            # Calculate time axis in seconds for waveform\n",
        "            time_axis_waveform = np.linspace(0, value.shape[1] / sample_rate, value.shape[1])\n",
        "\n",
        "            plt.plot(time_axis_waveform, value[0].cpu().numpy())\n",
        "            plt.xlabel(\"Time (seconds)\")\n",
        "            plt.title(f\"{key}\")\n",
        "            plt.show()\n",
        "            continue\n",
        "\n",
        "        print(f\"Processing {key} with shape {value.shape}\")\n",
        "\n",
        "        if value.dim() == 4 and value.shape[-1] == 2:\n",
        "            complex_representation = value[0, ..., 0] + 1j * value[0, ..., 1]  # Convert to complex\n",
        "            magnitude = torch.abs(complex_representation).cpu().numpy()\n",
        "            phase = torch.angle(complex_representation).cpu().numpy()\n",
        "        elif value.is_complex():\n",
        "            magnitude = torch.abs(value).squeeze().cpu().numpy()\n",
        "            phase = torch.angle(value).squeeze().cpu().numpy()\n",
        "        else:\n",
        "            magnitude = value.squeeze().cpu().numpy()\n",
        "            phase = None\n",
        "\n",
        "        # Calculate time axis in seconds for magnitude\n",
        "        time_axis_magnitude = np.linspace(0, magnitude.shape[1] * hop_length / sample_rate, magnitude.shape[1])\n",
        "\n",
        "        # Plot magnitude with inverted grayscale colormap\n",
        "        plt.figure(figsize=(12, 4))\n",
        "        plt.imshow(magnitude, cmap='gray_r', aspect='auto', origin='lower', extent=[time_axis_magnitude[0], time_axis_magnitude[-1], 0, magnitude.shape[0]])\n",
        "        plt.xlabel(\"Time (seconds)\")\n",
        "        plt.title(f\"{key} Magnitude\")\n",
        "        plt.colorbar()\n",
        "        plt.show()\n",
        "\n",
        "        # Plot phase\n",
        "        if phase is not None:\n",
        "            plt.figure(figsize=(12, 4))\n",
        "            plt.imshow(((phase + np.pi) % (2 * np.pi) - np.pi), cmap='hsv', aspect='auto', origin='lower', vmin=-np.pi, vmax=np.pi, extent=[time_axis_magnitude[0], time_axis_magnitude[-1], 0, phase.shape[0]])\n",
        "            plt.xlabel(\"Time (seconds)\")\n",
        "            plt.title(f\"{key} Phase\")\n",
        "            plt.colorbar()\n",
        "            plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jFTKwdi0v96g"
      },
      "outputs": [],
      "source": [
        "class AudioPreprocessing(nn.Module):\n",
        "    def __init__(self, debug=DEBUG, sample_rate=16000, n_fft=1024, win_length=1024, hop_length=196, augment=False):\n",
        "        super().__init__()\n",
        "        self.debug = debug\n",
        "        self.augment = augment\n",
        "        self.sample_rate = sample_rate\n",
        "        self.resampler = T.Resample(44100, sample_rate)\n",
        "        self.spectrogram = T.MelSpectrogram(sample_rate, n_fft=n_fft, win_length=win_length, hop_length=hop_length, f_min=500, f_max=15000)\n",
        "\n",
        "    def normalize(self, spectrogram):\n",
        "        min_val = torch.min(spectrogram)\n",
        "        return (spectrogram - min_val) / (torch.max(spectrogram) - min_val + 1e-5)\n",
        "\n",
        "    def median_blurring(self, spectrogram):\n",
        "        img = spectrogram.squeeze(0).cpu().numpy()\n",
        "        img = cv2.medianBlur(img.astype(np.float32), 5)\n",
        "        return torch.tensor(img, device=spectrogram.device).float().unsqueeze(0)\n",
        "\n",
        "    def binary_image_creation(self, spectrogram, threshold=1.5):\n",
        "        freq_median = torch.median(spectrogram, dim=2, keepdim=True).values\n",
        "        time_median = torch.median(spectrogram, dim=1, keepdim=True).values\n",
        "        mask = (spectrogram > threshold * freq_median) & (spectrogram > threshold * time_median)\n",
        "        return mask.float()\n",
        "\n",
        "    ## fastNlMeansDenoising works better but is too much cpu expensive and it bottlenecks the GPU on training\n",
        "    # def spot_removal(self, spectrogram):\n",
        "    #     img = spectrogram.squeeze(0).cpu().numpy()\n",
        "    #     img = cv2.fastNlMeansDenoising(img.astype(np.uint8),None,31,7,21)\n",
        "    #     return torch.tensor(img, device=spectrogram.device).float().unsqueeze(0)\n",
        "\n",
        "    def spot_removal(self, spectrogram, threshold=0.5):\n",
        "        # Threshold the spectrogram to get a binary mask\n",
        "        binary_mask = (spectrogram > threshold).float()\n",
        "\n",
        "        # Convert to numpy for morphological operations\n",
        "        binary_np = binary_mask.squeeze(0).cpu().numpy().astype(np.uint8)\n",
        "\n",
        "        # Define a kernel for morphological operations (adjust size as needed)\n",
        "        kernel = np.ones((3, 3), np.uint8)\n",
        "\n",
        "        # Perform morphological opening to remove small noise\n",
        "        cleaned_binary_np = cv2.morphologyEx(binary_np, cv2.MORPH_OPEN, kernel)\n",
        "\n",
        "        # Convert back to tensor\n",
        "        cleaned_binary_mask = torch.tensor(cleaned_binary_np, device=spectrogram.device).float().unsqueeze(0)\n",
        "\n",
        "        # Multiply with original spectrogram to remove the noise\n",
        "        denoised_spectrogram = spectrogram * cleaned_binary_mask\n",
        "\n",
        "        return denoised_spectrogram\n",
        "\n",
        "    def morph_closing(self, spectrogram):\n",
        "        img = spectrogram.squeeze(0).cpu().numpy()\n",
        "        kernel = np.ones((3, 3), np.uint8)\n",
        "        img = cv2.morphologyEx(img, cv2.MORPH_CLOSE, kernel)\n",
        "        return torch.tensor(img, device=spectrogram.device).float().unsqueeze(0)\n",
        "\n",
        "    def random_time_shift(self, waveform, max_shift_sec=0.2):\n",
        "        \"\"\" Randomly shifts the waveform in the time domain \"\"\"\n",
        "        max_shift = int(max_shift_sec * self.sample_rate)\n",
        "        shift = random.randint(-max_shift, max_shift)\n",
        "        return torch.roll(waveform, shifts=shift, dims=-1)\n",
        "\n",
        "    def random_pitch_shift(self, waveform, max_shift=2):\n",
        "        \"\"\" Randomly shifts the pitch of the waveform \"\"\"\n",
        "        shift = random.uniform(-max_shift, max_shift)\n",
        "        return T.FrequencyMasking(freq_mask_param=int(shift))(waveform)\n",
        "\n",
        "    def random_volume_gain(self, waveform, min_gain=0.5, max_gain=1.5):\n",
        "        \"\"\" Randomly changes the volume of the waveform \"\"\"\n",
        "        gain = random.uniform(min_gain, max_gain)\n",
        "        return waveform * gain\n",
        "\n",
        "    def random_noise_injection(self, waveform, noise_level=0.005):\n",
        "        \"\"\" Adds random noise to the waveform \"\"\"\n",
        "        noise = torch.randn_like(waveform) * noise_level\n",
        "        return waveform + noise\n",
        "\n",
        "    def forward(self, waveform):\n",
        "        intermediates = {}\n",
        "\n",
        "        # Data augmentation\n",
        "        if self.augment:\n",
        "            waveform = self.random_time_shift(waveform)\n",
        "            waveform = self.random_pitch_shift(waveform)\n",
        "            waveform = self.random_volume_gain(waveform)\n",
        "            # waveform = self.random_noise_injection(waveform)\n",
        "\n",
        "        # Resampling to the target sample rate\n",
        "        waveform = self.resampler(waveform)\n",
        "\n",
        "        # Convert stereo to mono if necessary by averaging the channels\n",
        "        if waveform.shape[0] > 1:\n",
        "            waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
        "\n",
        "        spectrogram = self.spectrogram(waveform)\n",
        "        if self.debug: intermediates['original_spectrograms'] = spectrogram\n",
        "\n",
        "        spectrogram = self.normalize(spectrogram)\n",
        "        spectrogram = self.median_blurring(spectrogram)\n",
        "        if self.debug: intermediates['spectrograms_after_median_blurring'] = spectrogram\n",
        "\n",
        "        spectrogram = self.binary_image_creation(spectrogram)\n",
        "        if self.debug: intermediates['binary_image'] = spectrogram\n",
        "\n",
        "        spectrogram = self.spot_removal(spectrogram)\n",
        "        if self.debug: intermediates['spectrograms_after_spot_removal'] = spectrogram\n",
        "\n",
        "        spectrogram = self.morph_closing(spectrogram)\n",
        "        if self.debug: intermediates['spectrograms_after_morph_closing'] = spectrogram\n",
        "\n",
        "        if not self.debug:\n",
        "            return spectrogram, {}\n",
        "\n",
        "        return (spectrogram, intermediates)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95KHcfMgocTZ"
      },
      "source": [
        "## Carga de datos\n",
        "\n",
        "Se leen los audios de forma individual. Cada audio es un objeto. `BirdSongDataset` define el método `__getitem__` para obtener cada instancia del dataset.\n",
        "\n",
        "No se tiene en cuenta en qué momento del audio suena cada pájaro, tan sólo qué pájaros suenan en cada audio. El problema se plantea como **clasificación multietiqueta**.\n",
        "\n",
        "El método `get_class_proportions` se utiliza para comprobar que los datasets *train* y *validation* contienen la misma proporción de clases, es decir, están estratíficados."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Og-dzxQhKdMs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ab7d6fc-f359-49cd-9287-c51222657ea8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchaudio/functional/functional.py:584: UserWarning: At least one mel filterbank has all zero values. The value for `n_mels` (128) may be set too high. Or, the value for `n_freqs` (513) may be set too low.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# class BirdSongDataset(Dataset):\n",
        "#     def __init__(self, df, audio_dir, class_info, transform=None):\n",
        "#         segments = []\n",
        "\n",
        "#         unique_filenames = df['filename'].unique()  # Only process each audio file once\n",
        "#         for unique_filename in unique_filenames:\n",
        "#             audio_path = os.path.join(audio_dir, unique_filename)\n",
        "#             waveform, sample_rate = torchaudio.load(audio_path)\n",
        "#             total_segments = int(math.ceil(waveform.shape[1] / sample_rate))  # Total segments in the audio\n",
        "\n",
        "#             # Calculate the unique labels for each segment\n",
        "#             for idx in range(total_segments):\n",
        "#                 start_time, end_time = idx, idx + 1\n",
        "#                 labels_in_segment = df[(df['filename'] == unique_filename) &\n",
        "#                                        (df['end'] > start_time) &\n",
        "#                                        (df['start'] < end_time)]['class'].unique().tolist()\n",
        "#                 segments.append({\n",
        "#                     'filename': unique_filename,\n",
        "#                     'segment_idx': idx,\n",
        "#                     'start': start_time,\n",
        "#                     'end': end_time,\n",
        "#                     'class': \",\".join(labels_in_segment)\n",
        "#                 })\n",
        "\n",
        "#         self.segments = pd.DataFrame(segments)\n",
        "#         self.audio_dir = audio_dir\n",
        "#         self.class_info = class_info\n",
        "#         self.transform = transform\n",
        "\n",
        "#     def __len__(self):\n",
        "#         return len(self.segments)\n",
        "\n",
        "#     def __getitem__(self, idx):\n",
        "#         row = self.segments.iloc[idx]\n",
        "#         audio_path = os.path.join(self.audio_dir, row['filename'])\n",
        "#         waveform, sample_rate = torchaudio.load(audio_path)\n",
        "\n",
        "#         # Extract 1-second segment\n",
        "#         start_sample = int(row['start'] * sample_rate)\n",
        "#         end_sample = int(row['end'] * sample_rate)\n",
        "#         waveform = waveform[:, start_sample:end_sample]\n",
        "\n",
        "#         # Padding if needed\n",
        "#         if waveform.shape[1] < sample_rate:\n",
        "#             num_padding = sample_rate - waveform.shape[1]\n",
        "#             waveform = torch.cat([waveform, torch.zeros(1, num_padding)], dim=1)\n",
        "\n",
        "#         class_names = row['class'].split(\",\") if row['class'] else []\n",
        "#         target = torch.zeros(len(self.class_info))\n",
        "#         for class_name in class_names:\n",
        "#             target[self.class_info.index(class_name)] = 1.0\n",
        "\n",
        "#         if self.transform:\n",
        "#             waveform = self.transform(waveform)\n",
        "\n",
        "#         return waveform, target\n",
        "\n",
        "#     def get_filename(self, idx):\n",
        "#         return self.segments.iloc[idx]['filename']\n",
        "\n",
        "class BirdSongDataset(Dataset):\n",
        "    def __init__(self, df, audio_dir, class_info, transform=None):\n",
        "        segments = []\n",
        "\n",
        "        unique_filenames = df['filename'].unique()  # Only process each audio file once\n",
        "        for unique_filename in unique_filenames:\n",
        "            audio_path = os.path.join(audio_dir, unique_filename)\n",
        "            waveform, sample_rate = torchaudio.load(audio_path)\n",
        "            total_segments = int(math.ceil(waveform.shape[1] / sample_rate))  # Total segments in the audio\n",
        "\n",
        "            # Calculate the unique labels for each segment\n",
        "            for idx in range(total_segments):\n",
        "                start_time, end_time = idx, idx + 1\n",
        "                labels_in_segment = df[(df['filename'] == unique_filename) &\n",
        "                                       (df['end'] > start_time) &\n",
        "                                       (df['start'] < end_time)]['class'].unique().tolist()\n",
        "                segments.append({\n",
        "                    'filename': unique_filename,\n",
        "                    'segment_idx': idx,\n",
        "                    'start': start_time,\n",
        "                    'end': end_time,\n",
        "                    'class': \",\".join(labels_in_segment)\n",
        "                })\n",
        "\n",
        "        self.segments = pd.DataFrame(segments)\n",
        "        self.audio_dir = audio_dir\n",
        "        self.class_info = class_info\n",
        "        # Store the transform, but we will not apply it in __getitem__\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.segments)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.segments.iloc[idx]\n",
        "        audio_path = os.path.join(self.audio_dir, row['filename'])\n",
        "        waveform, sample_rate = torchaudio.load(audio_path)\n",
        "\n",
        "        # Ensure waveform is mono\n",
        "        if waveform.shape[0] > 1:\n",
        "            waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
        "\n",
        "        # Resample from 44100 Hz to 16000 Hz if necessary\n",
        "        if sample_rate != 16000:\n",
        "            resampler = Resample(orig_freq=sample_rate, new_freq=16000)\n",
        "            waveform = resampler(waveform)\n",
        "\n",
        "        # Extract 1-second segment\n",
        "        start_sample = int(row['start'] * 16000)  # Use the new sample rate here\n",
        "        end_sample = int(row['end'] * 16000)      # Use the new sample rate here\n",
        "        waveform = waveform[:, start_sample:end_sample]\n",
        "\n",
        "        # Padding if needed\n",
        "        if waveform.shape[1] < 16000:  # Use the new sample rate here\n",
        "            num_padding = 16000 - waveform.shape[1]  # Use the new sample rate here\n",
        "            waveform = torch.cat([waveform, torch.zeros(1, num_padding)], dim=1)\n",
        "\n",
        "        class_names = row['class'].split(\",\") if row['class'] else []\n",
        "        target = torch.zeros(len(self.class_info))\n",
        "        for class_name in class_names:\n",
        "            target[self.class_info.index(class_name)] = 1.0\n",
        "\n",
        "        return waveform, target\n",
        "\n",
        "    def get_filename(self, idx):\n",
        "        return self.segments.iloc[idx]['filename']\n",
        "\n",
        "train_csv = pd.read_csv(f'{base_path}data/train.csv') # CSV with train audio filenames, and bird class names labels.\n",
        "class_info_csv = pd.read_csv(f'{base_path}data/class_info.csv')\n",
        "class_names = class_info_csv['class name'].tolist()\n",
        "\n",
        "# Convert the labels to a binary matrix form\n",
        "y = np.zeros((len(train_csv), len(class_names)))\n",
        "for i, (_, row) in enumerate(train_csv.iterrows()):\n",
        "    labels = row['class'].split(\",\")\n",
        "    for label in labels:\n",
        "        y[i, class_names.index(label)] = 1\n",
        "\n",
        "X_train, y_train, X_val, y_val = iterative_train_test_split(np.array(train_csv), y, test_size=0.2)\n",
        "\n",
        "train_df = pd.DataFrame(X_train, columns=train_csv.columns)\n",
        "valid_df = pd.DataFrame(X_val, columns=train_csv.columns)\n",
        "\n",
        "train_transform = nn.Sequential(\n",
        "    AudioPreprocessing(augment=True)\n",
        ")\n",
        "\n",
        "valid_transform = nn.Sequential(\n",
        "    AudioPreprocessing(augment=False)\n",
        ")\n",
        "\n",
        "train_dataset = BirdSongDataset(train_df, f'{base_path}data/train/', class_names, transform=train_transform)\n",
        "valid_dataset = BirdSongDataset(valid_df, f'{base_path}data/train/', class_names, transform=valid_transform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DBrPrvtp-rIt"
      },
      "outputs": [],
      "source": [
        "def aggregate_predictions(predictions, segments_df):\n",
        "    aggregated_predictions = {}\n",
        "    for filename in segments_df['filename'].unique():\n",
        "        aggregated_predictions[filename] = set()\n",
        "        segments = segments_df[segments_df['filename'] == filename]\n",
        "        for idx, row in segments.iterrows():\n",
        "            aggregated_predictions[filename].update(predictions[idx])\n",
        "    return aggregated_predictions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_FVZvTwP-rIt",
        "outputId": "0c6a03a1-1814-4e1d-9282-f3b3fcc81a80"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              filename  segment_idx  start  end                    class\n",
            "0  nips4b_train001.wav            0      0    1              Sylcan_song\n",
            "1  nips4b_train001.wav            1      1    2  Sylcan_song,Petpet_song\n",
            "2  nips4b_train001.wav            2      2    3  Petpet_song,Sylcan_song\n",
            "3  nips4b_train001.wav            3      3    4              Petpet_song\n",
            "4  nips4b_train001.wav            4      4    5                         \n",
            "5  nips4b_train001.wav            5      5    6                         \n"
          ]
        }
      ],
      "source": [
        "specific_audio_segments = train_dataset.segments[train_dataset.segments['filename'] == 'nips4b_train001.wav']\n",
        "print(specific_audio_segments)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wOJ3S-BI-rIu",
        "outputId": "1e77d1b9-c6f2-4fcf-b766-4e485d77f0bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Predicted Classes: ['Petpet_song', 'Sylcan_song']\n"
          ]
        }
      ],
      "source": [
        "segment_idx = 2\n",
        "waveform, label = train_dataset[segment_idx]\n",
        "print(f\"Label: {label}\")\n",
        "\n",
        "# Convert tensor label back to class names to check\n",
        "predicted_classes = [class_name for idx, class_name in enumerate(class_names) if label[idx] == 1.0]\n",
        "print(\"Predicted Classes:\", predicted_classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FhnnarsNJSUg",
        "outputId": "8fce4697-b4c8-4fde-9947-9b8c118b0417"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class Proportions in Training Dataset:\n",
            "Aegcau_call: 0.60%\n",
            "Alaarv_song: 2.86%\n",
            "Anttri_song: 2.23%\n",
            "Butbut_call: 0.38%\n",
            "Carcan_call: 1.25%\n",
            "Carcan_song: 1.72%\n",
            "Carcar_call: 1.58%\n",
            "Carcar_song: 2.67%\n",
            "Cerbra_call: 0.54%\n",
            "Cerbra_song: 0.35%\n",
            "Cetcet_song: 2.72%\n",
            "Chlchl_call: 0.38%\n",
            "Cicatr_song: 0.16%\n",
            "Cicorn_song: 0.19%\n",
            "Cisjun_song: 0.49%\n",
            "Colpal_song: 0.82%\n",
            "Corcor_call: 0.38%\n",
            "Denmaj_call: 0.49%\n",
            "Denmaj_drum: 0.38%\n",
            "Embcir_call: 0.71%\n",
            "Embcir_song: 0.93%\n",
            "Erirub_call: 0.79%\n",
            "Erirub_song: 1.55%\n",
            "Fricoe_call: 0.44%\n",
            "Fricoe_song: 1.14%\n",
            "Galcri_call: 0.82%\n",
            "Galcri_song: 0.87%\n",
            "Galthe_call: 0.27%\n",
            "Galthe_song: 2.51%\n",
            "Gargla_call: 0.27%\n",
            "Hirrus_call: 0.35%\n",
            "Jyntor_song: 0.19%\n",
            "Lopcri_call: 0.93%\n",
            "Loxcur_call: 1.44%\n",
            "Lularb_song: 3.81%\n",
            "Lusmeg_call: 0.74%\n",
            "Lusmeg_song: 1.91%\n",
            "Lyrple_song: 0.27%\n",
            "Motcin_call: 1.25%\n",
            "Musstr_call: 0.38%\n",
            "Noise: 1.82%\n",
            "Oriori_call: 0.27%\n",
            "Oriori_song: 0.87%\n",
            "Parate_call: 0.65%\n",
            "Parate_song: 1.91%\n",
            "Parcae_call: 1.63%\n",
            "Parcae_song: 1.42%\n",
            "Parmaj_call: 0.76%\n",
            "Parmaj_song: 2.45%\n",
            "Pasdom_call: 1.36%\n",
            "Pelgra_call: 0.49%\n",
            "Petpet_call: 0.82%\n",
            "Petpet_song: 0.87%\n",
            "Phofem_song: 0.82%\n",
            "Phycol_call: 0.19%\n",
            "Phycol_song: 0.93%\n",
            "Picpic_call: 0.82%\n",
            "Plaaff_song: 0.27%\n",
            "Plasab_song: 0.19%\n",
            "Poepal_call: 0.54%\n",
            "Poepal_song: 0.68%\n",
            "Prumod_song: 0.87%\n",
            "Ptehey_song: 0.41%\n",
            "Pyrpyr_call: 0.38%\n",
            "Regign_call: 0.63%\n",
            "Regign_song: 1.74%\n",
            "Serser_call: 0.44%\n",
            "Serser_song: 0.63%\n",
            "Siteur_call: 0.38%\n",
            "Siteur_song: 0.57%\n",
            "Strdec_song: 0.54%\n",
            "Strtur_song: 0.46%\n",
            "Stuvul_call: 0.27%\n",
            "Sylatr_call: 1.82%\n",
            "Sylatr_song: 1.20%\n",
            "Sylcan_call: 2.23%\n",
            "Sylcan_song: 4.38%\n",
            "Sylmel_call: 3.92%\n",
            "Sylmel_song: 2.83%\n",
            "Sylund_call: 0.49%\n",
            "Sylund_song: 4.17%\n",
            "Tetpyg_song: 0.60%\n",
            "Tibtom_song: 0.22%\n",
            "Trotro_song: 1.91%\n",
            "Turmer_call: 0.87%\n",
            "Turmer_song: 0.30%\n",
            "Turphi_call: 0.27%\n",
            "Turphi_song: 1.69%\n",
            "Unknown: 4.22%\n",
            "\n",
            "Class Proportions in Validation Dataset:\n",
            "Aegcau_call: 0.66%\n",
            "Alaarv_song: 2.84%\n",
            "Anttri_song: 2.29%\n",
            "Butbut_call: 0.33%\n",
            "Carcan_call: 1.31%\n",
            "Carcan_song: 1.75%\n",
            "Carcar_call: 1.53%\n",
            "Carcar_song: 2.73%\n",
            "Cerbra_call: 0.55%\n",
            "Cerbra_song: 0.33%\n",
            "Cetcet_song: 2.73%\n",
            "Chlchl_call: 0.33%\n",
            "Cicatr_song: 0.11%\n",
            "Cicorn_song: 0.22%\n",
            "Cisjun_song: 0.44%\n",
            "Colpal_song: 0.87%\n",
            "Corcor_call: 0.33%\n",
            "Denmaj_call: 0.44%\n",
            "Denmaj_drum: 0.44%\n",
            "Embcir_call: 0.76%\n",
            "Embcir_song: 0.87%\n",
            "Erirub_call: 0.76%\n",
            "Erirub_song: 1.53%\n",
            "Fricoe_call: 0.44%\n",
            "Fricoe_song: 1.20%\n",
            "Galcri_call: 0.76%\n",
            "Galcri_song: 0.87%\n",
            "Galthe_call: 0.22%\n",
            "Galthe_song: 2.51%\n",
            "Gargla_call: 0.22%\n",
            "Hirrus_call: 0.33%\n",
            "Jyntor_song: 0.22%\n",
            "Lopcri_call: 0.87%\n",
            "Loxcur_call: 1.42%\n",
            "Lularb_song: 3.82%\n",
            "Lusmeg_call: 0.76%\n",
            "Lusmeg_song: 1.97%\n",
            "Lyrple_song: 0.33%\n",
            "Motcin_call: 1.20%\n",
            "Musstr_call: 0.33%\n",
            "Noise: 1.86%\n",
            "Oriori_call: 0.33%\n",
            "Oriori_song: 0.87%\n",
            "Parate_call: 0.66%\n",
            "Parate_song: 1.86%\n",
            "Parcae_call: 1.64%\n",
            "Parcae_song: 1.42%\n",
            "Parmaj_call: 0.76%\n",
            "Parmaj_song: 2.40%\n",
            "Pasdom_call: 1.31%\n",
            "Pelgra_call: 0.55%\n",
            "Petpet_call: 0.76%\n",
            "Petpet_song: 0.87%\n",
            "Phofem_song: 0.76%\n",
            "Phycol_call: 0.22%\n",
            "Phycol_song: 0.98%\n",
            "Picpic_call: 0.87%\n",
            "Plaaff_song: 0.22%\n",
            "Plasab_song: 0.22%\n",
            "Poepal_call: 0.55%\n",
            "Poepal_song: 0.66%\n",
            "Prumod_song: 0.87%\n",
            "Ptehey_song: 0.44%\n",
            "Pyrpyr_call: 0.33%\n",
            "Regign_call: 0.66%\n",
            "Regign_song: 1.75%\n",
            "Serser_call: 0.44%\n",
            "Serser_song: 0.66%\n",
            "Siteur_call: 0.44%\n",
            "Siteur_song: 0.55%\n",
            "Strdec_song: 0.55%\n",
            "Strtur_song: 0.44%\n",
            "Stuvul_call: 0.22%\n",
            "Sylatr_call: 1.86%\n",
            "Sylatr_song: 1.20%\n",
            "Sylcan_call: 2.29%\n",
            "Sylcan_song: 4.37%\n",
            "Sylmel_call: 3.93%\n",
            "Sylmel_song: 2.84%\n",
            "Sylund_call: 0.55%\n",
            "Sylund_song: 4.15%\n",
            "Tetpyg_song: 0.55%\n",
            "Tibtom_song: 0.22%\n",
            "Trotro_song: 1.97%\n",
            "Turmer_call: 0.87%\n",
            "Turmer_song: 0.33%\n",
            "Turphi_call: 0.22%\n",
            "Turphi_song: 1.75%\n",
            "Unknown: 4.26%\n",
            "\n",
            "Differences in Proportions (Training - Validation):\n",
            "Aegcau_call: -0.06%\n",
            "Alaarv_song: 0.02%\n",
            "Anttri_song: -0.06%\n",
            "Butbut_call: 0.05%\n",
            "Carcan_call: -0.06%\n",
            "Carcan_song: -0.03%\n",
            "Carcar_call: 0.05%\n",
            "Carcar_song: -0.06%\n",
            "Cerbra_call: -0.00%\n",
            "Cerbra_song: 0.03%\n",
            "Cetcet_song: -0.01%\n",
            "Chlchl_call: 0.05%\n",
            "Cicatr_song: 0.05%\n",
            "Cicorn_song: -0.03%\n",
            "Cisjun_song: 0.05%\n",
            "Colpal_song: -0.06%\n",
            "Corcor_call: 0.05%\n",
            "Denmaj_call: 0.05%\n",
            "Denmaj_drum: -0.06%\n",
            "Embcir_call: -0.06%\n",
            "Embcir_song: 0.05%\n",
            "Erirub_call: 0.03%\n",
            "Erirub_song: 0.02%\n",
            "Fricoe_call: -0.00%\n",
            "Fricoe_song: -0.06%\n",
            "Galcri_call: 0.05%\n",
            "Galcri_song: -0.00%\n",
            "Galthe_call: 0.05%\n",
            "Galthe_song: -0.01%\n",
            "Gargla_call: 0.05%\n",
            "Hirrus_call: 0.03%\n",
            "Jyntor_song: -0.03%\n",
            "Lopcri_call: 0.05%\n",
            "Loxcur_call: 0.02%\n",
            "Lularb_song: -0.01%\n",
            "Lusmeg_call: -0.03%\n",
            "Lusmeg_song: -0.06%\n",
            "Lyrple_song: -0.06%\n",
            "Motcin_call: 0.05%\n",
            "Musstr_call: 0.05%\n",
            "Noise: -0.03%\n",
            "Oriori_call: -0.06%\n",
            "Oriori_song: -0.00%\n",
            "Parate_call: -0.00%\n",
            "Parate_song: 0.05%\n",
            "Parcae_call: -0.00%\n",
            "Parcae_song: -0.00%\n",
            "Parmaj_call: -0.00%\n",
            "Parmaj_song: 0.05%\n",
            "Pasdom_call: 0.05%\n",
            "Pelgra_call: -0.06%\n",
            "Petpet_call: 0.05%\n",
            "Petpet_song: -0.00%\n",
            "Phofem_song: 0.05%\n",
            "Phycol_call: -0.03%\n",
            "Phycol_song: -0.06%\n",
            "Picpic_call: -0.06%\n",
            "Plaaff_song: 0.05%\n",
            "Plasab_song: -0.03%\n",
            "Poepal_call: -0.00%\n",
            "Poepal_song: 0.03%\n",
            "Prumod_song: -0.00%\n",
            "Ptehey_song: -0.03%\n",
            "Pyrpyr_call: 0.05%\n",
            "Regign_call: -0.03%\n",
            "Regign_song: -0.00%\n",
            "Serser_call: -0.00%\n",
            "Serser_song: -0.03%\n",
            "Siteur_call: -0.06%\n",
            "Siteur_song: 0.03%\n",
            "Strdec_song: -0.00%\n",
            "Strtur_song: 0.03%\n",
            "Stuvul_call: 0.05%\n",
            "Sylatr_call: -0.03%\n",
            "Sylatr_song: -0.00%\n",
            "Sylcan_call: -0.06%\n",
            "Sylcan_song: 0.02%\n",
            "Sylmel_call: -0.01%\n",
            "Sylmel_song: -0.01%\n",
            "Sylund_call: -0.06%\n",
            "Sylund_song: 0.02%\n",
            "Tetpyg_song: 0.05%\n",
            "Tibtom_song: -0.00%\n",
            "Trotro_song: -0.06%\n",
            "Turmer_call: -0.00%\n",
            "Turmer_song: -0.03%\n",
            "Turphi_call: 0.05%\n",
            "Turphi_song: -0.06%\n",
            "Unknown: -0.04%\n"
          ]
        }
      ],
      "source": [
        "def get_class_proportions(y, class_names):\n",
        "    \"\"\"\n",
        "    Calculate the proportion of each class in the given binary matrix y.\n",
        "    \"\"\"\n",
        "    proportions = {}\n",
        "    total_samples = y.shape[0]\n",
        "\n",
        "    for idx, class_name in enumerate(class_names):\n",
        "        proportions[class_name] = np.sum(y[:, idx]) / total_samples\n",
        "\n",
        "    return proportions\n",
        "\n",
        "\n",
        "if DEBUG:\n",
        "    train_proportions = get_class_proportions(y_train, class_names)\n",
        "    valid_proportions = get_class_proportions(y_val, class_names)\n",
        "\n",
        "    print(\"Class Proportions in Training Dataset:\")\n",
        "    for class_name, proportion in train_proportions.items():\n",
        "        print(f\"{class_name}: {proportion * 100:.2f}%\")\n",
        "\n",
        "    print(\"\\nClass Proportions in Validation Dataset:\")\n",
        "    for class_name, proportion in valid_proportions.items():\n",
        "        print(f\"{class_name}: {proportion * 100:.2f}%\")\n",
        "\n",
        "    # Comparing the differences in proportions\n",
        "    print(\"\\nDifferences in Proportions (Training - Validation):\")\n",
        "    for class_name in class_names:\n",
        "        difference = train_proportions[class_name] - valid_proportions[class_name]\n",
        "        print(f\"{class_name}: {difference * 100:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eeesSAdr-rIu"
      },
      "outputs": [],
      "source": [
        "# if DEBUG:\n",
        "#     print(f\"Number of elements: {len(valid_dataset)}\")\n",
        "#     sample, target = valid_dataset[90]\n",
        "#     processed_sample, intermediates = sample\n",
        "\n",
        "#     print(processed_sample.shape)\n",
        "#     num_positive_labels = target.sum().item()\n",
        "#     print(target)\n",
        "#     print(f\"Number of positive labels: {num_positive_labels}\")\n",
        "\n",
        "#     predicted_classes = [class_name for idx, class_name in enumerate(class_names) if target[idx] == 1.0]\n",
        "#     print(\"Predicted Classes:\", predicted_classes)\n",
        "\n",
        "#     visualize_intermediates(intermediates)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EAU_nQNdH2RZ"
      },
      "outputs": [],
      "source": [
        "# if DEBUG:\n",
        "#     print(f\"Number of elements: {len(train_dataset)}\")\n",
        "#     sample, target = train_dataset[30]\n",
        "#     processed_sample, intermediates = sample\n",
        "\n",
        "#     print(processed_sample.shape)\n",
        "#     num_positive_labels = target.sum().item()\n",
        "#     print(target)\n",
        "#     print(f\"Number of positive labels: {num_positive_labels}\")\n",
        "\n",
        "#     predicted_classes = [class_name for idx, class_name in enumerate(class_names) if target[idx] == 1.0]\n",
        "#     print(\"Predicted Classes:\", predicted_classes)\n",
        "#     visualize_intermediates(intermediates)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-pXedOD0UwwG"
      },
      "source": [
        "**Calcular la longitud máxima de las formas de onda**\n",
        "\n",
        "Se determina la longitud máxima entre todas las formas de onda para poder rellenar (padding) o truncar los audios posteriormente, garantizando que todos tengan la misma longitud.\n",
        "\n",
        "La función `collate_fn` se utiliza para procesar y combinar un lote (batch) de muestras en el dataloader. Asegura que todas las formas de onda tengan la misma longitud (rellenando con ceros si es necesario) y devuelve las formas de onda junto con sus objetivos (etiquetas). Para esto, necesita la longitud máxima calculada anteriormente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gvjVdc97TymV"
      },
      "outputs": [],
      "source": [
        "def collate_fn(batch):\n",
        "    # Extract the waveform (or spectrogram) and ignore intermediates (the dict used to debug) if they exist\n",
        "    waveforms_or_spectrograms = [wf[0] if isinstance(wf, tuple) else wf for wf, _ in batch]\n",
        "\n",
        "    if isinstance(batch[0][1], str):\n",
        "        # This handles the Test set scenario where targets might be filenames\n",
        "        waveforms_or_spectrograms = torch.stack(waveforms_or_spectrograms)\n",
        "        _, filenames = zip(*batch)\n",
        "        return waveforms_or_spectrograms, filenames\n",
        "\n",
        "    elif isinstance(batch[0][1], dict):\n",
        "        # This handles the scenario where DEBUG is False and intermediates dictionary is returned\n",
        "        waveforms_or_spectrograms = torch.stack(waveforms_or_spectrograms)\n",
        "        return waveforms_or_spectrograms  # Note: Here we return only the waveforms as there are no target labels\n",
        "\n",
        "    else:\n",
        "        # This handles the Training or validation batch scenario\n",
        "        waveforms_or_spectrograms = torch.stack(waveforms_or_spectrograms)\n",
        "        _, targets = zip(*batch)\n",
        "        targets = torch.stack(targets)\n",
        "        return waveforms_or_spectrograms, targets\n",
        "\n",
        "BATCH_SIZE=6\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QHV02H3TVZtm"
      },
      "source": [
        "## Definición del modelo\n",
        "\n",
        "- Se define una arquitectura basada en el modelo ResNet50 preentrenado.\n",
        "- Se adapta la primera capa convolucional para aceptar imágenes de un solo canal (grises).\n",
        "- Se elimina la última capa completamente conectada del ResNet y se agrega una clasificación personalizada para adaptar la arquitectura al problema multietiqueta.\n",
        "\n",
        "Una de las cosas que se ha probado, es utilizar una mezcla de *transfer-learning* y *fine-tuning*.\n",
        "\n",
        "**Transferencia de aprendizaje**:\n",
        "\n",
        "El modelo se carga y se adaptan algunas capas. Se congelan los pesos de las capas del modelo preentrenado para que no se actualicen durante el entrenamiento inicial, por lo que sólo las capas personalizadas, como la capa de clasificación, se entrenarán. Es decir, se adapta a una tarea diferente el modelo, manteniendo los pesos originales.\n",
        "\n",
        "**Fine-tuning**:\n",
        "\n",
        "Después de algunas épocas de entrenamiento determinadas en el código se desbloquean las capas del modelo preentrenado para que sus pesos también puedan actualizarse durante el entrenamiento\n",
        "\n",
        "```python\n",
        "if epoch == X:\n",
        "    for param in model.features.parameters():\n",
        "        param.requires_grad = True\n",
        "```\n",
        "\n",
        "Este fine-tuning ajusta el modelo a los datos específicos para mejorar el rendimiento, aunque causa cierto *overfitting* al sobreescribir los pesos originales con los datos de entrenamiento.\n",
        "\n",
        "Finalmente, tan sólo se utiliza fine-tuning. Los resultados no cambiaban excesivamente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m8-S3jGAKdMt"
      },
      "outputs": [],
      "source": [
        "# class ResNetMultilabel(nn.Module):\n",
        "#     def __init__(self, num_classes, layers_to_unfreeze=None):\n",
        "#         super(ResNetMultilabel, self).__init__()\n",
        "\n",
        "#         # Initialize the pre-trained model\n",
        "#         self.resnet = models.resnet18(pretrained=True)\n",
        "\n",
        "#         # Replace the initial conv layer to handle grayscale images\n",
        "#         self.resnet.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
        "\n",
        "#         # Dropout for regularization\n",
        "#         # self.dropout = nn.Dropout(0.2)\n",
        "\n",
        "#         # Modify the final layer to match the number of classes\n",
        "#         fc_input_size = self.resnet.fc.in_features\n",
        "#         self.resnet.fc = nn.Sequential(\n",
        "#             nn.Dropout(0.3),\n",
        "#             nn.Linear(fc_input_size, 512),\n",
        "#             nn.ReLU(),\n",
        "#             nn.Dropout(0.3),\n",
        "#             nn.Linear(512, num_classes),\n",
        "#             nn.Sigmoid()\n",
        "#         )\n",
        "\n",
        "#         # Unfreeze selected layers for fine-tuning\n",
        "#         for name, child in self.resnet.named_children():\n",
        "#             if layers_to_unfreeze == \"all\" or name in layers_to_unfreeze:\n",
        "#                 for _, params in child.named_parameters():\n",
        "#                     params.requires_grad = True\n",
        "#             else:\n",
        "#                 for _, params in child.named_parameters():\n",
        "#                     params.requires_grad = False\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         return self.resnet(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QDEtRrK_KdMt"
      },
      "outputs": [],
      "source": [
        "# # Set up the device\n",
        "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "# print(f\"Using: {device}\")\n",
        "\n",
        "# # Initialize the model\n",
        "# model = ResNetMultilabel(num_classes=len(class_names), layers_to_unfreeze=\"all\").to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D1zpmtnhXXEk"
      },
      "source": [
        "## Entrenamiento\n",
        "\n",
        "- Se utiliza BCE (Binary Cross Entropy), adecuada para problemas de clasificación multietiqueta junto a un optimizador Adam con tasa de aprendizaje `0.0001`.\n",
        "- Se utiliza un programador de learning rate ReduceLROnPlateau, o CosineAnnealingLR, que disminuye la tasa de aprendizaje si la función de pérdida no mejora.\n",
        "\n",
        "El proceso de entrenamiento se ejecuta a través de 20-50 épocas, y durante cada época se calcula la pérdida en entrenamiento y se ajustan los pesos del modelo, se calcula el F1 en entrenamiento, y se pasa el modelo a modo de evaluación para evaluar en el conjunto de validación, calculando tanto la pérdida como el F1 score.\n",
        "\n",
        "Si el modelo mejora se guarda un checkpoint de los pesos. Está implementado un sistema de early stopping para evitar el sobreajuste restaurando el mejor modelo.\n",
        "\n",
        "Después de cada época se ajusta el learning rate según la evolución de la pérdida en validación.\n",
        "\n",
        "**Búsqueda de umbral (threshold)**:\n",
        "- Se inicializa una lista de posibles `thresholds` de 0.1 a 1 en incrementos de 0.1. Estos son los umbrales para decidir si una predicción (probabilidad) del modelo es positiva o negativa.\n",
        "- Para cada umbral se calcula el F1 score en entrenamiento y validación y se elige el umbral que produce el mejor F1 score en el conjunto de validación.\n",
        "\n",
        "Esto es importante porque las salidas del modelo son valores continuos entre 0 y 1, que representan la confianza del modelo en que esa etiqueta es positiva, y es necesario decidir un umbral (`threshold`) para convertir estas salidas continuas en etiquetas binarias definitivas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 859,
          "referenced_widgets": [
            "7c0963c5da7e4d2984f5105c6aebc850",
            "e86e25da6a4d455baf185bdd6e2810c0",
            "ce48315f8d60453bba069d219ccfcf1b",
            "65b15e00a8d6499da52c6cfa64a8ca12",
            "5eb86cdfaf9f437da8b12b56c7effdef",
            "8fe5c1c61d5f4fab89f3ce37b6ac2b21",
            "ed23ed237aeb400eb22bd653d51128df",
            "38be63abc6f6473fa40f4fbb5b4d7dcb",
            "7eba3ad936f942338d5dcff93c20218a",
            "6151429b76364d68a9cecbfeb04f67eb",
            "078b655d6b7f437ba5288b6f6f4a9493",
            "2510891d82994d3caad42a76ac3d784c",
            "8398712586e84304b86889967b5b2d8a",
            "e8436765994c4eaaac36319da55e4226",
            "97c3e1799a26493a9b5bc173d4b6de6d",
            "a44b6a3671b948f9aa05d5cac1572831",
            "4739111364654a10b4193ef90340cd2a",
            "c7f01491b23545739cf0554ff43b89a4",
            "da070cf88960405db1bc0a65bba481ae",
            "6575e706ce0e4253bb2b88572e992b18",
            "dd41b34ecffa4a36988d98a34a2db0d0",
            "02ac6bf62fa84f3da77670b1882a3db8",
            "c884682eeac64eb681475c91ca792ff3",
            "313301da4da5489983bd16bc1719c9b9",
            "02f6701d49bf4673b8ac8a4802470c65",
            "81f0e7485ff14b7d9d2f98502caca8d7",
            "c18324ddba554418a32401cbe1c9ce56",
            "2421814ff4a043489af631c5a182ccc8",
            "56c8bf93830b48ffba3f4488a7b19ed2",
            "78c3118c00d14dedbbeb6518b6a295d9",
            "74cb9e4179084be6ab58aa099a7f9d43",
            "d76bb443a7634a33bb3d51976705a7e4",
            "f25bd495623f4b88b689ba01b26db381"
          ]
        },
        "id": "U52GfnjY6SyF",
        "outputId": "365c3974-4622-4386-b939-f78de37caa4f"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)rocessor_config.json:   0%|          | 0.00/297 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7c0963c5da7e4d2984f5105c6aebc850"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)lve/main/config.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2510891d82994d3caad42a76ac3d784c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading model.safetensors:   0%|          | 0.00/346M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c884682eeac64eb681475c91ca792ff3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ASTForAudioClassification(\n",
              "  (audio_spectrogram_transformer): ASTModel(\n",
              "    (embeddings): ASTEmbeddings(\n",
              "      (patch_embeddings): ASTPatchEmbeddings(\n",
              "        (projection): Conv2d(1, 768, kernel_size=(16, 16), stride=(10, 10))\n",
              "      )\n",
              "      (dropout): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (encoder): ASTEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x ASTLayer(\n",
              "          (attention): ASTAttention(\n",
              "            (attention): ASTSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "            (output): ASTSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): ASTIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): ASTOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "  )\n",
              "  (classifier): ASTMLPHead(\n",
              "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "    (dense): Linear(in_features=768, out_features=89, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "from transformers import AutoFeatureExtractor, ASTForAudioClassification\n",
        "from torch.utils.data import DataLoader\n",
        "import torch\n",
        "\n",
        "# Initialize feature extractor and model\n",
        "feature_extractor = AutoFeatureExtractor.from_pretrained(\"MIT/ast-finetuned-audioset-10-10-0.4593\")\n",
        "model = ASTForAudioClassification.from_pretrained(\"MIT/ast-finetuned-audioset-10-10-0.4593\")\n",
        "\n",
        "# Replace the model's classification head\n",
        "# ASTForAudioClassification's classification head is accessed by 'classifier.dense' based on the HuggingFace structure\n",
        "model.classifier.dense = torch.nn.Linear(model.config.hidden_size, len(class_names))\n",
        "model.num_labels = len(class_names)\n",
        "\n",
        "# Move model to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u3_P-iwc6SyF",
        "outputId": "0517a99d-158a-479d-896d-b356d6d91907"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Train Loss: 0.0664, Train F1: 0.1627, Val Loss: 0.0447, Val F1: 0.4154\n",
            "Epoch 2, Train Loss: 0.0574, Train F1: 0.1766, Val Loss: 0.0446, Val F1: 0.2017\n",
            "Epoch 3, Train Loss: 0.0573, Train F1: 0.1873, Val Loss: 0.0436, Val F1: 0.4200\n",
            "Finished Training\n"
          ]
        }
      ],
      "source": [
        "total_epochs = 3\n",
        "\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=0.001)\n",
        "scheduler = ReduceLROnPlateau(optimizer, 'min', factor=0.5, patience=2)\n",
        "\n",
        "best_val_loss = float('inf')\n",
        "epochs_no_improve = 0\n",
        "n_epochs_stop = 20\n",
        "early_stop = False\n",
        "thresholds = np.arange(0.1, 1, 0.1)\n",
        "\n",
        "# Your training loop with modifications for the AST model\n",
        "for epoch in range(total_epochs):\n",
        "    model.train()\n",
        "    running_train_loss = 0.0\n",
        "    train_labels = []\n",
        "    train_preds = []\n",
        "\n",
        "    for inputs, labels in train_loader:\n",
        "        # Make sure inputs are single channel\n",
        "        if inputs.ndim == 3 and inputs.size(1) > 1:\n",
        "            print(f\"Input has multiple channels, shape: {inputs.shape}\")\n",
        "            inputs = torch.mean(inputs, dim=1, keepdim=True)\n",
        "        elif inputs.ndim == 2:\n",
        "            inputs = inputs.unsqueeze(1)  # Add channel dimension\n",
        "\n",
        "        # Now inputs should be of shape [batch, 1, time]\n",
        "        #print(f\"Shape before feature extractor: {inputs.shape}\")\n",
        "\n",
        "        # Process each waveform through the feature extractor\n",
        "        inputs = feature_extractor(inputs.squeeze(1).numpy(), sampling_rate=16000, return_tensors=\"pt\", padding=True)\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.logits\n",
        "\n",
        "        # Compute loss, no need to apply sigmoid since BCEWithLogitsLoss does that internally\n",
        "        loss = criterion(logits, labels.float())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_train_loss += loss.item()\n",
        "\n",
        "        # Store predictions and labels for F1 calculation\n",
        "        train_preds.extend(torch.sigmoid(logits).detach().cpu().numpy())\n",
        "        train_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    train_loss = running_train_loss / len(train_loader)\n",
        "\n",
        "    # Your validation loop remains the same, with changes for feature extraction and predictions handling\n",
        "    model.eval()\n",
        "    running_val_loss = 0.0\n",
        "    val_labels = []\n",
        "    val_preds = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in valid_loader:\n",
        "            # Process through feature extractor and to device\n",
        "            inputs = feature_extractor(inputs.squeeze(1).numpy(), sampling_rate=16000, return_tensors=\"pt\", padding=True)\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(**inputs)\n",
        "            logits = outputs.logits\n",
        "\n",
        "            loss = criterion(logits, labels.float())\n",
        "            running_val_loss += loss.item()\n",
        "\n",
        "            val_preds.extend(torch.sigmoid(logits).detach().cpu().numpy())\n",
        "            val_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    val_loss = running_val_loss / len(valid_loader)\n",
        "\n",
        "    # Calculate F1 scores without manual thresholding\n",
        "    train_f1 = f1_score(np.array(train_labels), np.array(train_preds) > 0.1, average='samples', zero_division=1)\n",
        "    val_f1 = f1_score(np.array(val_labels), np.array(val_preds) > 0.1, average='samples', zero_division=1)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Train F1: {train_f1:.4f}, Val Loss: {val_loss:.4f}, Val F1: {val_f1:.4f}\")\n",
        "\n",
        "    # Checkpointing\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        epochs_no_improve = 0\n",
        "        torch.save(model.state_dict(), 'best_model.pth')\n",
        "    else:\n",
        "        epochs_no_improve += 1\n",
        "\n",
        "    # Early stopping\n",
        "    if epochs_no_improve == n_epochs_stop:\n",
        "        print('Early stopping!')\n",
        "        early_stop = True\n",
        "        break\n",
        "\n",
        "    # Adjusting learning rate\n",
        "    scheduler.step(val_loss)\n",
        "\n",
        "if early_stop:\n",
        "    print(\"Stopped training. Loading best model weights!\")\n",
        "    model.load_state_dict(torch.load('best_model.pth'))\n",
        "\n",
        "print('Finished Training')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IPW3M4gxKdMu"
      },
      "outputs": [],
      "source": [
        "# total_epochs = 30\n",
        "\n",
        "# train_losses = []\n",
        "# val_losses = []\n",
        "\n",
        "# criterion = nn.BCELoss()\n",
        "# optimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=0.001)\n",
        "# #optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "# scheduler = ReduceLROnPlateau(optimizer, 'min', factor=0.5, patience=2)\n",
        "\n",
        "# best_val_loss = float('inf')\n",
        "# epochs_no_improve = 0\n",
        "# n_epochs_stop = 20\n",
        "# early_stop = False\n",
        "# thresholds = np.arange(0.1, 1, 0.1)\n",
        "\n",
        "# for epoch in range(total_epochs):\n",
        "\n",
        "#     # Training\n",
        "#     model.train()\n",
        "#     running_train_loss = 0.0\n",
        "#     all_train_preds = []\n",
        "#     all_train_labels = []\n",
        "#     for i, (inputs, labels) in enumerate(train_loader):\n",
        "#         inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "#         optimizer.zero_grad()\n",
        "\n",
        "#         outputs = model(inputs)\n",
        "#         loss = criterion(outputs, labels)\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "\n",
        "#         running_train_loss += loss.item()\n",
        "\n",
        "#         # Store training predictions and true labels\n",
        "#         all_train_preds.extend(outputs.detach().cpu().numpy().tolist())\n",
        "#         all_train_labels.extend(labels.cpu().numpy().tolist())\n",
        "\n",
        "#     train_loss = running_train_loss / len(train_loader)\n",
        "\n",
        "#     # Validation\n",
        "#     model.eval()\n",
        "#     running_val_loss = 0.0\n",
        "#     all_preds = []\n",
        "#     all_labels = []\n",
        "#     with torch.no_grad():\n",
        "#         for inputs, labels in valid_loader:\n",
        "#             inputs, labels = inputs.to(device), labels.to(device)\n",
        "#             outputs = model(inputs)\n",
        "#             loss = criterion(outputs, labels)\n",
        "#             running_val_loss += loss.item()\n",
        "#             # Store predictions and true labels\n",
        "#             all_preds.extend(outputs.cpu().numpy().tolist())\n",
        "#             all_labels.extend(labels.cpu().numpy().tolist())\n",
        "\n",
        "#     val_loss = running_val_loss / len(valid_loader)\n",
        "\n",
        "#     # Append losses to the lists\n",
        "#     train_losses.append(train_loss)\n",
        "#     val_losses.append(val_loss)\n",
        "\n",
        "#     # Calculate validation F1 scores over different thresholds\n",
        "#     val_f1_scores = []\n",
        "#     for threshold in thresholds:\n",
        "#         val_f1_scores.append(f1_score(all_labels, np.array(all_preds) > threshold, average='samples', zero_division=1))\n",
        "\n",
        "#     # Get the best F1 score and corresponding threshold from the validation data\n",
        "#     best_threshold_index_val = np.argmax(val_f1_scores)\n",
        "#     best_threshold_val = thresholds[best_threshold_index_val]\n",
        "#     validation_f1 = val_f1_scores[best_threshold_index_val]\n",
        "\n",
        "#     # Calculate training F1 score using the best_threshold_val\n",
        "#     train_best_f1 = f1_score(all_train_labels, np.array(all_train_preds) > best_threshold_val, average='samples', zero_division=1)\n",
        "\n",
        "#     print(f\"Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Training F1: {train_best_f1:.4f}, Validation Loss: {val_loss:.4f}, Validation F1: {validation_f1:.4f} using threshold {best_threshold_val:.2f}\")\n",
        "\n",
        "#     # Checkpointing\n",
        "#     if val_loss < best_val_loss:\n",
        "#         best_val_loss = val_loss\n",
        "#         epochs_no_improve = 0\n",
        "#         torch.save(model.state_dict(), 'best_model.pth')\n",
        "#     else:\n",
        "#         epochs_no_improve += 1\n",
        "\n",
        "#     # Early stopping\n",
        "#     if epochs_no_improve == n_epochs_stop:\n",
        "#         print('Early stopping!')\n",
        "#         early_stop = True\n",
        "#         break\n",
        "\n",
        "#     # Adjusting learning rate\n",
        "#     scheduler.step(-val_loss)\n",
        "\n",
        "# if early_stop:\n",
        "#     print(\"Stopped training. Loading best model weights!\")\n",
        "#     model.load_state_dict(torch.load('best_model.pth'))\n",
        "\n",
        "# print('Finished Training')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xuuNXqBT-rIw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 504
        },
        "outputId": "0b0b2d15-64b5-4225-ab11-d715ac712b23"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Finished Training\n"
          ]
        }
      ],
      "source": [
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(train_losses, label='Training Loss')\n",
        "plt.plot(val_losses, label='Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and Validation Loss over Epochs')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "print('Finished Training')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wt_5gGclbVWf"
      },
      "source": [
        "## Evaluación y Predicción en el conjunto de Test\n",
        "\n",
        "1. **Evaluación de las predicciones**:\n",
        "Se pone el modelo en modo `eval()` y se itera sobre el conjunto de validación para obtener las predicciones y se calcula el F1 usando el mejor umbral.\n",
        "\n",
        "2. **Preparación del conjunto de Test**:\n",
        "Se crea la clase `BirdSongTestDataset` que lee de `test.csv`, y se crea un DataLoader para el conjunto de Test.\n",
        "\n",
        "3. **Predicciones en el conjunto de Test**:\n",
        "Se itera sobre el conjunto de test y se obtienen las predicciones del modelo para cada archivo de audio. Se binarizan usando el mejor umbral y se almacenan en un diccionario con el nombre del archivo como clave.\n",
        "Las predicciones se convierten en un DataFrame de Pandas y se preparan los datos en el formato esperado, y por último se guarda el DataFrame en un archivo CSV."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RhZzEOkWH2Rb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "outputId": "3f4a90cc-a207-43c9-f8ac-078501d42def"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-d6f7f92cd22b>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Best threshold: {best_threshold_val}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'best_threshold_val' is not defined"
          ]
        }
      ],
      "source": [
        "print(f\"Best threshold: {best_threshold_val}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8g6kj9f2KdMv"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "audio_preds = defaultdict(list)\n",
        "audio_labels = defaultdict(list)\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch_idx, (inputs, labels) in enumerate(valid_loader):\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        outputs = model(inputs)\n",
        "        preds = (outputs > best_threshold_val).float()\n",
        "\n",
        "        # Fetch the filename for each segment\n",
        "        for i, (input_, label, pred) in enumerate(zip(inputs, labels, preds)):\n",
        "            # Compute the correct index in the dataset\n",
        "            dataset_index = batch_idx * valid_loader.batch_size + i\n",
        "            filename = valid_dataset.get_filename(dataset_index)\n",
        "            audio_preds[filename].append(pred.cpu().numpy())\n",
        "            # The label should be the same for all segments of the same audio, so we append it only once\n",
        "            if filename not in audio_labels:\n",
        "                audio_labels[filename] = label.cpu().numpy()\n",
        "\n",
        "# Aggregating the segment-level predictions for each audio to generate a single prediction for the whole audio\n",
        "for filename in audio_preds:\n",
        "    # Here, we take the max prediction for each class across all segments as the audio-level prediction\n",
        "    audio_preds[filename] = np.maximum.reduce(audio_preds[filename])\n",
        "\n",
        "all_labels = list(audio_labels.values())\n",
        "all_preds = list(audio_preds.values())\n",
        "\n",
        "f1_macro = f1_score(all_labels, all_preds, average='samples', zero_division=1)\n",
        "print(f\"F1 Score (Samples): {f1_macro}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q6z5GWm1KdMv"
      },
      "outputs": [],
      "source": [
        "class BirdSongTestDataset(Dataset):\n",
        "    def __init__(self, df, audio_dir, transform=None):\n",
        "        segments = []\n",
        "\n",
        "        unique_filenames = df['filename'].unique()\n",
        "        for unique_filename in unique_filenames:\n",
        "            audio_path = os.path.join(audio_dir, unique_filename)\n",
        "            waveform, sample_rate = torchaudio.load(audio_path)\n",
        "            total_segments = int(math.ceil(waveform.shape[1] / sample_rate))\n",
        "\n",
        "            for idx in range(total_segments):\n",
        "                start_time, end_time = idx, idx + 1\n",
        "                segments.append({\n",
        "                    'filename': unique_filename,\n",
        "                    'segment_idx': idx,\n",
        "                    'start': start_time,\n",
        "                    'end': end_time,\n",
        "                })\n",
        "\n",
        "        self.segments = pd.DataFrame(segments)\n",
        "        self.audio_dir = audio_dir\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.segments)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.segments.iloc[idx]\n",
        "        audio_path = os.path.join(self.audio_dir, row['filename'])\n",
        "        waveform, sample_rate = torchaudio.load(audio_path)\n",
        "\n",
        "        start_sample = int(row['start'] * sample_rate)\n",
        "        end_sample = int(row['end'] * sample_rate)\n",
        "        waveform = waveform[:, start_sample:end_sample]\n",
        "\n",
        "        if waveform.shape[1] < sample_rate:\n",
        "            num_padding = sample_rate - waveform.shape[1]\n",
        "            waveform = torch.cat([waveform, torch.zeros(1, num_padding)], dim=1)\n",
        "\n",
        "        if self.transform:\n",
        "            waveform = self.transform(waveform)\n",
        "\n",
        "        return waveform, row['filename'], row['segment_idx']\n",
        "\n",
        "\n",
        "test_csv = pd.read_csv(f'{base_path}data/test.csv')\n",
        "\n",
        "def collate_fn(batch):\n",
        "    # Extracting tensors from the first item of the data point\n",
        "    segments = [item[0][0] for item in batch]  # Accessing the tensor in the tuple\n",
        "\n",
        "    # Extracting filenames\n",
        "    filenames = [item[1] for item in batch]\n",
        "\n",
        "    # Stacking tensors\n",
        "    segments_tensor = torch.stack(segments, dim=0)\n",
        "\n",
        "    return segments_tensor, filenames\n",
        "\n",
        "\n",
        "test_dataset = BirdSongTestDataset(test_csv, f'{base_path}data/test/', transform=valid_transform)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AxzJ8JxeUPbv"
      },
      "outputs": [],
      "source": [
        "if DEBUG:\n",
        "  (sample, intermediates), filename, label = test_dataset[25]\n",
        "  print(filename)\n",
        "  print(sample.shape)\n",
        "  visualize_intermediates(intermediates)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gc77_1e9UOiz"
      },
      "outputs": [],
      "source": [
        "# Make predictions on test set\n",
        "model.eval()\n",
        "predictions = defaultdict(lambda: np.zeros(len(class_names), dtype=bool))\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, filenames in test_loader:\n",
        "        inputs = inputs.to(device)\n",
        "        outputs = model(inputs)\n",
        "        preds = (outputs > 0.1).float().cpu().numpy().astype(bool)\n",
        "\n",
        "        for fname, pred in zip(filenames, preds):\n",
        "            # Use logical OR to aggregate predictions for each audio. If the bird appears in one segment of the audio, it means that it appears in the whole audio.\n",
        "            predictions[fname] = np.logical_or(predictions[fname], pred)\n",
        "\n",
        "# Convert boolean values to integer (0 or 1)\n",
        "for key in predictions:\n",
        "    predictions[key] = predictions[key].astype(int)\n",
        "\n",
        "submission_df = pd.DataFrame.from_dict(predictions, orient='index', columns=class_names)\n",
        "submission_df.reset_index(inplace=True)\n",
        "submission_df.rename(columns={'index': 'filename'}, inplace=True)\n",
        "submission_df.to_csv('submission.csv', index=False)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "7c0963c5da7e4d2984f5105c6aebc850": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e86e25da6a4d455baf185bdd6e2810c0",
              "IPY_MODEL_ce48315f8d60453bba069d219ccfcf1b",
              "IPY_MODEL_65b15e00a8d6499da52c6cfa64a8ca12"
            ],
            "layout": "IPY_MODEL_5eb86cdfaf9f437da8b12b56c7effdef"
          }
        },
        "e86e25da6a4d455baf185bdd6e2810c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8fe5c1c61d5f4fab89f3ce37b6ac2b21",
            "placeholder": "​",
            "style": "IPY_MODEL_ed23ed237aeb400eb22bd653d51128df",
            "value": "Downloading (…)rocessor_config.json: 100%"
          }
        },
        "ce48315f8d60453bba069d219ccfcf1b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_38be63abc6f6473fa40f4fbb5b4d7dcb",
            "max": 297,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7eba3ad936f942338d5dcff93c20218a",
            "value": 297
          }
        },
        "65b15e00a8d6499da52c6cfa64a8ca12": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6151429b76364d68a9cecbfeb04f67eb",
            "placeholder": "​",
            "style": "IPY_MODEL_078b655d6b7f437ba5288b6f6f4a9493",
            "value": " 297/297 [00:00&lt;00:00, 11.1kB/s]"
          }
        },
        "5eb86cdfaf9f437da8b12b56c7effdef": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8fe5c1c61d5f4fab89f3ce37b6ac2b21": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ed23ed237aeb400eb22bd653d51128df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "38be63abc6f6473fa40f4fbb5b4d7dcb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7eba3ad936f942338d5dcff93c20218a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6151429b76364d68a9cecbfeb04f67eb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "078b655d6b7f437ba5288b6f6f4a9493": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2510891d82994d3caad42a76ac3d784c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8398712586e84304b86889967b5b2d8a",
              "IPY_MODEL_e8436765994c4eaaac36319da55e4226",
              "IPY_MODEL_97c3e1799a26493a9b5bc173d4b6de6d"
            ],
            "layout": "IPY_MODEL_a44b6a3671b948f9aa05d5cac1572831"
          }
        },
        "8398712586e84304b86889967b5b2d8a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4739111364654a10b4193ef90340cd2a",
            "placeholder": "​",
            "style": "IPY_MODEL_c7f01491b23545739cf0554ff43b89a4",
            "value": "Downloading (…)lve/main/config.json: 100%"
          }
        },
        "e8436765994c4eaaac36319da55e4226": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_da070cf88960405db1bc0a65bba481ae",
            "max": 26763,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6575e706ce0e4253bb2b88572e992b18",
            "value": 26763
          }
        },
        "97c3e1799a26493a9b5bc173d4b6de6d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dd41b34ecffa4a36988d98a34a2db0d0",
            "placeholder": "​",
            "style": "IPY_MODEL_02ac6bf62fa84f3da77670b1882a3db8",
            "value": " 26.8k/26.8k [00:00&lt;00:00, 1.16MB/s]"
          }
        },
        "a44b6a3671b948f9aa05d5cac1572831": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4739111364654a10b4193ef90340cd2a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c7f01491b23545739cf0554ff43b89a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "da070cf88960405db1bc0a65bba481ae": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6575e706ce0e4253bb2b88572e992b18": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "dd41b34ecffa4a36988d98a34a2db0d0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "02ac6bf62fa84f3da77670b1882a3db8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c884682eeac64eb681475c91ca792ff3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_313301da4da5489983bd16bc1719c9b9",
              "IPY_MODEL_02f6701d49bf4673b8ac8a4802470c65",
              "IPY_MODEL_81f0e7485ff14b7d9d2f98502caca8d7"
            ],
            "layout": "IPY_MODEL_c18324ddba554418a32401cbe1c9ce56"
          }
        },
        "313301da4da5489983bd16bc1719c9b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2421814ff4a043489af631c5a182ccc8",
            "placeholder": "​",
            "style": "IPY_MODEL_56c8bf93830b48ffba3f4488a7b19ed2",
            "value": "Downloading model.safetensors: 100%"
          }
        },
        "02f6701d49bf4673b8ac8a4802470c65": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_78c3118c00d14dedbbeb6518b6a295d9",
            "max": 346404948,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_74cb9e4179084be6ab58aa099a7f9d43",
            "value": 346404948
          }
        },
        "81f0e7485ff14b7d9d2f98502caca8d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d76bb443a7634a33bb3d51976705a7e4",
            "placeholder": "​",
            "style": "IPY_MODEL_f25bd495623f4b88b689ba01b26db381",
            "value": " 346M/346M [00:01&lt;00:00, 183MB/s]"
          }
        },
        "c18324ddba554418a32401cbe1c9ce56": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2421814ff4a043489af631c5a182ccc8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "56c8bf93830b48ffba3f4488a7b19ed2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "78c3118c00d14dedbbeb6518b6a295d9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "74cb9e4179084be6ab58aa099a7f9d43": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d76bb443a7634a33bb3d51976705a7e4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f25bd495623f4b88b689ba01b26db381": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}