{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "za_-0XklmBeX"
      },
      "source": [
        "# Aprendizaje Profundo\n",
        "Daniel López Gala - UO281798\n",
        "\n",
        "Se dispone del conjunto de datos NIPS4BPLUS, el cual contiene 674 ficheros de audio con una duración total de menos de una hora. En estos audios podemos encontrar grabaciones de aproximadamente 5 segundos con cantos de pájaros realizadas en 39 localizaciones diferentes repartidas por 7 regiones de Francia y España."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kISgPuot6Sx_"
      },
      "source": [
        "# Aprendizaje Profundo\n",
        "Daniel López Gala - UO281798\n",
        "\n",
        "Se dispone del conjunto de datos NIPS4BPLUS, el cual contiene 674 ficheros de audio con una duración total de menos de una hora. En estos audios podemos encontrar grabaciones de aproximadamente 5 segundos con cantos de pájaros realizadas en 39 localizaciones diferentes repartidas por 7 regiones de Francia y España."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "urB-A95SsJri"
      },
      "outputs": [],
      "source": [
        "#base_path = \"/content/drive/MyDrive/DeepLearning/\"\n",
        "base_path = \"\"\n",
        "DEBUG = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z8PtqgSelI8r",
        "outputId": "11e11d49-9a7b-417e-cdfe-1adbe70d0831"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "62qWTY1kk7e6",
        "outputId": "0407a75b-aae0-4c06-f38f-2a1d1bb175ea"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "from collections import defaultdict\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "\n",
        "import torchaudio\n",
        "import torchaudio.transforms as T\n",
        "from torchaudio.transforms import Resample\n",
        "import librosa\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau, CosineAnnealingLR\n",
        "import torchvision.models as models\n",
        "\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "#pip install transformers\n",
        "#pip install scikit-multilearn\n",
        "from skmultilearn.model_selection import iterative_train_test_split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fwU9YFOunaak"
      },
      "source": [
        "## Preprocesamiento y visualización\n",
        "\n",
        "- Se define una función `visualize_intermediates` para crear imágenes de los pasos intermedios usados en el preprocesamiento de los audios.\n",
        "\n",
        "- La clase `AudioPreprocessing` define los pasos para procesar la imagen. Se incluyen:\n",
        "  - Resample (No usado)\n",
        "  - STFT (Convertir a espectrograma)\n",
        "  - Normalización\n",
        "  - Median clipping\n",
        "  - Conectar puntos cercanos mediante filtros\n",
        "  - Closing\n",
        "  - Dilation\n",
        "  - Median blur\n",
        "  - Eliminar residuos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "xKj6CGbCH2RX"
      },
      "outputs": [],
      "source": [
        "def visualize_intermediates(intermediates, sample_rate=44100, hop_length=196):\n",
        "\n",
        "    # Set default background color for figures to white\n",
        "    plt.rcParams['figure.facecolor'] = 'white'\n",
        "\n",
        "    for key, value in intermediates.items():\n",
        "        if len(value.shape) == 2 and value.shape[1] > 2:  # This indicates a waveform\n",
        "            plt.figure(figsize=(12, 4))\n",
        "\n",
        "            # Calculate time axis in seconds for waveform\n",
        "            time_axis_waveform = np.linspace(0, value.shape[1] / sample_rate, value.shape[1])\n",
        "\n",
        "            plt.plot(time_axis_waveform, value[0].cpu().numpy())\n",
        "            plt.xlabel(\"Time (seconds)\")\n",
        "            plt.title(f\"{key}\")\n",
        "            plt.show()\n",
        "            continue\n",
        "\n",
        "        print(f\"Processing {key} with shape {value.shape}\")\n",
        "\n",
        "        if value.dim() == 4 and value.shape[-1] == 2:\n",
        "            complex_representation = value[0, ..., 0] + 1j * value[0, ..., 1]  # Convert to complex\n",
        "            magnitude = torch.abs(complex_representation).cpu().numpy()\n",
        "            phase = torch.angle(complex_representation).cpu().numpy()\n",
        "        elif value.is_complex():\n",
        "            magnitude = torch.abs(value).squeeze().cpu().numpy()\n",
        "            phase = torch.angle(value).squeeze().cpu().numpy()\n",
        "        else:\n",
        "            magnitude = value.squeeze().cpu().numpy()\n",
        "            phase = None\n",
        "\n",
        "        # Calculate time axis in seconds for magnitude\n",
        "        time_axis_magnitude = np.linspace(0, magnitude.shape[1] * hop_length / sample_rate, magnitude.shape[1])\n",
        "\n",
        "        # Plot magnitude with inverted grayscale colormap\n",
        "        plt.figure(figsize=(12, 4))\n",
        "        plt.imshow(magnitude, cmap='gray_r', aspect='auto', origin='lower', extent=[time_axis_magnitude[0], time_axis_magnitude[-1], 0, magnitude.shape[0]])\n",
        "        plt.xlabel(\"Time (seconds)\")\n",
        "        plt.title(f\"{key} Magnitude\")\n",
        "        plt.colorbar()\n",
        "        plt.show()\n",
        "\n",
        "        # Plot phase\n",
        "        if phase is not None:\n",
        "            plt.figure(figsize=(12, 4))\n",
        "            plt.imshow(((phase + np.pi) % (2 * np.pi) - np.pi), cmap='hsv', aspect='auto', origin='lower', vmin=-np.pi, vmax=np.pi, extent=[time_axis_magnitude[0], time_axis_magnitude[-1], 0, phase.shape[0]])\n",
        "            plt.xlabel(\"Time (seconds)\")\n",
        "            plt.title(f\"{key} Phase\")\n",
        "            plt.colorbar()\n",
        "            plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "jFTKwdi0v96g"
      },
      "outputs": [],
      "source": [
        "class AudioPreprocessing(nn.Module):\n",
        "    def __init__(self, debug=DEBUG, sample_rate=16000, n_fft=1024, win_length=1024, hop_length=196, augment=False):\n",
        "        super().__init__()\n",
        "        self.debug = debug\n",
        "        self.augment = augment\n",
        "        self.sample_rate = sample_rate\n",
        "        self.resampler = T.Resample(44100, sample_rate)\n",
        "        self.spectrogram = T.MelSpectrogram(sample_rate, n_fft=n_fft, win_length=win_length, hop_length=hop_length, f_min=500, f_max=15000)\n",
        "\n",
        "    def normalize(self, spectrogram):\n",
        "        min_val = torch.min(spectrogram)\n",
        "        return (spectrogram - min_val) / (torch.max(spectrogram) - min_val + 1e-5)\n",
        "\n",
        "    def median_blurring(self, spectrogram):\n",
        "        img = spectrogram.squeeze(0).cpu().numpy()\n",
        "        img = cv2.medianBlur(img.astype(np.float32), 5)\n",
        "        return torch.tensor(img, device=spectrogram.device).float().unsqueeze(0)\n",
        "\n",
        "    def binary_image_creation(self, spectrogram, threshold=1.5):\n",
        "        freq_median = torch.median(spectrogram, dim=2, keepdim=True).values\n",
        "        time_median = torch.median(spectrogram, dim=1, keepdim=True).values\n",
        "        mask = (spectrogram > threshold * freq_median) & (spectrogram > threshold * time_median)\n",
        "        return mask.float()\n",
        "\n",
        "    ## fastNlMeansDenoising works better but is too much cpu expensive and it bottlenecks the GPU on training\n",
        "    # def spot_removal(self, spectrogram):\n",
        "    #     img = spectrogram.squeeze(0).cpu().numpy()\n",
        "    #     img = cv2.fastNlMeansDenoising(img.astype(np.uint8),None,31,7,21)\n",
        "    #     return torch.tensor(img, device=spectrogram.device).float().unsqueeze(0)\n",
        "\n",
        "    def spot_removal(self, spectrogram, threshold=0.5):\n",
        "        # Threshold the spectrogram to get a binary mask\n",
        "        binary_mask = (spectrogram > threshold).float()\n",
        "\n",
        "        # Convert to numpy for morphological operations\n",
        "        binary_np = binary_mask.squeeze(0).cpu().numpy().astype(np.uint8)\n",
        "\n",
        "        # Define a kernel for morphological operations (adjust size as needed)\n",
        "        kernel = np.ones((3, 3), np.uint8)\n",
        "\n",
        "        # Perform morphological opening to remove small noise\n",
        "        cleaned_binary_np = cv2.morphologyEx(binary_np, cv2.MORPH_OPEN, kernel)\n",
        "\n",
        "        # Convert back to tensor\n",
        "        cleaned_binary_mask = torch.tensor(cleaned_binary_np, device=spectrogram.device).float().unsqueeze(0)\n",
        "\n",
        "        # Multiply with original spectrogram to remove the noise\n",
        "        denoised_spectrogram = spectrogram * cleaned_binary_mask\n",
        "\n",
        "        return denoised_spectrogram\n",
        "\n",
        "    def morph_closing(self, spectrogram):\n",
        "        img = spectrogram.squeeze(0).cpu().numpy()\n",
        "        kernel = np.ones((3, 3), np.uint8)\n",
        "        img = cv2.morphologyEx(img, cv2.MORPH_CLOSE, kernel)\n",
        "        return torch.tensor(img, device=spectrogram.device).float().unsqueeze(0)\n",
        "\n",
        "    def random_time_shift(self, waveform, max_shift_sec=0.2):\n",
        "        \"\"\" Randomly shifts the waveform in the time domain \"\"\"\n",
        "        max_shift = int(max_shift_sec * self.sample_rate)\n",
        "        shift = random.randint(-max_shift, max_shift)\n",
        "        return torch.roll(waveform, shifts=shift, dims=-1)\n",
        "\n",
        "    def random_pitch_shift(self, waveform, max_shift=2):\n",
        "        \"\"\" Randomly shifts the pitch of the waveform \"\"\"\n",
        "        shift = random.uniform(-max_shift, max_shift)\n",
        "        return T.FrequencyMasking(freq_mask_param=int(shift))(waveform)\n",
        "\n",
        "    def random_volume_gain(self, waveform, min_gain=0.5, max_gain=1.5):\n",
        "        \"\"\" Randomly changes the volume of the waveform \"\"\"\n",
        "        gain = random.uniform(min_gain, max_gain)\n",
        "        return waveform * gain\n",
        "\n",
        "    def random_noise_injection(self, waveform, noise_level=0.005):\n",
        "        \"\"\" Adds random noise to the waveform \"\"\"\n",
        "        noise = torch.randn_like(waveform) * noise_level\n",
        "        return waveform + noise\n",
        "\n",
        "    def forward(self, waveform):\n",
        "        intermediates = {}\n",
        "\n",
        "        # Data augmentation\n",
        "        if self.augment:\n",
        "            waveform = self.random_time_shift(waveform)\n",
        "            waveform = self.random_pitch_shift(waveform)\n",
        "            waveform = self.random_volume_gain(waveform)\n",
        "            # waveform = self.random_noise_injection(waveform)\n",
        "\n",
        "        # Resampling to the target sample rate\n",
        "        waveform = self.resampler(waveform)\n",
        "\n",
        "        # Convert stereo to mono if necessary by averaging the channels\n",
        "        if waveform.shape[0] > 1:\n",
        "            waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
        "\n",
        "        spectrogram = self.spectrogram(waveform)\n",
        "        if self.debug: intermediates['original_spectrograms'] = spectrogram\n",
        "\n",
        "        spectrogram = self.normalize(spectrogram)\n",
        "        spectrogram = self.median_blurring(spectrogram)\n",
        "        if self.debug: intermediates['spectrograms_after_median_blurring'] = spectrogram\n",
        "\n",
        "        spectrogram = self.binary_image_creation(spectrogram)\n",
        "        if self.debug: intermediates['binary_image'] = spectrogram\n",
        "\n",
        "        spectrogram = self.spot_removal(spectrogram)\n",
        "        if self.debug: intermediates['spectrograms_after_spot_removal'] = spectrogram\n",
        "\n",
        "        spectrogram = self.morph_closing(spectrogram)\n",
        "        if self.debug: intermediates['spectrograms_after_morph_closing'] = spectrogram\n",
        "\n",
        "        if not self.debug:\n",
        "            return spectrogram, {}\n",
        "\n",
        "        return (spectrogram, intermediates)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95KHcfMgocTZ"
      },
      "source": [
        "## Carga de datos\n",
        "\n",
        "Se leen los audios de forma individual. Cada audio es un objeto. `BirdSongDataset` define el método `__getitem__` para obtener cada instancia del dataset.\n",
        "\n",
        "No se tiene en cuenta en qué momento del audio suena cada pájaro, tan sólo qué pájaros suenan en cada audio. El problema se plantea como **clasificación multietiqueta**.\n",
        "\n",
        "El método `get_class_proportions` se utiliza para comprobar que los datasets *train* y *validation* contienen la misma proporción de clases, es decir, están estratíficados."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Og-dzxQhKdMs",
        "outputId": "8b4fae0f-802a-46dc-d358-88b2a15ed8e2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/UO281798/miniconda3/envs/pytorch-env/lib/python3.10/site-packages/torchaudio/functional/functional.py:584: UserWarning: At least one mel filterbank has all zero values. The value for `n_mels` (128) may be set too high. Or, the value for `n_freqs` (513) may be set too low.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# class BirdSongDataset(Dataset):\n",
        "#     def __init__(self, df, audio_dir, class_info, transform=None):\n",
        "#         segments = []\n",
        "\n",
        "#         unique_filenames = df['filename'].unique()  # Only process each audio file once\n",
        "#         for unique_filename in unique_filenames:\n",
        "#             audio_path = os.path.join(audio_dir, unique_filename)\n",
        "#             waveform, sample_rate = torchaudio.load(audio_path)\n",
        "#             total_segments = int(math.ceil(waveform.shape[1] / sample_rate))  # Total segments in the audio\n",
        "\n",
        "#             # Calculate the unique labels for each segment\n",
        "#             for idx in range(total_segments):\n",
        "#                 start_time, end_time = idx, idx + 1\n",
        "#                 labels_in_segment = df[(df['filename'] == unique_filename) &\n",
        "#                                        (df['end'] > start_time) &\n",
        "#                                        (df['start'] < end_time)]['class'].unique().tolist()\n",
        "#                 segments.append({\n",
        "#                     'filename': unique_filename,\n",
        "#                     'segment_idx': idx,\n",
        "#                     'start': start_time,\n",
        "#                     'end': end_time,\n",
        "#                     'class': \",\".join(labels_in_segment)\n",
        "#                 })\n",
        "\n",
        "#         self.segments = pd.DataFrame(segments)\n",
        "#         self.audio_dir = audio_dir\n",
        "#         self.class_info = class_info\n",
        "#         self.transform = transform\n",
        "\n",
        "#     def __len__(self):\n",
        "#         return len(self.segments)\n",
        "\n",
        "#     def __getitem__(self, idx):\n",
        "#         row = self.segments.iloc[idx]\n",
        "#         audio_path = os.path.join(self.audio_dir, row['filename'])\n",
        "#         waveform, sample_rate = torchaudio.load(audio_path)\n",
        "\n",
        "#         # Extract 1-second segment\n",
        "#         start_sample = int(row['start'] * sample_rate)\n",
        "#         end_sample = int(row['end'] * sample_rate)\n",
        "#         waveform = waveform[:, start_sample:end_sample]\n",
        "\n",
        "#         # Padding if needed\n",
        "#         if waveform.shape[1] < sample_rate:\n",
        "#             num_padding = sample_rate - waveform.shape[1]\n",
        "#             waveform = torch.cat([waveform, torch.zeros(1, num_padding)], dim=1)\n",
        "\n",
        "#         class_names = row['class'].split(\",\") if row['class'] else []\n",
        "#         target = torch.zeros(len(self.class_info))\n",
        "#         for class_name in class_names:\n",
        "#             target[self.class_info.index(class_name)] = 1.0\n",
        "\n",
        "#         if self.transform:\n",
        "#             waveform = self.transform(waveform)\n",
        "\n",
        "#         return waveform, target\n",
        "\n",
        "#     def get_filename(self, idx):\n",
        "#         return self.segments.iloc[idx]['filename']\n",
        "\n",
        "class BirdSongDataset(Dataset):\n",
        "    def __init__(self, df, audio_dir, class_info, transform=None):\n",
        "        segments = []\n",
        "\n",
        "        unique_filenames = df['filename'].unique()  # Only process each audio file once\n",
        "        for unique_filename in unique_filenames:\n",
        "            audio_path = os.path.join(audio_dir, unique_filename)\n",
        "            waveform, sample_rate = torchaudio.load(audio_path)\n",
        "            total_segments = int(math.ceil(waveform.shape[1] / sample_rate))  # Total segments in the audio\n",
        "\n",
        "            # Calculate the unique labels for each segment\n",
        "            for idx in range(total_segments):\n",
        "                start_time, end_time = idx, idx + 1\n",
        "                labels_in_segment = df[(df['filename'] == unique_filename) &\n",
        "                                       (df['end'] > start_time) &\n",
        "                                       (df['start'] < end_time)]['class'].unique().tolist()\n",
        "                segments.append({\n",
        "                    'filename': unique_filename,\n",
        "                    'segment_idx': idx,\n",
        "                    'start': start_time,\n",
        "                    'end': end_time,\n",
        "                    'class': \",\".join(labels_in_segment)\n",
        "                })\n",
        "\n",
        "        self.segments = pd.DataFrame(segments)\n",
        "        self.audio_dir = audio_dir\n",
        "        self.class_info = class_info\n",
        "        # Store the transform, but we will not apply it in __getitem__\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.segments)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.segments.iloc[idx]\n",
        "        audio_path = os.path.join(self.audio_dir, row['filename'])\n",
        "        waveform, sample_rate = torchaudio.load(audio_path)\n",
        "\n",
        "        # Ensure waveform is mono\n",
        "        if waveform.shape[0] > 1:\n",
        "            waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
        "\n",
        "        # Resample from 44100 Hz to 16000 Hz if necessary\n",
        "        if sample_rate != 16000:\n",
        "            resampler = Resample(orig_freq=sample_rate, new_freq=16000)\n",
        "            waveform = resampler(waveform)\n",
        "\n",
        "        # Extract 1-second segment\n",
        "        start_sample = int(row['start'] * 16000)  # Use the new sample rate here\n",
        "        end_sample = int(row['end'] * 16000)      # Use the new sample rate here\n",
        "        waveform = waveform[:, start_sample:end_sample]\n",
        "\n",
        "        # Padding if needed\n",
        "        if waveform.shape[1] < 16000:  # Use the new sample rate here\n",
        "            num_padding = 16000 - waveform.shape[1]  # Use the new sample rate here\n",
        "            waveform = torch.cat([waveform, torch.zeros(1, num_padding)], dim=1)\n",
        "\n",
        "        class_names = row['class'].split(\",\") if row['class'] else []\n",
        "        target = torch.zeros(len(self.class_info))\n",
        "        for class_name in class_names:\n",
        "            target[self.class_info.index(class_name)] = 1.0\n",
        "\n",
        "        return waveform, target\n",
        "\n",
        "    def get_filename(self, idx):\n",
        "        return self.segments.iloc[idx]['filename']\n",
        "\n",
        "train_csv = pd.read_csv(f'{base_path}data/train.csv') # CSV with train audio filenames, and bird class names labels.\n",
        "class_info_csv = pd.read_csv(f'{base_path}data/class_info.csv')\n",
        "class_names = class_info_csv['class name'].tolist()\n",
        "\n",
        "# Convert the labels to a binary matrix form\n",
        "y = np.zeros((len(train_csv), len(class_names)))\n",
        "for i, (_, row) in enumerate(train_csv.iterrows()):\n",
        "    labels = row['class'].split(\",\")\n",
        "    for label in labels:\n",
        "        y[i, class_names.index(label)] = 1\n",
        "\n",
        "X_train, y_train, X_val, y_val = iterative_train_test_split(np.array(train_csv), y, test_size=0.2)\n",
        "\n",
        "train_df = pd.DataFrame(X_train, columns=train_csv.columns)\n",
        "valid_df = pd.DataFrame(X_val, columns=train_csv.columns)\n",
        "\n",
        "train_transform = nn.Sequential(\n",
        "    AudioPreprocessing(augment=True)\n",
        ")\n",
        "\n",
        "valid_transform = nn.Sequential(\n",
        "    AudioPreprocessing(augment=False)\n",
        ")\n",
        "\n",
        "train_dataset = BirdSongDataset(train_df, f'{base_path}data/train/', class_names, transform=train_transform)\n",
        "valid_dataset = BirdSongDataset(valid_df, f'{base_path}data/train/', class_names, transform=valid_transform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "DBrPrvtp-rIt"
      },
      "outputs": [],
      "source": [
        "def aggregate_predictions(predictions, segments_df):\n",
        "    aggregated_predictions = {}\n",
        "    for filename in segments_df['filename'].unique():\n",
        "        aggregated_predictions[filename] = set()\n",
        "        segments = segments_df[segments_df['filename'] == filename]\n",
        "        for idx, row in segments.iterrows():\n",
        "            aggregated_predictions[filename].update(predictions[idx])\n",
        "    return aggregated_predictions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_FVZvTwP-rIt",
        "outputId": "e827c65d-7f82-429b-a5e8-612ef2a7bc8d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              filename  segment_idx  start  end                    class\n",
            "0  nips4b_train001.wav            0      0    1              Sylcan_song\n",
            "1  nips4b_train001.wav            1      1    2  Sylcan_song,Petpet_song\n",
            "2  nips4b_train001.wav            2      2    3              Sylcan_song\n",
            "3  nips4b_train001.wav            3      3    4              Petpet_song\n",
            "4  nips4b_train001.wav            4      4    5              Petpet_song\n",
            "5  nips4b_train001.wav            5      5    6              Petpet_song\n"
          ]
        }
      ],
      "source": [
        "specific_audio_segments = train_dataset.segments[train_dataset.segments['filename'] == 'nips4b_train001.wav']\n",
        "print(specific_audio_segments)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wOJ3S-BI-rIu",
        "outputId": "09823880-bcf2-4a71-b0ba-1e4e5171db20"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Label: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Predicted Classes: ['Sylcan_song']\n"
          ]
        }
      ],
      "source": [
        "segment_idx = 2\n",
        "waveform, label = train_dataset[segment_idx]\n",
        "print(f\"Label: {label}\")\n",
        "\n",
        "# Convert tensor label back to class names to check\n",
        "predicted_classes = [class_name for idx, class_name in enumerate(class_names) if label[idx] == 1.0]\n",
        "print(\"Predicted Classes:\", predicted_classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FhnnarsNJSUg",
        "outputId": "ecbf7232-6720-478e-e127-2831c674e496"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Class Proportions in Training Dataset:\n",
            "Aegcau_call: 0.60%\n",
            "Alaarv_song: 2.86%\n",
            "Anttri_song: 2.23%\n",
            "Butbut_call: 0.38%\n",
            "Carcan_call: 1.25%\n",
            "Carcan_song: 1.72%\n",
            "Carcar_call: 1.58%\n",
            "Carcar_song: 2.67%\n",
            "Cerbra_call: 0.54%\n",
            "Cerbra_song: 0.35%\n",
            "Cetcet_song: 2.72%\n",
            "Chlchl_call: 0.38%\n",
            "Cicatr_song: 0.16%\n",
            "Cicorn_song: 0.19%\n",
            "Cisjun_song: 0.49%\n",
            "Colpal_song: 0.82%\n",
            "Corcor_call: 0.38%\n",
            "Denmaj_call: 0.49%\n",
            "Denmaj_drum: 0.38%\n",
            "Embcir_call: 0.71%\n",
            "Embcir_song: 0.93%\n",
            "Erirub_call: 0.79%\n",
            "Erirub_song: 1.55%\n",
            "Fricoe_call: 0.44%\n",
            "Fricoe_song: 1.14%\n",
            "Galcri_call: 0.82%\n",
            "Galcri_song: 0.87%\n",
            "Galthe_call: 0.27%\n",
            "Galthe_song: 2.51%\n",
            "Gargla_call: 0.27%\n",
            "Hirrus_call: 0.35%\n",
            "Jyntor_song: 0.19%\n",
            "Lopcri_call: 0.93%\n",
            "Loxcur_call: 1.44%\n",
            "Lularb_song: 3.81%\n",
            "Lusmeg_call: 0.74%\n",
            "Lusmeg_song: 1.91%\n",
            "Lyrple_song: 0.27%\n",
            "Motcin_call: 1.25%\n",
            "Musstr_call: 0.38%\n",
            "Noise: 1.82%\n",
            "Oriori_call: 0.27%\n",
            "Oriori_song: 0.87%\n",
            "Parate_call: 0.65%\n",
            "Parate_song: 1.91%\n",
            "Parcae_call: 1.63%\n",
            "Parcae_song: 1.42%\n",
            "Parmaj_call: 0.76%\n",
            "Parmaj_song: 2.45%\n",
            "Pasdom_call: 1.36%\n",
            "Pelgra_call: 0.49%\n",
            "Petpet_call: 0.82%\n",
            "Petpet_song: 0.87%\n",
            "Phofem_song: 0.82%\n",
            "Phycol_call: 0.19%\n",
            "Phycol_song: 0.93%\n",
            "Picpic_call: 0.82%\n",
            "Plaaff_song: 0.27%\n",
            "Plasab_song: 0.19%\n",
            "Poepal_call: 0.54%\n",
            "Poepal_song: 0.68%\n",
            "Prumod_song: 0.87%\n",
            "Ptehey_song: 0.41%\n",
            "Pyrpyr_call: 0.38%\n",
            "Regign_call: 0.63%\n",
            "Regign_song: 1.74%\n",
            "Serser_call: 0.44%\n",
            "Serser_song: 0.63%\n",
            "Siteur_call: 0.38%\n",
            "Siteur_song: 0.57%\n",
            "Strdec_song: 0.54%\n",
            "Strtur_song: 0.46%\n",
            "Stuvul_call: 0.27%\n",
            "Sylatr_call: 1.82%\n",
            "Sylatr_song: 1.20%\n",
            "Sylcan_call: 2.23%\n",
            "Sylcan_song: 4.38%\n",
            "Sylmel_call: 3.92%\n",
            "Sylmel_song: 2.83%\n",
            "Sylund_call: 0.49%\n",
            "Sylund_song: 4.17%\n",
            "Tetpyg_song: 0.60%\n",
            "Tibtom_song: 0.22%\n",
            "Trotro_song: 1.91%\n",
            "Turmer_call: 0.87%\n",
            "Turmer_song: 0.30%\n",
            "Turphi_call: 0.27%\n",
            "Turphi_song: 1.69%\n",
            "Unknown: 4.22%\n",
            "\n",
            "Class Proportions in Validation Dataset:\n",
            "Aegcau_call: 0.66%\n",
            "Alaarv_song: 2.84%\n",
            "Anttri_song: 2.29%\n",
            "Butbut_call: 0.33%\n",
            "Carcan_call: 1.31%\n",
            "Carcan_song: 1.75%\n",
            "Carcar_call: 1.53%\n",
            "Carcar_song: 2.73%\n",
            "Cerbra_call: 0.55%\n",
            "Cerbra_song: 0.33%\n",
            "Cetcet_song: 2.73%\n",
            "Chlchl_call: 0.33%\n",
            "Cicatr_song: 0.11%\n",
            "Cicorn_song: 0.22%\n",
            "Cisjun_song: 0.44%\n",
            "Colpal_song: 0.87%\n",
            "Corcor_call: 0.33%\n",
            "Denmaj_call: 0.44%\n",
            "Denmaj_drum: 0.44%\n",
            "Embcir_call: 0.76%\n",
            "Embcir_song: 0.87%\n",
            "Erirub_call: 0.76%\n",
            "Erirub_song: 1.53%\n",
            "Fricoe_call: 0.44%\n",
            "Fricoe_song: 1.20%\n",
            "Galcri_call: 0.76%\n",
            "Galcri_song: 0.87%\n",
            "Galthe_call: 0.22%\n",
            "Galthe_song: 2.51%\n",
            "Gargla_call: 0.22%\n",
            "Hirrus_call: 0.33%\n",
            "Jyntor_song: 0.22%\n",
            "Lopcri_call: 0.87%\n",
            "Loxcur_call: 1.42%\n",
            "Lularb_song: 3.82%\n",
            "Lusmeg_call: 0.76%\n",
            "Lusmeg_song: 1.97%\n",
            "Lyrple_song: 0.33%\n",
            "Motcin_call: 1.20%\n",
            "Musstr_call: 0.33%\n",
            "Noise: 1.86%\n",
            "Oriori_call: 0.33%\n",
            "Oriori_song: 0.87%\n",
            "Parate_call: 0.66%\n",
            "Parate_song: 1.86%\n",
            "Parcae_call: 1.64%\n",
            "Parcae_song: 1.42%\n",
            "Parmaj_call: 0.76%\n",
            "Parmaj_song: 2.40%\n",
            "Pasdom_call: 1.31%\n",
            "Pelgra_call: 0.55%\n",
            "Petpet_call: 0.76%\n",
            "Petpet_song: 0.87%\n",
            "Phofem_song: 0.76%\n",
            "Phycol_call: 0.22%\n",
            "Phycol_song: 0.98%\n",
            "Picpic_call: 0.87%\n",
            "Plaaff_song: 0.22%\n",
            "Plasab_song: 0.22%\n",
            "Poepal_call: 0.55%\n",
            "Poepal_song: 0.66%\n",
            "Prumod_song: 0.87%\n",
            "Ptehey_song: 0.44%\n",
            "Pyrpyr_call: 0.33%\n",
            "Regign_call: 0.66%\n",
            "Regign_song: 1.75%\n",
            "Serser_call: 0.44%\n",
            "Serser_song: 0.66%\n",
            "Siteur_call: 0.44%\n",
            "Siteur_song: 0.55%\n",
            "Strdec_song: 0.55%\n",
            "Strtur_song: 0.44%\n",
            "Stuvul_call: 0.22%\n",
            "Sylatr_call: 1.86%\n",
            "Sylatr_song: 1.20%\n",
            "Sylcan_call: 2.29%\n",
            "Sylcan_song: 4.37%\n",
            "Sylmel_call: 3.93%\n",
            "Sylmel_song: 2.84%\n",
            "Sylund_call: 0.55%\n",
            "Sylund_song: 4.15%\n",
            "Tetpyg_song: 0.55%\n",
            "Tibtom_song: 0.22%\n",
            "Trotro_song: 1.97%\n",
            "Turmer_call: 0.87%\n",
            "Turmer_song: 0.33%\n",
            "Turphi_call: 0.22%\n",
            "Turphi_song: 1.75%\n",
            "Unknown: 4.26%\n",
            "\n",
            "Differences in Proportions (Training - Validation):\n",
            "Aegcau_call: -0.06%\n",
            "Alaarv_song: 0.02%\n",
            "Anttri_song: -0.06%\n",
            "Butbut_call: 0.05%\n",
            "Carcan_call: -0.06%\n",
            "Carcan_song: -0.03%\n",
            "Carcar_call: 0.05%\n",
            "Carcar_song: -0.06%\n",
            "Cerbra_call: -0.00%\n",
            "Cerbra_song: 0.03%\n",
            "Cetcet_song: -0.01%\n",
            "Chlchl_call: 0.05%\n",
            "Cicatr_song: 0.05%\n",
            "Cicorn_song: -0.03%\n",
            "Cisjun_song: 0.05%\n",
            "Colpal_song: -0.06%\n",
            "Corcor_call: 0.05%\n",
            "Denmaj_call: 0.05%\n",
            "Denmaj_drum: -0.06%\n",
            "Embcir_call: -0.06%\n",
            "Embcir_song: 0.05%\n",
            "Erirub_call: 0.03%\n",
            "Erirub_song: 0.02%\n",
            "Fricoe_call: -0.00%\n",
            "Fricoe_song: -0.06%\n",
            "Galcri_call: 0.05%\n",
            "Galcri_song: -0.00%\n",
            "Galthe_call: 0.05%\n",
            "Galthe_song: -0.01%\n",
            "Gargla_call: 0.05%\n",
            "Hirrus_call: 0.03%\n",
            "Jyntor_song: -0.03%\n",
            "Lopcri_call: 0.05%\n",
            "Loxcur_call: 0.02%\n",
            "Lularb_song: -0.01%\n",
            "Lusmeg_call: -0.03%\n",
            "Lusmeg_song: -0.06%\n",
            "Lyrple_song: -0.06%\n",
            "Motcin_call: 0.05%\n",
            "Musstr_call: 0.05%\n",
            "Noise: -0.03%\n",
            "Oriori_call: -0.06%\n",
            "Oriori_song: -0.00%\n",
            "Parate_call: -0.00%\n",
            "Parate_song: 0.05%\n",
            "Parcae_call: -0.00%\n",
            "Parcae_song: -0.00%\n",
            "Parmaj_call: -0.00%\n",
            "Parmaj_song: 0.05%\n",
            "Pasdom_call: 0.05%\n",
            "Pelgra_call: -0.06%\n",
            "Petpet_call: 0.05%\n",
            "Petpet_song: -0.00%\n",
            "Phofem_song: 0.05%\n",
            "Phycol_call: -0.03%\n",
            "Phycol_song: -0.06%\n",
            "Picpic_call: -0.06%\n",
            "Plaaff_song: 0.05%\n",
            "Plasab_song: -0.03%\n",
            "Poepal_call: -0.00%\n",
            "Poepal_song: 0.03%\n",
            "Prumod_song: -0.00%\n",
            "Ptehey_song: -0.03%\n",
            "Pyrpyr_call: 0.05%\n",
            "Regign_call: -0.03%\n",
            "Regign_song: -0.00%\n",
            "Serser_call: -0.00%\n",
            "Serser_song: -0.03%\n",
            "Siteur_call: -0.06%\n",
            "Siteur_song: 0.03%\n",
            "Strdec_song: -0.00%\n",
            "Strtur_song: 0.03%\n",
            "Stuvul_call: 0.05%\n",
            "Sylatr_call: -0.03%\n",
            "Sylatr_song: -0.00%\n",
            "Sylcan_call: -0.06%\n",
            "Sylcan_song: 0.02%\n",
            "Sylmel_call: -0.01%\n",
            "Sylmel_song: -0.01%\n",
            "Sylund_call: -0.06%\n",
            "Sylund_song: 0.02%\n",
            "Tetpyg_song: 0.05%\n",
            "Tibtom_song: -0.00%\n",
            "Trotro_song: -0.06%\n",
            "Turmer_call: -0.00%\n",
            "Turmer_song: -0.03%\n",
            "Turphi_call: 0.05%\n",
            "Turphi_song: -0.06%\n",
            "Unknown: -0.04%\n"
          ]
        }
      ],
      "source": [
        "def get_class_proportions(y, class_names):\n",
        "    \"\"\"\n",
        "    Calculate the proportion of each class in the given binary matrix y.\n",
        "    \"\"\"\n",
        "    proportions = {}\n",
        "    total_samples = y.shape[0]\n",
        "\n",
        "    for idx, class_name in enumerate(class_names):\n",
        "        proportions[class_name] = np.sum(y[:, idx]) / total_samples\n",
        "\n",
        "    return proportions\n",
        "\n",
        "\n",
        "if DEBUG:\n",
        "    train_proportions = get_class_proportions(y_train, class_names)\n",
        "    valid_proportions = get_class_proportions(y_val, class_names)\n",
        "\n",
        "    print(\"Class Proportions in Training Dataset:\")\n",
        "    for class_name, proportion in train_proportions.items():\n",
        "        print(f\"{class_name}: {proportion * 100:.2f}%\")\n",
        "\n",
        "    print(\"\\nClass Proportions in Validation Dataset:\")\n",
        "    for class_name, proportion in valid_proportions.items():\n",
        "        print(f\"{class_name}: {proportion * 100:.2f}%\")\n",
        "\n",
        "    # Comparing the differences in proportions\n",
        "    print(\"\\nDifferences in Proportions (Training - Validation):\")\n",
        "    for class_name in class_names:\n",
        "        difference = train_proportions[class_name] - valid_proportions[class_name]\n",
        "        print(f\"{class_name}: {difference * 100:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "eeesSAdr-rIu"
      },
      "outputs": [],
      "source": [
        "# if DEBUG:\n",
        "#     print(f\"Number of elements: {len(valid_dataset)}\")\n",
        "#     sample, target = valid_dataset[90]\n",
        "#     processed_sample, intermediates = sample\n",
        "\n",
        "#     print(processed_sample.shape)\n",
        "#     num_positive_labels = target.sum().item()\n",
        "#     print(target)\n",
        "#     print(f\"Number of positive labels: {num_positive_labels}\")\n",
        "\n",
        "#     predicted_classes = [class_name for idx, class_name in enumerate(class_names) if target[idx] == 1.0]\n",
        "#     print(\"Predicted Classes:\", predicted_classes)\n",
        "\n",
        "#     visualize_intermediates(intermediates)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "EAU_nQNdH2RZ"
      },
      "outputs": [],
      "source": [
        "# if DEBUG:\n",
        "#     print(f\"Number of elements: {len(train_dataset)}\")\n",
        "#     sample, target = train_dataset[30]\n",
        "#     processed_sample, intermediates = sample\n",
        "\n",
        "#     print(processed_sample.shape)\n",
        "#     num_positive_labels = target.sum().item()\n",
        "#     print(target)\n",
        "#     print(f\"Number of positive labels: {num_positive_labels}\")\n",
        "\n",
        "#     predicted_classes = [class_name for idx, class_name in enumerate(class_names) if target[idx] == 1.0]\n",
        "#     print(\"Predicted Classes:\", predicted_classes)\n",
        "#     visualize_intermediates(intermediates)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-pXedOD0UwwG"
      },
      "source": [
        "**Calcular la longitud máxima de las formas de onda**\n",
        "\n",
        "Se determina la longitud máxima entre todas las formas de onda para poder rellenar (padding) o truncar los audios posteriormente, garantizando que todos tengan la misma longitud.\n",
        "\n",
        "La función `collate_fn` se utiliza para procesar y combinar un lote (batch) de muestras en el dataloader. Asegura que todas las formas de onda tengan la misma longitud (rellenando con ceros si es necesario) y devuelve las formas de onda junto con sus objetivos (etiquetas). Para esto, necesita la longitud máxima calculada anteriormente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "gvjVdc97TymV"
      },
      "outputs": [],
      "source": [
        "def collate_fn(batch):\n",
        "    # Extract the waveform (or spectrogram) and ignore intermediates (the dict used to debug) if they exist\n",
        "    waveforms_or_spectrograms = [wf[0] if isinstance(wf, tuple) else wf for wf, _ in batch]\n",
        "\n",
        "    if isinstance(batch[0][1], str):\n",
        "        # This handles the Test set scenario where targets might be filenames\n",
        "        waveforms_or_spectrograms = torch.stack(waveforms_or_spectrograms)\n",
        "        _, filenames = zip(*batch)\n",
        "        return waveforms_or_spectrograms, filenames\n",
        "\n",
        "    elif isinstance(batch[0][1], dict):\n",
        "        # This handles the scenario where DEBUG is False and intermediates dictionary is returned\n",
        "        waveforms_or_spectrograms = torch.stack(waveforms_or_spectrograms)\n",
        "        return waveforms_or_spectrograms  # Note: Here we return only the waveforms as there are no target labels\n",
        "\n",
        "    else:\n",
        "        # This handles the Training or validation batch scenario\n",
        "        waveforms_or_spectrograms = torch.stack(waveforms_or_spectrograms)\n",
        "        _, targets = zip(*batch)\n",
        "        targets = torch.stack(targets)\n",
        "        return waveforms_or_spectrograms, targets\n",
        "\n",
        "BATCH_SIZE=6\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QHV02H3TVZtm"
      },
      "source": [
        "## Definición del modelo\n",
        "\n",
        "- Se define una arquitectura basada en el modelo ResNet50 preentrenado.\n",
        "- Se adapta la primera capa convolucional para aceptar imágenes de un solo canal (grises).\n",
        "- Se elimina la última capa completamente conectada del ResNet y se agrega una clasificación personalizada para adaptar la arquitectura al problema multietiqueta.\n",
        "\n",
        "Una de las cosas que se ha probado, es utilizar una mezcla de *transfer-learning* y *fine-tuning*.\n",
        "\n",
        "**Transferencia de aprendizaje**:\n",
        "\n",
        "El modelo se carga y se adaptan algunas capas. Se congelan los pesos de las capas del modelo preentrenado para que no se actualicen durante el entrenamiento inicial, por lo que sólo las capas personalizadas, como la capa de clasificación, se entrenarán. Es decir, se adapta a una tarea diferente el modelo, manteniendo los pesos originales.\n",
        "\n",
        "**Fine-tuning**:\n",
        "\n",
        "Después de algunas épocas de entrenamiento determinadas en el código se desbloquean las capas del modelo preentrenado para que sus pesos también puedan actualizarse durante el entrenamiento\n",
        "\n",
        "```python\n",
        "if epoch == X:\n",
        "    for param in model.features.parameters():\n",
        "        param.requires_grad = True\n",
        "```\n",
        "\n",
        "Este fine-tuning ajusta el modelo a los datos específicos para mejorar el rendimiento, aunque causa cierto *overfitting* al sobreescribir los pesos originales con los datos de entrenamiento.\n",
        "\n",
        "Finalmente, tan sólo se utiliza fine-tuning. Los resultados no cambiaban excesivamente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "m8-S3jGAKdMt"
      },
      "outputs": [],
      "source": [
        "# class ResNetMultilabel(nn.Module):\n",
        "#     def __init__(self, num_classes, layers_to_unfreeze=None):\n",
        "#         super(ResNetMultilabel, self).__init__()\n",
        "\n",
        "#         # Initialize the pre-trained model\n",
        "#         self.resnet = models.resnet18(pretrained=True)\n",
        "\n",
        "#         # Replace the initial conv layer to handle grayscale images\n",
        "#         self.resnet.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
        "\n",
        "#         # Dropout for regularization\n",
        "#         # self.dropout = nn.Dropout(0.2)\n",
        "\n",
        "#         # Modify the final layer to match the number of classes\n",
        "#         fc_input_size = self.resnet.fc.in_features\n",
        "#         self.resnet.fc = nn.Sequential(\n",
        "#             nn.Dropout(0.3),\n",
        "#             nn.Linear(fc_input_size, 512),\n",
        "#             nn.ReLU(),\n",
        "#             nn.Dropout(0.3),\n",
        "#             nn.Linear(512, num_classes),\n",
        "#             nn.Sigmoid()\n",
        "#         )\n",
        "\n",
        "#         # Unfreeze selected layers for fine-tuning\n",
        "#         for name, child in self.resnet.named_children():\n",
        "#             if layers_to_unfreeze == \"all\" or name in layers_to_unfreeze:\n",
        "#                 for _, params in child.named_parameters():\n",
        "#                     params.requires_grad = True\n",
        "#             else:\n",
        "#                 for _, params in child.named_parameters():\n",
        "#                     params.requires_grad = False\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         return self.resnet(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "QDEtRrK_KdMt"
      },
      "outputs": [],
      "source": [
        "# # Set up the device\n",
        "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "# print(f\"Using: {device}\")\n",
        "\n",
        "# # Initialize the model\n",
        "# model = ResNetMultilabel(num_classes=len(class_names), layers_to_unfreeze=\"all\").to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D1zpmtnhXXEk"
      },
      "source": [
        "## Entrenamiento\n",
        "\n",
        "- Se utiliza BCE (Binary Cross Entropy), adecuada para problemas de clasificación multietiqueta junto a un optimizador Adam con tasa de aprendizaje `0.0001`.\n",
        "- Se utiliza un programador de learning rate ReduceLROnPlateau, o CosineAnnealingLR, que disminuye la tasa de aprendizaje si la función de pérdida no mejora.\n",
        "\n",
        "El proceso de entrenamiento se ejecuta a través de 20-50 épocas, y durante cada época se calcula la pérdida en entrenamiento y se ajustan los pesos del modelo, se calcula el F1 en entrenamiento, y se pasa el modelo a modo de evaluación para evaluar en el conjunto de validación, calculando tanto la pérdida como el F1 score.\n",
        "\n",
        "Si el modelo mejora se guarda un checkpoint de los pesos. Está implementado un sistema de early stopping para evitar el sobreajuste restaurando el mejor modelo.\n",
        "\n",
        "Después de cada época se ajusta el learning rate según la evolución de la pérdida en validación.\n",
        "\n",
        "**Búsqueda de umbral (threshold)**:\n",
        "- Se inicializa una lista de posibles `thresholds` de 0.1 a 1 en incrementos de 0.1. Estos son los umbrales para decidir si una predicción (probabilidad) del modelo es positiva o negativa.\n",
        "- Para cada umbral se calcula el F1 score en entrenamiento y validación y se elige el umbral que produce el mejor F1 score en el conjunto de validación.\n",
        "\n",
        "Esto es importante porque las salidas del modelo son valores continuos entre 0 y 1, que representan la confianza del modelo en que esa etiqueta es positiva, y es necesario decidir un umbral (`threshold`) para convertir estas salidas continuas en etiquetas binarias definitivas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U52GfnjY6SyF",
        "outputId": "a7c8073c-c6e5-4cdc-d3ca-5571f696d77b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cpu\n",
            "No GPU available, using CPU.\n",
            "Torch version: 2.1.1+cu121\n",
            "CUDA available: False\n",
            "Number of GPUs: 0\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoFeatureExtractor, ASTForAudioClassification\n",
        "import torch\n",
        "\n",
        "# Initialize feature extractor and model\n",
        "feature_extractor = AutoFeatureExtractor.from_pretrained(\"MIT/ast-finetuned-audioset-10-10-0.4593\")\n",
        "model = ASTForAudioClassification.from_pretrained(\"MIT/ast-finetuned-audioset-10-10-0.4593\")\n",
        "\n",
        "# Assuming class_names is defined somewhere in your code\n",
        "model.classifier.dense = torch.nn.Linear(model.config.hidden_size, len(class_names))\n",
        "model.num_labels = len(class_names)\n",
        "\n",
        "# Move model to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Debugging information\n",
        "print(f\"Using device: {device}\")\n",
        "if device.type == 'cuda':\n",
        "    print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU Memory Allocated: {torch.cuda.memory_allocated(0)}\")\n",
        "    print(f\"GPU Memory Cached: {torch.cuda.memory_cached(0)}\")\n",
        "else:\n",
        "    print(\"No GPU available, using CPU.\")\n",
        "\n",
        "# Additional system info\n",
        "print(f\"Torch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
        "\n",
        "# Note: Uncomment and modify the lines related to 'class_names' based on your specific use case.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u3_P-iwc6SyF",
        "outputId": "39884118-b997-4513-e015-0697b24c7a26"
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m/home/UO281798/DeepLearning/11-22-transformers.ipynb Cell 24\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B156.35.160.76/home/UO281798/DeepLearning/11-22-transformers.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=38'>39</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B156.35.160.76/home/UO281798/DeepLearning/11-22-transformers.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=40'>41</a>\u001b[0m \u001b[39m# Forward pass\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B156.35.160.76/home/UO281798/DeepLearning/11-22-transformers.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=41'>42</a>\u001b[0m outputs \u001b[39m=\u001b[39m model(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49minputs)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B156.35.160.76/home/UO281798/DeepLearning/11-22-transformers.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=42'>43</a>\u001b[0m logits \u001b[39m=\u001b[39m outputs\u001b[39m.\u001b[39mlogits\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B156.35.160.76/home/UO281798/DeepLearning/11-22-transformers.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=44'>45</a>\u001b[0m \u001b[39m# Compute loss, no need to apply sigmoid since BCEWithLogitsLoss does that internally\u001b[39;00m\n",
            "File \u001b[0;32m~/miniconda3/envs/pytorch-env/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[0;32m~/miniconda3/envs/pytorch-env/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/miniconda3/envs/pytorch-env/lib/python3.10/site-packages/transformers/models/audio_spectrogram_transformer/modeling_audio_spectrogram_transformer.py:573\u001b[0m, in \u001b[0;36mASTForAudioClassification.forward\u001b[0;34m(self, input_values, head_mask, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    565\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    566\u001b[0m \u001b[39mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[1;32m    567\u001b[0m \u001b[39m    Labels for computing the audio classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[1;32m    568\u001b[0m \u001b[39m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[1;32m    569\u001b[0m \u001b[39m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[1;32m    570\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    571\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[0;32m--> 573\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49maudio_spectrogram_transformer(\n\u001b[1;32m    574\u001b[0m     input_values,\n\u001b[1;32m    575\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m    576\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    577\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m    578\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m    579\u001b[0m )\n\u001b[1;32m    581\u001b[0m pooled_output \u001b[39m=\u001b[39m outputs[\u001b[39m1\u001b[39m]\n\u001b[1;32m    582\u001b[0m logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclassifier(pooled_output)\n",
            "File \u001b[0;32m~/miniconda3/envs/pytorch-env/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[0;32m~/miniconda3/envs/pytorch-env/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/miniconda3/envs/pytorch-env/lib/python3.10/site-packages/transformers/models/audio_spectrogram_transformer/modeling_audio_spectrogram_transformer.py:492\u001b[0m, in \u001b[0;36mASTModel.forward\u001b[0;34m(self, input_values, head_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    488\u001b[0m head_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_head_mask(head_mask, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m    490\u001b[0m embedding_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings(input_values)\n\u001b[0;32m--> 492\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(\n\u001b[1;32m    493\u001b[0m     embedding_output,\n\u001b[1;32m    494\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m    495\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    496\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m    497\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m    498\u001b[0m )\n\u001b[1;32m    499\u001b[0m sequence_output \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    500\u001b[0m sequence_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayernorm(sequence_output)\n",
            "File \u001b[0;32m~/miniconda3/envs/pytorch-env/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[0;32m~/miniconda3/envs/pytorch-env/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/miniconda3/envs/pytorch-env/lib/python3.10/site-packages/transformers/models/audio_spectrogram_transformer/modeling_audio_spectrogram_transformer.py:346\u001b[0m, in \u001b[0;36mASTEncoder.forward\u001b[0;34m(self, hidden_states, head_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    339\u001b[0m     layer_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    340\u001b[0m         layer_module\u001b[39m.\u001b[39m\u001b[39m__call__\u001b[39m,\n\u001b[1;32m    341\u001b[0m         hidden_states,\n\u001b[1;32m    342\u001b[0m         layer_head_mask,\n\u001b[1;32m    343\u001b[0m         output_attentions,\n\u001b[1;32m    344\u001b[0m     )\n\u001b[1;32m    345\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 346\u001b[0m     layer_outputs \u001b[39m=\u001b[39m layer_module(hidden_states, layer_head_mask, output_attentions)\n\u001b[1;32m    348\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    350\u001b[0m \u001b[39mif\u001b[39;00m output_attentions:\n",
            "File \u001b[0;32m~/miniconda3/envs/pytorch-env/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[0;32m~/miniconda3/envs/pytorch-env/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/miniconda3/envs/pytorch-env/lib/python3.10/site-packages/transformers/models/audio_spectrogram_transformer/modeling_audio_spectrogram_transformer.py:290\u001b[0m, in \u001b[0;36mASTLayer.forward\u001b[0;34m(self, hidden_states, head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m    285\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    286\u001b[0m     hidden_states: torch\u001b[39m.\u001b[39mTensor,\n\u001b[1;32m    287\u001b[0m     head_mask: Optional[torch\u001b[39m.\u001b[39mTensor] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    288\u001b[0m     output_attentions: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    289\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Union[Tuple[torch\u001b[39m.\u001b[39mTensor, torch\u001b[39m.\u001b[39mTensor], Tuple[torch\u001b[39m.\u001b[39mTensor]]:\n\u001b[0;32m--> 290\u001b[0m     self_attention_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattention(\n\u001b[1;32m    291\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlayernorm_before(hidden_states),  \u001b[39m# in AST, layernorm is applied before self-attention\u001b[39;49;00m\n\u001b[1;32m    292\u001b[0m         head_mask,\n\u001b[1;32m    293\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    294\u001b[0m     )\n\u001b[1;32m    295\u001b[0m     attention_output \u001b[39m=\u001b[39m self_attention_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    296\u001b[0m     outputs \u001b[39m=\u001b[39m self_attention_outputs[\u001b[39m1\u001b[39m:]  \u001b[39m# add self attentions if we output attention weights\u001b[39;00m\n",
            "File \u001b[0;32m~/miniconda3/envs/pytorch-env/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[0;32m~/miniconda3/envs/pytorch-env/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/miniconda3/envs/pytorch-env/lib/python3.10/site-packages/transformers/models/audio_spectrogram_transformer/modeling_audio_spectrogram_transformer.py:229\u001b[0m, in \u001b[0;36mASTAttention.forward\u001b[0;34m(self, hidden_states, head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m    224\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    225\u001b[0m     hidden_states: torch\u001b[39m.\u001b[39mTensor,\n\u001b[1;32m    226\u001b[0m     head_mask: Optional[torch\u001b[39m.\u001b[39mTensor] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    227\u001b[0m     output_attentions: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    228\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Union[Tuple[torch\u001b[39m.\u001b[39mTensor, torch\u001b[39m.\u001b[39mTensor], Tuple[torch\u001b[39m.\u001b[39mTensor]]:\n\u001b[0;32m--> 229\u001b[0m     self_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattention(hidden_states, head_mask, output_attentions)\n\u001b[1;32m    231\u001b[0m     attention_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput(self_outputs[\u001b[39m0\u001b[39m], hidden_states)\n\u001b[1;32m    233\u001b[0m     outputs \u001b[39m=\u001b[39m (attention_output,) \u001b[39m+\u001b[39m self_outputs[\u001b[39m1\u001b[39m:]  \u001b[39m# add attentions if we output them\u001b[39;00m\n",
            "File \u001b[0;32m~/miniconda3/envs/pytorch-env/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[0;32m~/miniconda3/envs/pytorch-env/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "total_epochs = 1\n",
        "\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=0.001)\n",
        "scheduler = ReduceLROnPlateau(optimizer, 'min', factor=0.5, patience=2)\n",
        "\n",
        "best_val_loss = float('inf')\n",
        "epochs_no_improve = 0\n",
        "n_epochs_stop = 20\n",
        "early_stop = False\n",
        "thresholds = np.arange(0.1, 1, 0.1)\n",
        "\n",
        "# Your training loop with modifications for the AST model\n",
        "for epoch in range(total_epochs):\n",
        "    model.train()\n",
        "    running_train_loss = 0.0\n",
        "    train_labels = []\n",
        "    train_preds = []\n",
        "\n",
        "    for inputs, labels in train_loader:\n",
        "        # Make sure inputs are single channel\n",
        "        if inputs.ndim == 3 and inputs.size(1) > 1:\n",
        "            print(f\"Input has multiple channels, shape: {inputs.shape}\")\n",
        "            inputs = torch.mean(inputs, dim=1, keepdim=True)\n",
        "        elif inputs.ndim == 2:\n",
        "            inputs = inputs.unsqueeze(1)  # Add channel dimension\n",
        "\n",
        "        # Now inputs should be of shape [batch, 1, time]\n",
        "        #print(f\"Shape before feature extractor: {inputs.shape}\")\n",
        "\n",
        "        # Process each waveform through the feature extractor\n",
        "        inputs = feature_extractor(inputs.squeeze(1).numpy(), sampling_rate=16000, return_tensors=\"pt\", padding=True)\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.logits\n",
        "\n",
        "        # Compute loss, no need to apply sigmoid since BCEWithLogitsLoss does that internally\n",
        "        loss = criterion(logits, labels.float())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_train_loss += loss.item()\n",
        "\n",
        "        # Store predictions and labels for F1 calculation\n",
        "        train_preds.extend(torch.sigmoid(logits).detach().cpu().numpy())\n",
        "        train_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    train_loss = running_train_loss / len(train_loader)\n",
        "\n",
        "    # Your validation loop remains the same, with changes for feature extraction and predictions handling\n",
        "    model.eval()\n",
        "    running_val_loss = 0.0\n",
        "    val_labels = []\n",
        "    val_preds = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in valid_loader:\n",
        "            # Process through feature extractor and to device\n",
        "            inputs = feature_extractor(inputs.squeeze(1).numpy(), sampling_rate=16000, return_tensors=\"pt\", padding=True)\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(**inputs)\n",
        "            logits = outputs.logits\n",
        "\n",
        "            loss = criterion(logits, labels.float())\n",
        "            running_val_loss += loss.item()\n",
        "\n",
        "            val_preds.extend(torch.sigmoid(logits).detach().cpu().numpy())\n",
        "            val_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    val_loss = running_val_loss / len(valid_loader)\n",
        "\n",
        "    # Calculate F1 scores without manual thresholding\n",
        "    train_f1 = f1_score(np.array(train_labels), np.array(train_preds) > 0.1, average='samples', zero_division=1)\n",
        "    val_f1 = f1_score(np.array(val_labels), np.array(val_preds) > 0.1, average='samples', zero_division=1)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Train F1: {train_f1:.4f}, Val Loss: {val_loss:.4f}, Val F1: {val_f1:.4f}\")\n",
        "\n",
        "    # Checkpointing\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        epochs_no_improve = 0\n",
        "        torch.save(model.state_dict(), 'best_model.pth')\n",
        "    else:\n",
        "        epochs_no_improve += 1\n",
        "\n",
        "    # Early stopping\n",
        "    if epochs_no_improve == n_epochs_stop:\n",
        "        print('Early stopping!')\n",
        "        early_stop = True\n",
        "        break\n",
        "\n",
        "    # Adjusting learning rate\n",
        "    scheduler.step(val_loss)\n",
        "\n",
        "if early_stop:\n",
        "    print(\"Stopped training. Loading best model weights!\")\n",
        "    # model.load_state_dict(torch.load('best_model.pth'))\n",
        "\n",
        "print('Finished Training')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IPW3M4gxKdMu"
      },
      "outputs": [],
      "source": [
        "# total_epochs = 30\n",
        "\n",
        "# train_losses = []\n",
        "# val_losses = []\n",
        "\n",
        "# criterion = nn.BCELoss()\n",
        "# optimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=0.001)\n",
        "# #optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "# scheduler = ReduceLROnPlateau(optimizer, 'min', factor=0.5, patience=2)\n",
        "\n",
        "# best_val_loss = float('inf')\n",
        "# epochs_no_improve = 0\n",
        "# n_epochs_stop = 20\n",
        "# early_stop = False\n",
        "# thresholds = np.arange(0.1, 1, 0.1)\n",
        "\n",
        "# for epoch in range(total_epochs):\n",
        "\n",
        "#     # Training\n",
        "#     model.train()\n",
        "#     running_train_loss = 0.0\n",
        "#     all_train_preds = []\n",
        "#     all_train_labels = []\n",
        "#     for i, (inputs, labels) in enumerate(train_loader):\n",
        "#         inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "#         optimizer.zero_grad()\n",
        "\n",
        "#         outputs = model(inputs)\n",
        "#         loss = criterion(outputs, labels)\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "\n",
        "#         running_train_loss += loss.item()\n",
        "\n",
        "#         # Store training predictions and true labels\n",
        "#         all_train_preds.extend(outputs.detach().cpu().numpy().tolist())\n",
        "#         all_train_labels.extend(labels.cpu().numpy().tolist())\n",
        "\n",
        "#     train_loss = running_train_loss / len(train_loader)\n",
        "\n",
        "#     # Validation\n",
        "#     model.eval()\n",
        "#     running_val_loss = 0.0\n",
        "#     all_preds = []\n",
        "#     all_labels = []\n",
        "#     with torch.no_grad():\n",
        "#         for inputs, labels in valid_loader:\n",
        "#             inputs, labels = inputs.to(device), labels.to(device)\n",
        "#             outputs = model(inputs)\n",
        "#             loss = criterion(outputs, labels)\n",
        "#             running_val_loss += loss.item()\n",
        "#             # Store predictions and true labels\n",
        "#             all_preds.extend(outputs.cpu().numpy().tolist())\n",
        "#             all_labels.extend(labels.cpu().numpy().tolist())\n",
        "\n",
        "#     val_loss = running_val_loss / len(valid_loader)\n",
        "\n",
        "#     # Append losses to the lists\n",
        "#     train_losses.append(train_loss)\n",
        "#     val_losses.append(val_loss)\n",
        "\n",
        "#     # Calculate validation F1 scores over different thresholds\n",
        "#     val_f1_scores = []\n",
        "#     for threshold in thresholds:\n",
        "#         val_f1_scores.append(f1_score(all_labels, np.array(all_preds) > threshold, average='samples', zero_division=1))\n",
        "\n",
        "#     # Get the best F1 score and corresponding threshold from the validation data\n",
        "#     best_threshold_index_val = np.argmax(val_f1_scores)\n",
        "#     best_threshold_val = thresholds[best_threshold_index_val]\n",
        "#     validation_f1 = val_f1_scores[best_threshold_index_val]\n",
        "\n",
        "#     # Calculate training F1 score using the best_threshold_val\n",
        "#     train_best_f1 = f1_score(all_train_labels, np.array(all_train_preds) > best_threshold_val, average='samples', zero_division=1)\n",
        "\n",
        "#     print(f\"Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Training F1: {train_best_f1:.4f}, Validation Loss: {val_loss:.4f}, Validation F1: {validation_f1:.4f} using threshold {best_threshold_val:.2f}\")\n",
        "\n",
        "#     # Checkpointing\n",
        "#     if val_loss < best_val_loss:\n",
        "#         best_val_loss = val_loss\n",
        "#         epochs_no_improve = 0\n",
        "#         torch.save(model.state_dict(), 'best_model.pth')\n",
        "#     else:\n",
        "#         epochs_no_improve += 1\n",
        "\n",
        "#     # Early stopping\n",
        "#     if epochs_no_improve == n_epochs_stop:\n",
        "#         print('Early stopping!')\n",
        "#         early_stop = True\n",
        "#         break\n",
        "\n",
        "#     # Adjusting learning rate\n",
        "#     scheduler.step(-val_loss)\n",
        "\n",
        "# if early_stop:\n",
        "#     print(\"Stopped training. Loading best model weights!\")\n",
        "#     model.load_state_dict(torch.load('best_model.pth'))\n",
        "\n",
        "# print('Finished Training')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xuuNXqBT-rIw"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(train_losses, label='Training Loss')\n",
        "plt.plot(val_losses, label='Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and Validation Loss over Epochs')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "print('Finished Training')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wt_5gGclbVWf"
      },
      "source": [
        "## Evaluación y Predicción en el conjunto de Test\n",
        "\n",
        "1. **Evaluación de las predicciones**:\n",
        "Se pone el modelo en modo `eval()` y se itera sobre el conjunto de validación para obtener las predicciones y se calcula el F1 usando el mejor umbral.\n",
        "\n",
        "2. **Preparación del conjunto de Test**:\n",
        "Se crea la clase `BirdSongTestDataset` que lee de `test.csv`, y se crea un DataLoader para el conjunto de Test.\n",
        "\n",
        "3. **Predicciones en el conjunto de Test**:\n",
        "Se itera sobre el conjunto de test y se obtienen las predicciones del modelo para cada archivo de audio. Se binarizan usando el mejor umbral y se almacenan en un diccionario con el nombre del archivo como clave.\n",
        "Las predicciones se convierten en un DataFrame de Pandas y se preparan los datos en el formato esperado, y por último se guarda el DataFrame en un archivo CSV."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RhZzEOkWH2Rb"
      },
      "outputs": [],
      "source": [
        "print(f\"Best threshold: {best_threshold_val}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8g6kj9f2KdMv"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "audio_preds = defaultdict(list)\n",
        "audio_labels = defaultdict(list)\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch_idx, (inputs, labels) in enumerate(valid_loader):\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        outputs = model(inputs)\n",
        "        preds = (outputs > best_threshold_val).float()\n",
        "\n",
        "        # Fetch the filename for each segment\n",
        "        for i, (input_, label, pred) in enumerate(zip(inputs, labels, preds)):\n",
        "            # Compute the correct index in the dataset\n",
        "            dataset_index = batch_idx * valid_loader.batch_size + i\n",
        "            filename = valid_dataset.get_filename(dataset_index)\n",
        "            audio_preds[filename].append(pred.cpu().numpy())\n",
        "            # The label should be the same for all segments of the same audio, so we append it only once\n",
        "            if filename not in audio_labels:\n",
        "                audio_labels[filename] = label.cpu().numpy()\n",
        "\n",
        "# Aggregating the segment-level predictions for each audio to generate a single prediction for the whole audio\n",
        "for filename in audio_preds:\n",
        "    # Here, we take the max prediction for each class across all segments as the audio-level prediction\n",
        "    audio_preds[filename] = np.maximum.reduce(audio_preds[filename])\n",
        "\n",
        "all_labels = list(audio_labels.values())\n",
        "all_preds = list(audio_preds.values())\n",
        "\n",
        "f1_macro = f1_score(all_labels, all_preds, average='samples', zero_division=1)\n",
        "print(f\"F1 Score (Samples): {f1_macro}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q6z5GWm1KdMv"
      },
      "outputs": [],
      "source": [
        "class BirdSongTestDataset(Dataset):\n",
        "    def __init__(self, df, audio_dir, transform=None):\n",
        "        segments = []\n",
        "\n",
        "        unique_filenames = df['filename'].unique()\n",
        "        for unique_filename in unique_filenames:\n",
        "            audio_path = os.path.join(audio_dir, unique_filename)\n",
        "            waveform, sample_rate = torchaudio.load(audio_path)\n",
        "            total_segments = int(math.ceil(waveform.shape[1] / sample_rate))\n",
        "\n",
        "            for idx in range(total_segments):\n",
        "                start_time, end_time = idx, idx + 1\n",
        "                segments.append({\n",
        "                    'filename': unique_filename,\n",
        "                    'segment_idx': idx,\n",
        "                    'start': start_time,\n",
        "                    'end': end_time,\n",
        "                })\n",
        "\n",
        "        self.segments = pd.DataFrame(segments)\n",
        "        self.audio_dir = audio_dir\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.segments)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.segments.iloc[idx]\n",
        "        audio_path = os.path.join(self.audio_dir, row['filename'])\n",
        "        waveform, sample_rate = torchaudio.load(audio_path)\n",
        "\n",
        "        start_sample = int(row['start'] * sample_rate)\n",
        "        end_sample = int(row['end'] * sample_rate)\n",
        "        waveform = waveform[:, start_sample:end_sample]\n",
        "\n",
        "        if waveform.shape[1] < sample_rate:\n",
        "            num_padding = sample_rate - waveform.shape[1]\n",
        "            waveform = torch.cat([waveform, torch.zeros(1, num_padding)], dim=1)\n",
        "\n",
        "        if self.transform:\n",
        "            waveform = self.transform(waveform)\n",
        "\n",
        "        return waveform, row['filename'], row['segment_idx']\n",
        "\n",
        "\n",
        "test_csv = pd.read_csv(f'{base_path}data/test.csv')\n",
        "\n",
        "def collate_fn(batch):\n",
        "    # Extracting tensors from the first item of the data point\n",
        "    segments = [item[0][0] for item in batch]  # Accessing the tensor in the tuple\n",
        "\n",
        "    # Extracting filenames\n",
        "    filenames = [item[1] for item in batch]\n",
        "\n",
        "    # Stacking tensors\n",
        "    segments_tensor = torch.stack(segments, dim=0)\n",
        "\n",
        "    return segments_tensor, filenames\n",
        "\n",
        "\n",
        "test_dataset = BirdSongTestDataset(test_csv, f'{base_path}data/test/', transform=valid_transform)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AxzJ8JxeUPbv"
      },
      "outputs": [],
      "source": [
        "if DEBUG:\n",
        "  (sample, intermediates), filename, label = test_dataset[25]\n",
        "  print(filename)\n",
        "  print(sample.shape)\n",
        "  visualize_intermediates(intermediates)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gc77_1e9UOiz"
      },
      "outputs": [],
      "source": [
        "# Make predictions on test set\n",
        "model.eval()\n",
        "predictions = defaultdict(lambda: np.zeros(len(class_names), dtype=bool))\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, filenames in test_loader:\n",
        "        inputs = inputs.to(device)\n",
        "        outputs = model(inputs)\n",
        "        preds = (outputs > 0.1).float().cpu().numpy().astype(bool)\n",
        "\n",
        "        for fname, pred in zip(filenames, preds):\n",
        "            # Use logical OR to aggregate predictions for each audio. If the bird appears in one segment of the audio, it means that it appears in the whole audio.\n",
        "            predictions[fname] = np.logical_or(predictions[fname], pred)\n",
        "\n",
        "# Convert boolean values to integer (0 or 1)\n",
        "for key in predictions:\n",
        "    predictions[key] = predictions[key].astype(int)\n",
        "\n",
        "submission_df = pd.DataFrame.from_dict(predictions, orient='index', columns=class_names)\n",
        "submission_df.reset_index(inplace=True)\n",
        "submission_df.rename(columns={'index': 'filename'}, inplace=True)\n",
        "submission_df.to_csv('submission.csv', index=False)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
