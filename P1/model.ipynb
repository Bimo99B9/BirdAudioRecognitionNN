{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "za_-0XklmBeX"
      },
      "source": [
        "# Aprendizaje Profundo\n",
        "Daniel López Gala - UO281798\n",
        "\n",
        "Se dispone del conjunto de datos NIPS4BPLUS, el cual contiene 674 ficheros de audio con una duración total de menos de una hora. En estos audios podemos encontrar grabaciones de aproximadamente 5 segundos con cantos de pájaros realizadas en 39 localizaciones diferentes repartidas por 7 regiones de Francia y España."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "urB-A95SsJri"
      },
      "outputs": [],
      "source": [
        "base_path = \"/content/drive/MyDrive/DeepLearning/\"\n",
        "#base_path = \"\"\n",
        "DEBUG = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z8PtqgSelI8r",
        "outputId": "e1a4d6bf-9f27-4b95-ca54-9dba07eb701a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "62qWTY1kk7e6",
        "outputId": "5cbdba1d-8358-4672-a12f-7c1146cef709"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting scikit-multilearn\n",
            "  Downloading scikit_multilearn-0.2.0-py3-none-any.whl (89 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/89.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━\u001b[0m \u001b[32m81.9/89.4 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.4/89.4 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: scikit-multilearn\n",
            "Successfully installed scikit-multilearn-0.2.0\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "\n",
        "import torchaudio\n",
        "import torchaudio.transforms as T\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "import torchvision.models as models\n",
        "\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "!pip install scikit-multilearn\n",
        "from skmultilearn.model_selection import iterative_train_test_split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fwU9YFOunaak"
      },
      "source": [
        "## Preprocesamiento y visualización\n",
        "\n",
        "- Se define una función `visualize_intermediates` para crear imágenes de los pasos intermedios usados en el preprocesamiento de los audios.\n",
        "\n",
        "- La clase `AudioPreprocessing` define los pasos para procesar la imagen. Se incluyen:\n",
        "  - Resample (De 44100Hz a 22050Hz)\n",
        "  - STFT (Convertir a espectrograma)\n",
        "  - Normalización\n",
        "  - Median clipping\n",
        "  - Conectar puntos cercanos mediante filtros\n",
        "  - Closing\n",
        "  - Dilation\n",
        "  - Median blur\n",
        "  - Eliminar residuos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "xKj6CGbCH2RX"
      },
      "outputs": [],
      "source": [
        "def visualize_intermediates(intermediates, sample_rate=22050, hop_length=int(512 * 0.75)):\n",
        "\n",
        "    # Set default background color for figures to white\n",
        "    plt.rcParams['figure.facecolor'] = 'white'\n",
        "\n",
        "    for key, value in intermediates.items():\n",
        "        if len(value.shape) == 2 and value.shape[1] > 2:  # This indicates a waveform\n",
        "            plt.figure(figsize=(12, 4))\n",
        "\n",
        "            # Calculate time axis in seconds for waveform\n",
        "            time_axis_waveform = np.linspace(0, value.shape[1] / sample_rate, value.shape[1])\n",
        "\n",
        "            plt.plot(time_axis_waveform, value[0].cpu().numpy())\n",
        "            plt.xlabel(\"Time (seconds)\")\n",
        "            plt.title(f\"{key}\")\n",
        "            plt.show()\n",
        "            continue\n",
        "\n",
        "        print(f\"Processing {key} with shape {value.shape}\")\n",
        "\n",
        "        if value.dim() == 4 and value.shape[-1] == 2:\n",
        "            complex_representation = value[0, ..., 0] + 1j * value[0, ..., 1]  # Convert to complex\n",
        "            magnitude = torch.abs(complex_representation).cpu().numpy()\n",
        "            phase = torch.angle(complex_representation).cpu().numpy()\n",
        "        elif value.is_complex():\n",
        "            magnitude = torch.abs(value).squeeze().cpu().numpy()\n",
        "            phase = torch.angle(value).squeeze().cpu().numpy()\n",
        "        else:\n",
        "            magnitude = value.squeeze().cpu().numpy()\n",
        "            phase = None\n",
        "\n",
        "        # Calculate time axis in seconds for magnitude\n",
        "        time_axis_magnitude = np.linspace(0, magnitude.shape[1] * hop_length / sample_rate, magnitude.shape[1])\n",
        "\n",
        "        # Plot magnitude with inverted grayscale colormap\n",
        "        plt.figure(figsize=(12, 4))\n",
        "        plt.imshow(magnitude, cmap='gray_r', aspect='auto', origin='lower', extent=[time_axis_magnitude[0], time_axis_magnitude[-1], 0, magnitude.shape[0]])\n",
        "        plt.xlabel(\"Time (seconds)\")\n",
        "        plt.title(f\"{key} Magnitude\")\n",
        "        plt.colorbar()\n",
        "        plt.show()\n",
        "\n",
        "        # Plot phase\n",
        "        if phase is not None:\n",
        "            plt.figure(figsize=(12, 4))\n",
        "            plt.imshow(((phase + np.pi) % (2 * np.pi) - np.pi), cmap='hsv', aspect='auto', origin='lower', vmin=-np.pi, vmax=np.pi, extent=[time_axis_magnitude[0], time_axis_magnitude[-1], 0, phase.shape[0]])\n",
        "            plt.xlabel(\"Time (seconds)\")\n",
        "            plt.title(f\"{key} Phase\")\n",
        "            plt.colorbar()\n",
        "            plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "jFTKwdi0v96g"
      },
      "outputs": [],
      "source": [
        "class AudioPreprocessing(nn.Module):\n",
        "    def __init__(self, debug=DEBUG, sample_rate=22050, n_fft=512, win_length=512, hop_length=int(512 * 0.75)):\n",
        "        super(AudioPreprocessing, self).__init__()\n",
        "        self.debug = debug\n",
        "        self.sample_rate = sample_rate\n",
        "        self.resampler = T.Resample(orig_freq=44100, new_freq=sample_rate)\n",
        "        self.spectrogram = T.Spectrogram(n_fft=n_fft, win_length=win_length, hop_length=hop_length, power=None, window_fn=torch.hann_window, center=False, return_complex=True)\n",
        "        self.time_stretch = T.TimeStretch(n_freq=n_fft // 2 + 1, fixed_rate=1.1)\n",
        "\n",
        "\n",
        "    def normalize(self, spectrogram, method='max'):\n",
        "        if method == 'max':\n",
        "            return spectrogram / (spectrogram.max() + 1e-5)\n",
        "        elif method == 'min-max':\n",
        "            return (spectrogram - spectrogram.min()) / (spectrogram.max() - spectrogram.min() + 1e-5)\n",
        "        elif method == 'mean-std':\n",
        "            return (spectrogram - spectrogram.mean()) / (spectrogram.std() + 1e-5)\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown normalization method: {method}\")\n",
        "\n",
        "    def median_clipping(self, spectrogram, threshold=3):\n",
        "        freq_median = torch.median(spectrogram, dim=2, keepdim=True)[0]\n",
        "        time_median = torch.median(spectrogram, dim=1, keepdim=True)[0]\n",
        "        mask = (spectrogram > (threshold * freq_median)) & (spectrogram > (threshold * time_median))\n",
        "        return torch.where(mask, torch.tensor(1.0).to(spectrogram.device), spectrogram)\n",
        "\n",
        "    def image_processing(self, spectrogram):\n",
        "        img = spectrogram.squeeze(0).cpu().numpy()\n",
        "\n",
        "        # Morphological closing to emphasize birdsong patterns\n",
        "        # kernel = np.ones((3,3), np.uint8)\n",
        "        # img = cv2.morphologyEx(img, cv2.MORPH_CLOSE, kernel)\n",
        "\n",
        "        # Median blurring for noise reduction\n",
        "        # img = cv2.medianBlur(img.astype(np.float32), 3)\n",
        "\n",
        "        return torch.tensor(img).float().unsqueeze(0)\n",
        "\n",
        "    def augment(self, waveform):\n",
        "        # TODO\n",
        "        return waveform\n",
        "\n",
        "    def forward(self, waveform):\n",
        "        intermediates = {}\n",
        "\n",
        "        # Resampling\n",
        "        # waveform = self.resampler(waveform)\n",
        "        # if self.debug:\n",
        "        #     intermediates['resampled'] = waveform\n",
        "\n",
        "        # Data Augmentation on the waveform\n",
        "        # waveform = self.augment(waveform)\n",
        "\n",
        "        # Apply STFT and obtain complex spectrogram\n",
        "        complex_spectrogram = self.spectrogram(waveform)\n",
        "\n",
        "        # Time stretch on complex spectrogram\n",
        "        stretched_complex_spectrogram = self.time_stretch(complex_spectrogram)\n",
        "\n",
        "        # Extract magnitude from the complex spectrogram for subsequent steps\n",
        "        spectrogram = torch.abs(stretched_complex_spectrogram)\n",
        "        if self.debug:\n",
        "            intermediates['stft'] = spectrogram\n",
        "\n",
        "        # Normalize\n",
        "        spectrogram = self.normalize(spectrogram, method='max')\n",
        "        if self.debug:\n",
        "            intermediates['normalized'] = spectrogram\n",
        "\n",
        "        # Median Clipping\n",
        "        # spectrogram = self.median_clipping(spectrogram, threshold=3)\n",
        "        # if self.debug:\n",
        "        #     intermediates['median_clipped'] = spectrogram\n",
        "\n",
        "        # Image Processing\n",
        "        # spectrogram = self.image_processing(spectrogram)\n",
        "        # if self.debug:\n",
        "        #     intermediates['image_processed'] = spectrogram\n",
        "\n",
        "        return spectrogram, intermediates if self.debug else spectrogram"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95KHcfMgocTZ"
      },
      "source": [
        "## Carga de datos\n",
        "\n",
        "Se leen los audios de forma individual. Cada audio es un objeto. `BirdSongDataset` define el método `__getitem__` para obtener cada instancia del dataset.\n",
        "\n",
        "No se tiene en cuenta en qué momento del audio suena cada pájaro, tan sólo qué pájaros suenan en cada audio. El problema se plantea como **clasificación multietiqueta**.\n",
        "\n",
        "El método `get_class_proportions` se utiliza para comprobar que los datasets *train* y *validation* contienen la misma proporción de clases, es decir, están estratíficados."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Og-dzxQhKdMs",
        "outputId": "c8f427f2-0f42-4f60-c268-9907d026f256"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchaudio/transforms/_transforms.py:94: UserWarning: `return_complex` argument is now deprecated and is not effective.`torchaudio.transforms.Spectrogram(power=None)` always returns a tensor with complex dtype. Please remove the argument in the function call.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "class BirdSongDataset(Dataset):\n",
        "    def __init__(self, df, audio_dir, class_info, transform=None):\n",
        "        self.df = df\n",
        "        self.audio_dir = audio_dir\n",
        "        self.class_info = class_info\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        filename = self.df.iloc[idx, 0]\n",
        "        audio_path = os.path.join(self.audio_dir, filename)\n",
        "        waveform, sample_rate = torchaudio.load(audio_path) # Get the waveform and sample rate for the current audio\n",
        "\n",
        "        labels = self.df[self.df['filename'] == filename] # Get all the rows for the current audio\n",
        "        target = torch.zeros(len(self.class_info)) # Create a torch tensor\n",
        "        for _, label in labels.iterrows(): # Iterate each bird sound label in the audio\n",
        "            class_name = label['class'] # Get the class name from the CSV (Ej.: Petpet_song)\n",
        "            target[self.class_info.index(class_name)] = 1.0 # Set to 1 in the position of that bird from the class_info file.\n",
        "\n",
        "        if self.transform:\n",
        "            waveform = self.transform(waveform) # Transform the waveform, where transform is AudioPreprocessing()\n",
        "\n",
        "        return waveform, target\n",
        "\n",
        "train_csv = pd.read_csv(f'{base_path}data/train.csv') # CSV with train audio filenames, and bird class names labels.\n",
        "class_info_csv = pd.read_csv(f'{base_path}data/class_info.csv')\n",
        "class_names = class_info_csv['class name'].tolist()\n",
        "\n",
        "# Convert the labels to a binary matrix form\n",
        "y = np.zeros((len(train_csv), len(class_names)))\n",
        "for i, (_, row) in enumerate(train_csv.iterrows()):\n",
        "    labels = row['class'].split(\",\")  # Classes are comma-separated\n",
        "    for label in labels:\n",
        "        y[i, class_names.index(label)] = 1\n",
        "\n",
        "X_train, y_train, X_val, y_val = iterative_train_test_split(np.array(train_csv), y, test_size=0.1)\n",
        "\n",
        "train_df = pd.DataFrame(X_train, columns=train_csv.columns)\n",
        "valid_df = pd.DataFrame(X_val, columns=train_csv.columns)\n",
        "\n",
        "transform = nn.Sequential(\n",
        "    AudioPreprocessing()\n",
        ")\n",
        "\n",
        "train_dataset = BirdSongDataset(train_df, f'{base_path}data/train/', class_names, transform=transform)\n",
        "valid_dataset = BirdSongDataset(valid_df, f'{base_path}data/train/', class_names, transform=transform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FhnnarsNJSUg",
        "outputId": "a19765ab-3bd7-41c5-e360-862b4da31e50"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Differences in Proportions (Training - Validation):\n",
            "Aegcau_call: -0.04%\n",
            "Alaarv_song: 0.04%\n",
            "Anttri_song: 0.08%\n",
            "Butbut_call: -0.07%\n",
            "Carcan_call: -0.04%\n",
            "Carcan_song: -0.01%\n",
            "Carcar_call: 0.06%\n",
            "Carcar_song: 0.09%\n",
            "Cerbra_call: -0.12%\n",
            "Cerbra_song: -0.09%\n",
            "Cetcet_song: -0.11%\n",
            "Chlchl_call: -0.07%\n",
            "Cicatr_song: -0.07%\n",
            "Cicorn_song: -0.02%\n",
            "Cisjun_song: 0.05%\n",
            "Colpal_song: -0.04%\n",
            "Corcor_call: -0.07%\n",
            "Denmaj_call: 0.05%\n",
            "Denmaj_drum: -0.05%\n",
            "Embcir_call: 0.08%\n",
            "Embcir_song: 0.05%\n",
            "Erirub_call: -0.09%\n",
            "Erirub_song: 0.03%\n",
            "Fricoe_call: 0.00%\n",
            "Fricoe_song: 0.08%\n",
            "Galcri_call: -0.07%\n",
            "Galcri_song: 0.00%\n",
            "Galthe_call: 0.05%\n",
            "Galthe_song: 0.13%\n",
            "Gargla_call: 0.05%\n",
            "Hirrus_call: -0.09%\n",
            "Jyntor_song: -0.02%\n",
            "Lopcri_call: 0.05%\n",
            "Loxcur_call: -0.09%\n",
            "Lularb_song: 0.14%\n",
            "Lusmeg_call: 0.10%\n",
            "Lusmeg_song: -0.04%\n",
            "Lyrple_song: 0.07%\n",
            "Motcin_call: -0.07%\n",
            "Musstr_call: -0.07%\n",
            "Noise: 0.11%\n",
            "Oriori_call: 0.07%\n",
            "Oriori_song: 0.00%\n",
            "Parate_call: 0.00%\n",
            "Parate_song: -0.06%\n",
            "Parcae_call: -0.11%\n",
            "Parcae_song: -0.11%\n",
            "Parmaj_call: -0.12%\n",
            "Parmaj_song: 0.06%\n",
            "Pasdom_call: 0.06%\n",
            "Pelgra_call: 0.08%\n",
            "Petpet_call: -0.07%\n",
            "Petpet_song: 0.00%\n",
            "Phofem_song: -0.07%\n",
            "Phycol_call: -0.02%\n",
            "Phycol_song: 0.08%\n",
            "Picpic_call: -0.04%\n",
            "Plaaff_song: 0.05%\n",
            "Plasab_song: -0.02%\n",
            "Poepal_call: -0.12%\n",
            "Poepal_song: 0.03%\n",
            "Prumod_song: 0.00%\n",
            "Ptehey_song: -0.02%\n",
            "Pyrpyr_call: -0.07%\n",
            "Regign_call: -0.02%\n",
            "Regign_song: 0.01%\n",
            "Serser_call: 0.00%\n",
            "Serser_song: -0.02%\n",
            "Siteur_call: -0.05%\n",
            "Siteur_song: -0.09%\n",
            "Strdec_song: 0.12%\n",
            "Strtur_song: 0.03%\n",
            "Stuvul_call: 0.05%\n",
            "Sylatr_call: 0.11%\n",
            "Sylatr_song: -0.11%\n",
            "Sylcan_call: 0.08%\n",
            "Sylcan_song: 0.05%\n",
            "Sylmel_call: 0.02%\n",
            "Sylmel_song: 0.02%\n",
            "Sylund_call: 0.08%\n",
            "Sylund_song: 0.05%\n",
            "Tetpyg_song: -0.07%\n",
            "Tibtom_song: 0.00%\n",
            "Trotro_song: -0.04%\n",
            "Turmer_call: 0.00%\n",
            "Turmer_song: 0.10%\n",
            "Turphi_call: 0.05%\n",
            "Turphi_song: -0.04%\n",
            "Unknown: 0.12%\n"
          ]
        }
      ],
      "source": [
        "def get_class_proportions(y, class_names):\n",
        "    \"\"\"\n",
        "    Calculate the proportion of each class in the given binary matrix y.\n",
        "    \"\"\"\n",
        "    proportions = {}\n",
        "    total_samples = y.shape[0]\n",
        "\n",
        "    for idx, class_name in enumerate(class_names):\n",
        "        proportions[class_name] = np.sum(y[:, idx]) / total_samples\n",
        "\n",
        "    return proportions\n",
        "\n",
        "\n",
        "train_proportions = get_class_proportions(y_train, class_names)\n",
        "valid_proportions = get_class_proportions(y_val, class_names)\n",
        "\n",
        "if DEBUG:\n",
        "    print(\"Class Proportions in Training Dataset:\")\n",
        "    for class_name, proportion in train_proportions.items():\n",
        "        print(f\"{class_name}: {proportion * 100:.2f}%\")\n",
        "\n",
        "    print(\"\\nClass Proportions in Validation Dataset:\")\n",
        "    for class_name, proportion in valid_proportions.items():\n",
        "        print(f\"{class_name}: {proportion * 100:.2f}%\")\n",
        "\n",
        "# Comparing the differences in proportions\n",
        "print(\"\\nDifferences in Proportions (Training - Validation):\")\n",
        "for class_name in class_names:\n",
        "    difference = train_proportions[class_name] - valid_proportions[class_name]\n",
        "    print(f\"{class_name}: {difference * 100:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "EAU_nQNdH2RZ"
      },
      "outputs": [],
      "source": [
        "if DEBUG:\n",
        "    sample, target = train_dataset[75]\n",
        "    processed_sample, intermediates = sample\n",
        "\n",
        "    print(processed_sample.shape)\n",
        "    num_positive_labels = target.sum().item()\n",
        "    print(f\"Number of positive labels: {num_positive_labels}\")\n",
        "    visualize_intermediates(intermediates)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-pXedOD0UwwG"
      },
      "source": [
        "**Calcular la longitud máxima de las formas de onda**\n",
        "\n",
        "Se determina la longitud máxima entre todas las formas de onda para poder rellenar (padding) o truncar los audios posteriormente, garantizando que todos tengan la misma longitud.\n",
        "\n",
        "La función `collate_fn` se utiliza para procesar y combinar un lote (batch) de muestras en el dataloader. Asegura que todas las formas de onda tengan la misma longitud (rellenando con ceros si es necesario) y devuelve las formas de onda junto con sus objetivos (etiquetas). Para esto, necesita la longitud máxima calculada anteriormente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "c-ZXYhlCKdMs"
      },
      "outputs": [],
      "source": [
        "# Calculate the global max length of waveforms in the dataset\n",
        "global_max_len = max(\n",
        "    max(dataset[i][0][0].shape[2] for i in range(len(dataset)))\n",
        "    for dataset in [train_dataset, valid_dataset]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "gvjVdc97TymV"
      },
      "outputs": [],
      "source": [
        "def collate_fn(batch):\n",
        "    # Test set scenario (Does not have targets, the filename is return to have the same output shape)\n",
        "    if isinstance(batch[0][1], str):\n",
        "        waveforms, filenames = zip(*batch)\n",
        "        # Directly pad and return, no need to stack targets\n",
        "        waveforms = [torch.cat([wf[0], torch.zeros(wf[0].shape[0], wf[0].shape[1], global_max_len - wf[0].shape[2])], dim=2) for wf in waveforms]\n",
        "        waveforms = torch.stack(waveforms)\n",
        "        return waveforms, filenames\n",
        "\n",
        "    # Training or validation batch\n",
        "    waveforms, targets = zip(*batch)\n",
        "    waveforms = [torch.cat([wf[0], torch.zeros((1, wf[0].shape[1], global_max_len - wf[0].shape[2]))], dim=2) for wf in waveforms]\n",
        "    waveforms = torch.stack(waveforms)\n",
        "    targets = torch.stack(targets)\n",
        "    return waveforms, targets\n",
        "\n",
        "BATCH_SIZE=32\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QHV02H3TVZtm"
      },
      "source": [
        "## Definición del modelo\n",
        "\n",
        "- Se define una arquitectura basada en el modelo ResNet50 preentrenado.\n",
        "- Se adapta la primera capa convolucional para aceptar imágenes de un solo canal (grises).\n",
        "- Se elimina la última capa completamente conectada del ResNet y se agrega una clasificación personalizada para adaptar la arquitectura al problema multietiqueta.\n",
        "\n",
        "Se utiliza una mezcla de *transfer-learning* y *fine-tuning*.\n",
        "\n",
        "**Transferencia de aprendizaje**:\n",
        "\n",
        "El modelo se carga y se adaptan algunas capas. Se congelan los pesos de las capas del modelo preentrenado para que no se actualicen durante el entrenamiento inicial, por lo que sólo las capas personalizadas, como la capa de clasificación, se entrenarán. Es decir, se adapta a una tarea diferente el modelo, manteniendo los pesos originales.\n",
        "\n",
        "**Fine-tuning**:\n",
        "\n",
        "Después de algunas épocas de entrenamiento determinadas en el código se desbloquean las capas del modelo preentrenado para que sus pesos también puedan actualizarse durante el entrenamiento\n",
        "\n",
        "```python\n",
        "if epoch == X:\n",
        "    for param in model.features.parameters():\n",
        "        param.requires_grad = True\n",
        "```\n",
        "\n",
        "Este fine-tuning ajusta el modelo a los datos específicos para mejorar el rendimiento, aunque causa cierto *overfitting* al sobreescribir los pesos originales con los datos de entrenamiento.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "m8-S3jGAKdMt"
      },
      "outputs": [],
      "source": [
        "class ResNetMultilabel(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(ResNetMultilabel, self).__init__()\n",
        "        # Load pre-trained resnet model\n",
        "        self.resnet = models.resnet50(pretrained=True)\n",
        "\n",
        "        # Modify the first convolutional layer to accept single-channel (grayscale) images\n",
        "        self.resnet.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
        "\n",
        "        # Remove the last fully connected layer to adapt for our task\n",
        "        layers = list(self.resnet.children())[:-1]\n",
        "        self.features = nn.Sequential(*layers)\n",
        "\n",
        "        # Custom classifier for our multilabel task\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(self.resnet.fc.in_features, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(512, num_classes),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.classifier(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QDEtRrK_KdMt",
        "outputId": "d3306ba5-d2c3-4a1a-d193-38c9e0890461"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using: cuda\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n",
            "100%|██████████| 97.8M/97.8M [00:00<00:00, 190MB/s]\n"
          ]
        }
      ],
      "source": [
        "# Set up the device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using: {device}\")\n",
        "\n",
        "# Initialize the model\n",
        "model = ResNetMultilabel(num_classes=len(class_names)).to(device)\n",
        "\n",
        "# The pre-trained layers are in the 'features' submodule\n",
        "for param in model.features.parameters():\n",
        "    param.requires_grad = False\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D1zpmtnhXXEk"
      },
      "source": [
        "## Entrenamiento\n",
        "\n",
        "- Se utiliza BCE (Binary Cross Entropy), adecuada para problemas de clasificación multietiqueta junto a un optimizador Adam con las tasas de aprendizaje diferentes para cada fase del entrenamiento.\n",
        "- Se utiliza un programador de learning rate (ReduceLROnPlateau) que disminuye la tasa de aprendizaje si la función de pérdida no mejora.\n",
        "\n",
        "El proceso de entrenamiento se ejecuta a través de 20 épocas, y durante cada época se calcula la pérdida en entrenamiento y se ajustan los pesos del modelo, se calcula el F1 en entrenamiento, y se pasa el modelo a modo de evaluación para evaluar en el conjunto de validación, calculando tanto la pérdida como el F1 score.\n",
        "\n",
        "Si el modelo mejora (en F1) se guarda un checkpoint de los pesos. Está implementada, aunque no se usa actualmente, una lógica de early-stop para evitar el sobreajuste.\n",
        "\n",
        "Después de cada época se ajusta el learning rate según la evolución de la pérdida en validación.\n",
        "\n",
        "**Búsqueda de umbral**:\n",
        "- Se inicializa una lista de posibles `thresholds` de 0.1 a 0.5 en incrementos de 0.05. Estos son los umbrales para decidir si una predicción (probabilidad) del modelo es positiva o negativa.\n",
        "- Para cada umbral se calcula el F1 score en entrenamiento y validación y se elige el umbral que produce el mejor F1 score en el conjunto de validación.\n",
        "\n",
        "Esto es importante porque las salidas del modelo son valores continuos entre 0 y 1, que representan la confianza del modelo en que esa etiqueta es positiva, y es necesario decidir un umbral (`threshold`) para convertir estas salidas continuas en etiquetas binarias definitivas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IPW3M4gxKdMu",
        "outputId": "de0ff786-68a6-4b55-e0b5-44f41e68a816"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1, Train Loss: 0.1321, Training F1: 0.1130, Validation Loss: 0.0834, Validation F1: 0.1479 using threshold 0.10\n",
            "Epoch 2, Train Loss: 0.0994, Training F1: 0.2037, Validation Loss: 0.0750, Validation F1: 0.1999 using threshold 0.10\n",
            "Epoch 3, Train Loss: 0.0886, Training F1: 0.2697, Validation Loss: 0.0696, Validation F1: 0.2590 using threshold 0.10\n",
            "Epoch 4, Train Loss: 0.0812, Training F1: 0.3179, Validation Loss: 0.0649, Validation F1: 0.2870 using threshold 0.15\n",
            "Epoch 5, Train Loss: 0.0750, Training F1: 0.3573, Validation Loss: 0.0607, Validation F1: 0.3435 using threshold 0.15\n",
            "Epoch 6, Train Loss: 0.0712, Training F1: 0.3910, Validation Loss: 0.0593, Validation F1: 0.3398 using threshold 0.15\n",
            "Epoch 7, Train Loss: 0.0685, Training F1: 0.4062, Validation Loss: 0.0554, Validation F1: 0.3912 using threshold 0.15\n",
            "Epoch 00007: reducing learning rate of group 0 to 5.0000e-05.\n",
            "Epoch 00007: reducing learning rate of group 1 to 5.0000e-04.\n",
            "Epoch 8, Train Loss: 0.0637, Training F1: 0.4487, Validation Loss: 0.0554, Validation F1: 0.3815 using threshold 0.15\n",
            "Epoch 9, Train Loss: 0.0624, Training F1: 0.4510, Validation Loss: 0.0546, Validation F1: 0.4079 using threshold 0.15\n",
            "Epoch 10, Train Loss: 0.0613, Training F1: 0.4632, Validation Loss: 0.0528, Validation F1: 0.4225 using threshold 0.15\n",
            "Epoch 11, Train Loss: 0.0419, Training F1: 0.6279, Validation Loss: 0.0430, Validation F1: 0.6340 using threshold 0.20\n",
            "Epoch 12, Train Loss: 0.0272, Training F1: 0.7619, Validation Loss: 0.0416, Validation F1: 0.7241 using threshold 0.25\n",
            "Epoch 13, Train Loss: 0.0218, Training F1: 0.8191, Validation Loss: 0.0431, Validation F1: 0.7389 using threshold 0.25\n",
            "Epoch 00013: reducing learning rate of group 0 to 2.5000e-05.\n",
            "Epoch 00013: reducing learning rate of group 1 to 2.5000e-05.\n",
            "Epoch 14, Train Loss: 0.0175, Training F1: 0.8580, Validation Loss: 0.0430, Validation F1: 0.7519 using threshold 0.25\n",
            "Epoch 15, Train Loss: 0.0160, Training F1: 0.8718, Validation Loss: 0.0452, Validation F1: 0.7554 using threshold 0.25\n",
            "Epoch 16, Train Loss: 0.0148, Training F1: 0.8879, Validation Loss: 0.0458, Validation F1: 0.7621 using threshold 0.25\n",
            "Epoch 17, Train Loss: 0.0139, Training F1: 0.8920, Validation Loss: 0.0459, Validation F1: 0.7646 using threshold 0.25\n",
            "Epoch 18, Train Loss: 0.0128, Training F1: 0.9069, Validation Loss: 0.0475, Validation F1: 0.7647 using threshold 0.25\n",
            "Epoch 19, Train Loss: 0.0125, Training F1: 0.9055, Validation Loss: 0.0465, Validation F1: 0.7702 using threshold 0.25\n",
            "Epoch 00019: reducing learning rate of group 0 to 1.2500e-05.\n",
            "Epoch 00019: reducing learning rate of group 1 to 1.2500e-05.\n",
            "Epoch 20, Train Loss: 0.0114, Training F1: 0.9173, Validation Loss: 0.0487, Validation F1: 0.7755 using threshold 0.25\n",
            "Epoch 21, Train Loss: 0.0110, Training F1: 0.9220, Validation Loss: 0.0493, Validation F1: 0.7802 using threshold 0.25\n",
            "Epoch 22, Train Loss: 0.0107, Training F1: 0.9267, Validation Loss: 0.0496, Validation F1: 0.7758 using threshold 0.25\n",
            "Epoch 23, Train Loss: 0.0101, Training F1: 0.9322, Validation Loss: 0.0492, Validation F1: 0.7747 using threshold 0.25\n",
            "Epoch 24, Train Loss: 0.0101, Training F1: 0.9311, Validation Loss: 0.0504, Validation F1: 0.7732 using threshold 0.25\n",
            "Early stopping!\n",
            "Stopped training. Loading best model weights!\n",
            "Finished Training\n"
          ]
        }
      ],
      "source": [
        "# Use discriminative learning rates\n",
        "transfer_learning_lr = 0.001\n",
        "fine_tuning_lr = 0.0005\n",
        "\n",
        "# Definimos el número de épocas para cada fase\n",
        "transfer_learning_epochs = 10\n",
        "fine_tuning_epochs = 15\n",
        "total_epochs = transfer_learning_epochs + fine_tuning_epochs\n",
        "\n",
        "optimizer = optim.Adam([\n",
        "    {'params': model.features.parameters(), 'lr': transfer_learning_lr / 10}, # Discriminative learning rate for pre-trained layers\n",
        "    {'params': model.classifier.parameters(), 'lr': transfer_learning_lr} # Learning rate for the classifier\n",
        "], weight_decay=1e-5)  # L2 regularization\n",
        "\n",
        "\n",
        "criterion = nn.BCELoss()\n",
        "scheduler = ReduceLROnPlateau(optimizer, 'min', factor=0.5, patience=5, verbose=True)\n",
        "\n",
        "best_val_loss = float('inf')\n",
        "best_f1 = float('-inf')\n",
        "epochs_no_improve = 0\n",
        "n_epochs_stop = 3\n",
        "early_stop = False\n",
        "thresholds = np.arange(0.1, 0.3, 0.05)\n",
        "\n",
        "for epoch in range(total_epochs):\n",
        "    # Cambiar a fine-tuning\n",
        "    if epoch == transfer_learning_epochs:\n",
        "        for param in model.features.parameters():\n",
        "            param.requires_grad = True\n",
        "\n",
        "        # Ajustar learning rates para fine-tuning\n",
        "        for param_group in optimizer.param_groups:\n",
        "            if param_group['params'] == model.classifier.parameters():\n",
        "                param_group['lr'] = fine_tuning_lr\n",
        "            else:\n",
        "                param_group['lr'] = fine_tuning_lr / 10\n",
        "\n",
        "    # Training\n",
        "    model.train()\n",
        "    running_train_loss = 0.0\n",
        "    all_train_preds = []\n",
        "    all_train_labels = []\n",
        "    for i, (inputs, labels) in enumerate(train_loader):\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_train_loss += loss.item()\n",
        "\n",
        "        # Store training predictions and true labels\n",
        "        all_train_preds.extend(outputs.detach().cpu().numpy().tolist())\n",
        "        all_train_labels.extend(labels.cpu().numpy().tolist())\n",
        "\n",
        "    train_loss = running_train_loss / len(train_loader)\n",
        "\n",
        "    # Calculate training F1 score and also find the best threshold on training data\n",
        "    train_f1_scores = []\n",
        "    for threshold in thresholds:\n",
        "        train_f1_scores.append(f1_score(all_train_labels, np.array(all_train_preds) > threshold, average='samples'))\n",
        "\n",
        "    # Get the best F1 score and corresponding threshold from the training data\n",
        "    best_threshold_index_train = np.argmax(train_f1_scores)\n",
        "    best_threshold_train = thresholds[best_threshold_index_train]\n",
        "    train_best_f1 = train_f1_scores[best_threshold_index_train]\n",
        "\n",
        "    # Validation using the threshold obtained from training data\n",
        "    model.eval()\n",
        "    running_val_loss = 0.0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in valid_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            running_val_loss += loss.item()\n",
        "            # Store predictions and true labels\n",
        "            all_preds.extend(outputs.cpu().numpy().tolist())\n",
        "            all_labels.extend(labels.cpu().numpy().tolist())\n",
        "\n",
        "    val_loss = running_val_loss / len(valid_loader)\n",
        "\n",
        "    # Calculate validation F1 score using threshold from training data\n",
        "    validation_f1 = f1_score(all_labels, np.array(all_preds) > best_threshold_train, average='samples')\n",
        "\n",
        "    print(f\"Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Training F1: {train_best_f1:.4f}, Validation Loss: {val_loss:.4f}, Validation F1: {validation_f1:.4f} using threshold {best_threshold_train:.2f}\")\n",
        "\n",
        "    # Checkpointing\n",
        "    if validation_f1 > best_f1:\n",
        "        best_f1 = validation_f1\n",
        "        epochs_no_improve = 0\n",
        "        torch.save(model.state_dict(), 'best_model.pth')\n",
        "    else:\n",
        "        epochs_no_improve += 1\n",
        "\n",
        "    # Early stopping\n",
        "    if epochs_no_improve == n_epochs_stop:\n",
        "        print('Early stopping!')\n",
        "        early_stop = True\n",
        "        break\n",
        "\n",
        "    # Adjusting learning rate\n",
        "    scheduler.step(-val_loss)  # Pass negative F1 score since ReduceLROnPlateau expects to minimize the metric\n",
        "\n",
        "if early_stop:\n",
        "    print(\"Stopped training. Loading best model weights!\")\n",
        "    model.load_state_dict(torch.load('best_model.pth'))\n",
        "\n",
        "print('Finished Training')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wt_5gGclbVWf"
      },
      "source": [
        "## Evaluación y Predicción en el conjunto de Test\n",
        "\n",
        "1. **Evaluación de las predicciones**:\n",
        "Se pone el modelo en modo `eval()` y se itera sobre el conjunto de validación para obtener las predicciones y se calcula el F1 usando el mejor umbral.\n",
        "\n",
        "2. **Preparación del conjunto de Test**:\n",
        "Se crea la clase `BirdSongTestDataset` que lee de `test.csv`, y se crea un DataLoader para el conjunto de Test.\n",
        "\n",
        "3. **Predicciones en el conjunto de Test**:\n",
        "Se itera sobre el conjunto de test y se obtienen las predicciones del modelo para cada archivo de audio. Se binarizan usando el mejor umbral y se almacenan en un diccionario con el nombre del archivo como clave.\n",
        "Las predicciones se convierten en un DataFrame de Pandas y se preparan los datos en el formato esperado, y por último se guarda el DataFrame en un archivo CSV."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RhZzEOkWH2Rb",
        "outputId": "8d5b62a3-8dee-4c12-c288-56470abca96c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best threshold: 0.25000000000000006\n"
          ]
        }
      ],
      "source": [
        "print(f\"Best threshold: {best_threshold_train}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8g6kj9f2KdMv",
        "outputId": "ee3b7ba2-9281-4388-aa23-3ac35e2f349e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "F1 Score (Samples): 0.7801682771314008\n"
          ]
        }
      ],
      "source": [
        "model.eval()\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in valid_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        outputs = model(inputs)\n",
        "        preds = (outputs > best_threshold_train).float()\n",
        "\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "f1_macro = f1_score(all_labels, all_preds, average='samples')\n",
        "print(f\"F1 Score (Samples): {f1_macro}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "q6z5GWm1KdMv"
      },
      "outputs": [],
      "source": [
        "class BirdSongTestDataset(Dataset):\n",
        "    def __init__(self, df, audio_dir, transform=None):\n",
        "        self.df = df\n",
        "        self.audio_dir = audio_dir\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        filename = self.df.iloc[idx, 0]\n",
        "        #print(f\"File: {filename}\")\n",
        "        audio_path = os.path.join(self.audio_dir, filename)\n",
        "        waveform, sample_rate = torchaudio.load(audio_path)\n",
        "\n",
        "        if self.transform:\n",
        "            waveform = self.transform(waveform)\n",
        "\n",
        "        return waveform, filename  # Return both waveform and filename to match the expected shape\n",
        "\n",
        "test_csv = pd.read_csv(f'{base_path}data/test.csv')\n",
        "\n",
        "test_dataset = BirdSongTestDataset(test_csv, f'{base_path}data/test/', transform=transform)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "AxzJ8JxeUPbv"
      },
      "outputs": [],
      "source": [
        "if DEBUG:\n",
        "  sample, _ = test_dataset[99]\n",
        "  processed_sample, intermediates = sample\n",
        "\n",
        "  print(processed_sample.shape)\n",
        "  visualize_intermediates(intermediates)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "Gc77_1e9UOiz"
      },
      "outputs": [],
      "source": [
        "# Make predictions on test set\n",
        "model.eval()\n",
        "predictions = {}\n",
        "with torch.no_grad():\n",
        "    for inputs, filenames in test_loader:\n",
        "        inputs = inputs.to(device)\n",
        "        outputs = model(inputs)\n",
        "        preds = (outputs > best_threshold_train).float().cpu().numpy().astype(int)\n",
        "        for fname, pred in zip(filenames, preds):\n",
        "            predictions[fname] = pred\n",
        "\n",
        "# Convert predictions to submission format\n",
        "submission_df = pd.DataFrame.from_dict(predictions, orient='index', columns=class_names)\n",
        "submission_df.reset_index(inplace=True)\n",
        "submission_df.rename(columns={'index': 'filename'}, inplace=True)\n",
        "submission_df.to_csv('submission.csv', index=False)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
